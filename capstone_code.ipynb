{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Kv7BO5dHzPS"
      },
      "outputs": [],
      "source": [
        "import os, json, warnings, glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from scipy.signal import firwin, filtfilt, resample_poly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnPKUGJ6f9cF",
        "outputId": "86c7fa05-682a-4c1c-c61e-a3c93cf34f5e"
      },
      "outputs": [],
      "source": [
        "import psutil\n",
        "vm = psutil.virtual_memory()\n",
        "print(\"Total:\", vm.total, \"bytes | Available:\", vm.available, \"bytes\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoiyupqVi252",
        "outputId": "eed398ee-5b34-4b66-9c0a-d22b30ccac34"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "print(sys.version)        # full version string\n",
        "print(sys.version_info)   # major/minor/micro tuple\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8ej9PLlJnfM",
        "outputId": "f49e158b-7df7-4ae6-897a-8ae2f8af9aa2"
      },
      "outputs": [],
      "source": [
        "#Loop to find WFDBRecords files\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"CWD:\", Path.cwd())\n",
        "print(\"Looking for any folder named 'WFDBRecords'...\")\n",
        "\n",
        "candidates = sorted(Path(\".\").rglob(\"WFDBRecords\"))\n",
        "for i, p in enumerate(candidates, 1):\n",
        "    print(f\"{i}. {p}\")\n",
        "\n",
        "if not candidates:\n",
        "    raise FileNotFoundError(\"Couldn't find any folder named 'WFDBRecords' under the current directory tree. \"\n",
        "                            \"Open the notebook in the project root, or paste the absolute path to WFDBRecords.\")\n",
        "\n",
        "# If there are multiple matches, pick the one that sits under a folder named 'results'\n",
        "# (this matches your screenshot). Otherwise take the first.\n",
        "chosen = None\n",
        "for p in candidates:\n",
        "    if p.parent.name.lower() == \"results\":\n",
        "        chosen = p\n",
        "        break\n",
        "if chosen is None:\n",
        "    chosen = candidates[0]\n",
        "\n",
        "print(\"\\nUsing:\", chosen.resolve())\n",
        "DATA_DIR = chosen  # <-- this is what we'll use below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oe7x0PqTKPgE",
        "outputId": "e0168af8-a8ab-4aae-ceb5-20e9a508c003"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.signal import butter, sosfiltfilt, resample_poly\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "import multiprocessing as mp\n",
        "import warnings, os\n",
        "\n",
        "# ----------------- CONFIG -----------------\n",
        "DATA_DIR = Path(\"/content/drive/MyDrive/HuBERT-ECG/code/a-large-scale-12-lead-electrocardiogram-database-for-arrhythmia-study-1.0.0/results/WFDBRecords\")\n",
        "OUT_DIR  = Path(\"./processed_ptbxl_5s_100hz_fast\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True); (OUT_DIR/\"npy\").mkdir(exist_ok=True)\n",
        "\n",
        "EXPECTED = ['time','I','II','III','aVR','aVL','aVF','V1','V2','V3','V4','V5','V6']\n",
        "TARGET_FS = 100\n",
        "BAND = (0.05, 47.0)   # Hz\n",
        "SEG_SEC = 5\n",
        "MAX_WORKERS = max(1, mp.cpu_count() - 1)  # leave 1 core free\n",
        "# ------------------------------------------\n",
        "\n",
        "def infer_fs(t):\n",
        "    dt = float(np.median(np.diff(t)))\n",
        "    if dt <= 0 or not np.isfinite(dt):\n",
        "        raise ValueError(\"Bad time axis\")\n",
        "    return int(round(1.0/dt)), dt\n",
        "\n",
        "def center_crop_samples(n, want):\n",
        "    if n <= want: return 0, n\n",
        "    start = (n - want) // 2\n",
        "    return start, start + want\n",
        "\n",
        "def preprocess_one(csv_path: Path):\n",
        "    # 1) load just the needed columns, fast\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path, usecols=EXPECTED)\n",
        "    except Exception as e:\n",
        "        return {\"ok\": False, \"path\": str(csv_path), \"err\": f\"read:{e}\"}\n",
        "\n",
        "    if any(c not in df.columns for c in EXPECTED):\n",
        "        return {\"ok\": False, \"path\": str(csv_path), \"err\": \"missing cols\"}\n",
        "\n",
        "    try:\n",
        "        t = df['time'].to_numpy()\n",
        "        fs_src, dt = infer_fs(t)\n",
        "\n",
        "        # (n,12) matrix\n",
        "        X = df[EXPECTED[1:]].to_numpy(dtype=float)\n",
        "\n",
        "        # 2) center crop to 5 s at source fs (reduces work)\n",
        "        want = SEG_SEC * fs_src\n",
        "        a, b = center_crop_samples(X.shape[0], want)\n",
        "        X = X[a:b, :]\n",
        "        if X.shape[0] < want:\n",
        "            # pad if record shorter than 5s (rare)\n",
        "            pad = want - X.shape[0]\n",
        "            X = np.pad(X, ((0, pad), (0, 0)), mode=\"constant\")\n",
        "\n",
        "        # 3) band-pass (Butterworth, zero-phase), vectorized across leads\n",
        "        nyq = fs_src / 2.0\n",
        "        hi = min(BAND[1], nyq * 0.98)  # keep under Nyquist\n",
        "        sos = butter(4, [BAND[0]/nyq, hi/nyq], btype=\"band\", output=\"sos\")\n",
        "        X = sosfiltfilt(sos, X, axis=0)\n",
        "\n",
        "        # 4) resample to 100 Hz (anti-alias)\n",
        "        if fs_src != TARGET_FS:\n",
        "            from math import gcd\n",
        "            up, down = TARGET_FS, fs_src\n",
        "            g = gcd(up, down); up//=g; down//=g\n",
        "            X = resample_poly(X, up, down, axis=0)\n",
        "\n",
        "        # 5) scale jointly to [-1,1]\n",
        "        mx, mn = np.nanmax(X), np.nanmin(X)\n",
        "        X = np.zeros_like(X, dtype=np.float32) if (not np.isfinite(mx) or not np.isfinite(mn) or mx == mn) \\\n",
        "            else (2.0*(X - mn)/(mx - mn) - 1.0).astype(np.float32)\n",
        "\n",
        "        # 6) ensure exact 5s@100Hz: 500x12\n",
        "        want_100 = SEG_SEC * TARGET_FS  # 500\n",
        "        if X.shape[0] >= want_100:\n",
        "            a, b = center_crop_samples(X.shape[0], want_100)\n",
        "            X = X[a:b, :]\n",
        "        else:\n",
        "            X = np.pad(X, ((0, want_100 - X.shape[0]), (0, 0)), mode=\"constant\")\n",
        "\n",
        "        # save\n",
        "        rel = csv_path.relative_to(DATA_DIR).with_suffix(\"\")\n",
        "        rid = \"_\".join(rel.parts)\n",
        "        out_npy = OUT_DIR/\"npy\"/f\"{rid}.npy\"\n",
        "        np.save(out_npy, X)\n",
        "\n",
        "        meta = {\n",
        "            \"record_id\": rid,\n",
        "            \"src_path\": str(csv_path),\n",
        "            \"out_path\": str(out_npy),\n",
        "            \"fs_src_hz\": fs_src,\n",
        "            \"fs_tgt_hz\": TARGET_FS,\n",
        "            \"segment_seconds\": SEG_SEC,\n",
        "            \"segment_shape\": \"500x12\"\n",
        "        }\n",
        "        return {\"ok\": True, \"meta\": meta}\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"ok\": False, \"path\": str(csv_path), \"err\": f\"proc:{e}\"}\n",
        "\n",
        "# Gather files (recursive)\n",
        "csv_files = list(DATA_DIR.rglob(\"*.csv\"))\n",
        "print(f\"Found {len(csv_files)} CSVs under {DATA_DIR}\")\n",
        "\n",
        "# Parallel execution\n",
        "results = []\n",
        "with ProcessPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
        "    futures = {ex.submit(preprocess_one, p): p for p in csv_files}\n",
        "    for i, fut in enumerate(as_completed(futures), 1):\n",
        "        res = fut.result()\n",
        "        results.append(res)\n",
        "        if i % 200 == 0:\n",
        "            ok = sum(1 for r in results if r.get(\"ok\"))\n",
        "            print(f\"Progress: {i}/{len(csv_files)} processed, ok={ok}\")\n",
        "\n",
        "# Collate metadata\n",
        "ok_metas = [r[\"meta\"] for r in results if r.get(\"ok\")]\n",
        "err_list = [r for r in results if not r.get(\"ok\")]\n",
        "pd.DataFrame(ok_metas).to_csv(OUT_DIR/\"metadata.csv\", index=False)\n",
        "if err_list:\n",
        "    pd.DataFrame(err_list).to_csv(OUT_DIR/\"errors.csv\", index=False)\n",
        "\n",
        "print(f\"Done. OK: {len(ok_metas)}, Errors: {len(err_list)}. Output -> {OUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "e7VXpLj8KaNe",
        "outputId": "5f6bcfac-9dca-4060-8673-2fb97a1049ac"
      },
      "outputs": [],
      "source": [
        "# ===== Build AF/AFL labels from .hea with progress =====\n",
        "# Works with your structure:\n",
        "# HuBERT-ECG/code/a-large-scale-12-lead-electrocardiogram-database-for-arrhythmia-study-1.0.0/results/WFDBRecords/...\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import re, json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ----------------- CONFIG -----------------\n",
        "OUT_DIR = Path(\"processed_ptbxl_5s_100hz_fast\")  # your processed folder\n",
        "META_CSV = OUT_DIR / \"metadata.csv\"              # created by your preprocessing\n",
        "LABELS_CSV = OUT_DIR / \"labels.csv\"\n",
        "MISSING_CSV = OUT_DIR / \"labels_missing.csv\"\n",
        "\n",
        "# SNOMED → AF/AFL mapping\n",
        "SNOMED_TO_TARGET = {\n",
        "    \"164889003\": \"AF\",   # Atrial fibrillation\n",
        "    \"164890007\": \"AFL\",  # Atrial flutter\n",
        "}\n",
        "\n",
        "# Known roots where .hea files may live (use both absolute-ish and relative variants)\n",
        "KNOWN_ROOTS = [\n",
        "    Path(\"HuBERT-ECG/code/a-large-scale-12-lead-electrocardiogram-database-for-arrhythmia-study-1.0.0/results/WFDBRecords\"),\n",
        "    Path(\"code/a-large-scale-12-lead-electrocardiogram-database-for-arrhythmia-study-1.0.0/results/WFDBRecords\"),\n",
        "    Path(\".\"),  # conservative fallback\n",
        "]\n",
        "# ------------------------------------------\n",
        "\n",
        "assert META_CSV.exists(), f\"Missing metadata file: {META_CSV}\"\n",
        "\n",
        "# Load metadata\n",
        "meta = pd.read_csv(META_CSV)\n",
        "assert {'record_id','src_path','out_path'}.issubset(meta.columns), \"metadata.csv must have record_id, src_path, out_path\"\n",
        "\n",
        "# Auto-derive additional search roots from src_path (parent dirs)\n",
        "auto_roots = sorted({ Path(p).with_suffix(\"\").parent for p in meta['src_path'].tolist() })\n",
        "SEARCH_ROOTS = auto_roots + KNOWN_ROOTS\n",
        "\n",
        "# Deduplicate while preserving order\n",
        "seen = set()\n",
        "SEARCH_ROOTS = [p for p in SEARCH_ROOTS if not (p in seen or seen.add(p))]\n",
        "\n",
        "# Keep only existing\n",
        "SEARCH_ROOTS = [p for p in SEARCH_ROOTS if p.exists()]\n",
        "if not SEARCH_ROOTS:\n",
        "    raise FileNotFoundError(\"Could not find any existing SEARCH_ROOTS. Check your paths.\")\n",
        "\n",
        "print(\"Will search .hea under:\")\n",
        "for r in SEARCH_ROOTS:\n",
        "    print(\"  -\", r.resolve())\n",
        "\n",
        "# ---- helpers ----\n",
        "def parse_target_from_text(txt: str) -> str | None:\n",
        "    \"\"\"Map free text to AF/AFL; check AFL first to avoid 'AF' inside 'AFL'.\"\"\"\n",
        "    if not isinstance(txt, str):\n",
        "        return None\n",
        "    up = txt.upper()\n",
        "    if \"FLUTTER\" in up or re.search(r\"\\bAFL\\b\", up):\n",
        "        return \"AFL\"\n",
        "    if \"FIBRILLATION\" in up or \"AFIB\" in up or re.search(r\"\\bAF\\b\", up):\n",
        "        return \"AF\"\n",
        "    return None\n",
        "\n",
        "def parse_target_from_hea(hea_path: Path) -> str | None:\n",
        "    \"\"\"Parse #Dx: codes → AF/AFL; fallback to text scan.\"\"\"\n",
        "    try:\n",
        "        txt = hea_path.read_text(errors=\"ignore\")\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "    # 1) Try SNOMED codes on '#Dx:' lines\n",
        "    # Example: \"#Dx: 164889003,59118001,164934002\"\n",
        "    matches = re.findall(r\"^\\s*#\\s*Dx\\s*:\\s*([0-9,\\s]+)\\s*$\", txt, flags=re.IGNORECASE | re.MULTILINE)\n",
        "    for m in matches:\n",
        "        codes = [c for c in re.split(r\"[,\\s]+\", m.strip()) if c.isdigit()]\n",
        "        for c in codes:\n",
        "            if c in SNOMED_TO_TARGET:\n",
        "                return SNOMED_TO_TARGET[c]\n",
        "\n",
        "    # 2) Fallback: plain-text scan anywhere in header\n",
        "    return parse_target_from_text(txt)\n",
        "\n",
        "def guess_hea_path(record_id: str, src_path: str) -> Path | None:\n",
        "    \"\"\"\n",
        "    First try sibling of the src CSV with .hea; else search by stub (e.g., 'JS36735.hea') under SEARCH_ROOTS.\n",
        "    \"\"\"\n",
        "    # Direct sibling: replace .csv with .hea\n",
        "    src = Path(src_path)\n",
        "    sib = src.with_suffix(\".hea\")\n",
        "    if sib.exists():\n",
        "        return sib\n",
        "\n",
        "    # Search by stub name (record filename stem)\n",
        "    stub = src.stem  # e.g., JS36735\n",
        "    for root in SEARCH_ROOTS:\n",
        "        hit = next(root.rglob(f\"{stub}.hea\"), None)\n",
        "        if hit and hit.exists():\n",
        "            return hit\n",
        "    return None\n",
        "\n",
        "# ---- main loop with progress ----\n",
        "rows_ok, rows_missing = [], []\n",
        "cnt_af = cnt_afl = 0\n",
        "\n",
        "print(\"\\nScanning headers and building labels.csv ...\")\n",
        "for _, row in tqdm(meta.iterrows(), total=len(meta), ncols=100):\n",
        "    rid = row['record_id']\n",
        "    hea = guess_hea_path(rid, row['src_path'])\n",
        "    tgt = parse_target_from_hea(hea) if hea else None\n",
        "\n",
        "    if tgt in {\"AF\",\"AFL\"}:\n",
        "        rows_ok.append({\"record_id\": rid, \"target\": tgt})\n",
        "        if tgt == \"AF\":  cnt_af  += 1\n",
        "        if tgt == \"AFL\": cnt_afl += 1\n",
        "    else:\n",
        "        why = \"no .hea found\" if hea is None else \"no AF/AFL code/text in .hea\"\n",
        "        rows_missing.append({\"record_id\": rid, \"why\": why})\n",
        "\n",
        "# ---- write outputs ----\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "pd.DataFrame(rows_ok).drop_duplicates(\"record_id\").to_csv(LABELS_CSV, index=False)\n",
        "pd.DataFrame(rows_missing).to_csv(MISSING_CSV, index=False)\n",
        "\n",
        "print(\"\\nDone.\")\n",
        "print(f\"  Labeled: {len(rows_ok)}  (AF={cnt_af}, AFL={cnt_afl})\")\n",
        "print(f\"  Missing: {len(rows_missing)}  -> {MISSING_CSV}\")\n",
        "print(f\"  Labels  -> {LABELS_CSV}\")\n",
        "\n",
        "# Quick peek\n",
        "if rows_ok:\n",
        "    print(\"\\nHead of labels.csv:\")\n",
        "    print(pd.read_csv(LABELS_CSV).head())\n",
        "if rows_missing[:5]:\n",
        "    print(\"\\nExamples of missing (first 5):\")\n",
        "    print(pd.DataFrame(rows_missing).head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQcy04vhKcFd"
      },
      "outputs": [],
      "source": [
        "# === Use HuBERT-ECG reproducibility/chapman CSVs to build AF/AFL labels + splits for YOUR preprocessed data ===\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# Paths\n",
        "OUT_DIR = Path(\"processed_ptbxl_5s_100hz_fast\")\n",
        "META_CSV = OUT_DIR / \"metadata.csv\"  # from your preprocessing step\n",
        "CHAPMAN_DIR = Path(\"reproducibility/chapman\")  # adjust if your tree differs\n",
        "\n",
        "assert META_CSV.exists(), f\"Missing {META_CSV}\"\n",
        "assert CHAPMAN_DIR.exists(), f\"Missing {CHAPMAN_DIR} (point to HuBERT-ECG/reproducibility/chapman)\"\n",
        "\n",
        "# Helper: load and concatenate HuBERT-ECG splits\n",
        "def load_chapman_splits(chapman_dir: Path) -> pd.DataFrame:\n",
        "    # train folds\n",
        "    trains = sorted(chapman_dir.glob(\"chapman_train*.csv\"))\n",
        "    vals   = sorted(chapman_dir.glob(\"chapman_val*.csv\"))\n",
        "    tests  = sorted(chapman_dir.glob(\"chapman_test.csv\"))\n",
        "    dfs = []\n",
        "    for p in trains:\n",
        "        d = pd.read_csv(p)\n",
        "        d[\"split_src\"] = p.name\n",
        "        d[\"split\"] = \"train\"\n",
        "        dfs.append(d)\n",
        "    for p in vals:\n",
        "        d = pd.read_csv(p)\n",
        "        d[\"split_src\"] = p.name\n",
        "        d[\"split\"] = \"val\"\n",
        "        dfs.append(d)\n",
        "    for p in tests:\n",
        "        d = pd.read_csv(p)\n",
        "        d[\"split_src\"] = p.name\n",
        "        d[\"split\"] = \"test\"\n",
        "        dfs.append(d)\n",
        "    assert dfs, \"No chapman_* CSVs found.\"\n",
        "    all_df = pd.concat(dfs, ignore_index=True)\n",
        "    return all_df\n",
        "\n",
        "chap = load_chapman_splits(CHAPMAN_DIR)\n",
        "\n",
        "# Columns vary by repo version; detect AF/AFL columns robustly\n",
        "lowcols = {c.lower(): c for c in chap.columns}\n",
        "def find_col(cands):\n",
        "    for c in cands:\n",
        "        if c.lower() in lowcols:\n",
        "            return lowcols[c.lower()]\n",
        "    return None\n",
        "\n",
        "AF_COL  = find_col([\"AF\", \"AFIB\"])\n",
        "AFL_COL = find_col([\"AFL\", \"AFLT\", \"FLUTTER\"])\n",
        "\n",
        "assert AF_COL or AFL_COL, f\"Could not find AF/AFL columns in: {chap.columns.tolist()}\"\n",
        "\n",
        "# Extract JS id from \"filename\" column (e.g., \"JS000123.hea.npy\" -> \"JS000123\")\n",
        "FILENAME_COL = find_col([\"filename\", \"file\", \"path\", \"fname\"])\n",
        "assert FILENAME_COL, \"No filename column found.\"\n",
        "chap[\"js_id\"] = chap[FILENAME_COL].astype(str).str.extract(r\"(JS\\d+)\")\n",
        "\n",
        "# Keep only AF/AFL positives\n",
        "def to_target(row):\n",
        "    af  = row.get(AF_COL, 0)\n",
        "    afl = row.get(AFL_COL, 0)\n",
        "    if afl == 1: return \"AFL\"\n",
        "    if af  == 1: return \"AF\"\n",
        "    return np.nan\n",
        "\n",
        "chap[\"target\"] = chap.apply(to_target, axis=1)\n",
        "chap = chap.dropna(subset=[\"js_id\", \"target\"]).reset_index(drop=True)\n",
        "\n",
        "# Load your metadata and map your record_id -> js_id (last token)\n",
        "meta = pd.read_csv(META_CSV)\n",
        "assert {\"record_id\",\"out_path\"}.issubset(meta.columns), \"metadata.csv must have record_id,out_path\"\n",
        "meta[\"js_id\"] = meta[\"record_id\"].astype(str).str.extract(r\"(JS\\d+)$\")\n",
        "\n",
        "# Merge labels with your processed files\n",
        "m = chap.merge(meta[[\"record_id\",\"js_id\",\"out_path\"]], on=\"js_id\", how=\"inner\")\n",
        "\n",
        "# If multiple files map to same js_id (rare), keep unique by record_id\n",
        "m = m.drop_duplicates(subset=[\"record_id\"]).reset_index(drop=True)\n",
        "\n",
        "# Save a simple labels.csv for your pipeline\n",
        "labels_df = m[[\"record_id\",\"target\"]].sort_values(\"record_id\")\n",
        "labels_df.to_csv(OUT_DIR/\"labels.csv\", index=False)\n",
        "print(\"Wrote labels.csv:\", OUT_DIR/\"labels.csv\", \"rows:\", len(labels_df))\n",
        "\n",
        "# Also save split_* files aligned to YOUR processed npy (train/val/test)\n",
        "def write_split(name):\n",
        "    df = m[m[\"split\"]==name][[\"record_id\",\"target\",\"out_path\"]].copy()\n",
        "    df = df.dropna(subset=[\"out_path\"]).drop_duplicates(\"record_id\")\n",
        "    outp = OUT_DIR / f\"split_{name}.csv\"\n",
        "    df.to_csv(outp, index=False)\n",
        "    print(f\"Wrote {outp} rows={len(df)} by filtering to AF/AFL & available npy\")\n",
        "\n",
        "for s in [\"train\",\"val\",\"test\"]:\n",
        "    write_split(s)\n",
        "\n",
        "# Quick peek\n",
        "print(\"\\nLabel balance (overall):\")\n",
        "print(labels_df[\"target\"].value_counts())\n",
        "for s in [\"train\",\"val\",\"test\"]:\n",
        "    p = OUT_DIR / f\"split_{s}.csv\"\n",
        "    if p.exists():\n",
        "        d = pd.read_csv(p)\n",
        "        print(f\"{s.upper()} counts:\", d[\"target\"].value_counts().to_dict())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "D1evpCIhKd1N",
        "outputId": "fceaa71e-7b3e-4803-dd90-be6e1b677491"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "OUT_DIR = Path(\"processed_ptbxl_5s_100hz_fast\")\n",
        "META = pd.read_csv(OUT_DIR/\"metadata.csv\")  # record_id, out_path\n",
        "META[\"js_id\"] = META[\"record_id\"].astype(str).str.extract(r\"(JS\\d+)$\")\n",
        "\n",
        "# 1) Load HuBERT-ECG chapman CSVs and extract AF/AFL\n",
        "CHAP = pd.concat([\n",
        "    pd.read_csv(p).assign(split=(\"train\" if \"train\" in p.stem else \"val\" if \"val\" in p.stem else \"test\"))\n",
        "    for p in sorted(Path(\"reproducibility/chapman\").glob(\"chapman_*.csv\"))\n",
        "], ignore_index=True)\n",
        "\n",
        "# columns vary; pick them robustly\n",
        "low = {c.lower(): c for c in CHAP.columns}\n",
        "fname = low.get(\"filename\") or low.get(\"file\") or low.get(\"path\")\n",
        "af    = low.get(\"af\") or low.get(\"afib\")\n",
        "afl   = low.get(\"afl\") or low.get(\"aflt\") or low.get(\"flutter\")\n",
        "assert fname and (af or afl), f\"Check columns: {CHAP.columns.tolist()}\"\n",
        "\n",
        "CHAP[\"js_id\"] = CHAP[fname].astype(str).str.extract(r\"(JS\\d+)\")\n",
        "CHAP[\"target\"] = np.where(afl and (CHAP[afl]==1), \"AFL\",\n",
        "                   np.where(af and (CHAP[af]==1), \"AF\", np.nan))\n",
        "CHAP = CHAP.dropna(subset=[\"js_id\",\"target\"])\n",
        "\n",
        "# 2) Merge to metadata to get the TRUE out_path\n",
        "M = CHAP.merge(META[[\"record_id\",\"js_id\",\"out_path\"]], on=\"js_id\", how=\"inner\").drop_duplicates(\"record_id\")\n",
        "\n",
        "# 3) If any split is tiny after filtering, rebuild stratified splits from M\n",
        "def save_split(name, df): df[[\"record_id\",\"target\",\"out_path\"]].to_csv(OUT_DIR/f\"split_{name}.csv\", index=False)\n",
        "\n",
        "train = M[M[\"split\"]==\"train\"]; val = M[M[\"split\"]==\"val\"]; test = M[M[\"split\"]==\"test\"]\n",
        "\n",
        "MIN_PER_CLASS = 10\n",
        "def ok(d):\n",
        "    c = d[\"target\"].value_counts().to_dict()\n",
        "    return all(c.get(k,0) >= MIN_PER_CLASS for k in [\"AF\",\"AFL\"])\n",
        "\n",
        "if not ok(val) or not ok(test):\n",
        "    df_all = M[[\"record_id\",\"target\",\"out_path\"]].reset_index(drop=True)\n",
        "    sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.20, random_state=42)\n",
        "    idx_tr, idx_te = next(sss1.split(df_all, df_all[\"target\"]))\n",
        "    rest, test = df_all.iloc[idx_tr], df_all.iloc[idx_te]\n",
        "    sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=42)\n",
        "    idx_tr, idx_va = next(sss2.split(rest, rest[\"target\"]))\n",
        "    train, val = rest.iloc[idx_tr], rest.iloc[idx_va]\n",
        "\n",
        "save_split(\"train\", train)\n",
        "save_split(\"val\",   val)\n",
        "save_split(\"test\",  test)\n",
        "\n",
        "# 4) Verify again\n",
        "for s in [\"train\",\"val\",\"test\"]:\n",
        "    df = pd.read_csv(OUT_DIR/f\"split_{s}.csv\")\n",
        "    ok = sum(Path(p).exists() for p in df[\"out_path\"])\n",
        "    print(s, \"final →\", ok, \"/\", len(df), \"paths exist\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiC83eYLKkd1",
        "outputId": "62697632-a639-4278-d708-60e48f504546"
      },
      "outputs": [],
      "source": [
        "!pip install loguru\n",
        "!pip install transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWotJF4YKmMQ",
        "outputId": "db8b5c37-eff3-43f7-b903-101c58476c68"
      },
      "outputs": [],
      "source": [
        "!python -m pip install --upgrade pip setuptools wheel\n",
        "\n",
        "# Pin Transformers + Hub (works well on Py3.10)\n",
        "!python -m pip install \"transformers==4.41.2\" \"huggingface_hub>=0.20,<0.26\" safetensors\n",
        "\n",
        "# Core deps (if you haven’t yet)\n",
        "!python -m pip install pyyaml numpy pandas scipy scikit-learn tqdm einops\n",
        "\n",
        "# Torch (CPU example; replace with CUDA wheel if you have a GPU)\n",
        "!ython -m pip install torch torchaudio --index-url https://download.pytorch.org/whl/cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsQ9zGggKnuX"
      },
      "outputs": [],
      "source": [
        "!python -m pip install \"wandb>=0.15,<0.18\"\n",
        "\n",
        "# If you have an account/token\n",
        "#!wandb login\n",
        "\n",
        "# Or disable online logging for this run\n",
        "#export WANDB_MODE=offline            # (Linux/macOS)\n",
        "# setx WANDB_MODE offline            # (Windows CMD)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrD9Ivc8Kp3f"
      },
      "outputs": [],
      "source": [
        "!python -m pip install neurokit2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wd7TXuHTKrFd"
      },
      "outputs": [],
      "source": [
        "!python -m pip install torchmetrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sWKD4dxKsVf"
      },
      "outputs": [],
      "source": [
        "!python -m pip install torcheval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amXSjkvXKuM3"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "!python - << 'PY'\n",
        "import torch; print(\"torch:\", torch.__version__, \"cuda_available:\", torch.cuda.is_available())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VL8PTbe5KvTH"
      },
      "outputs": [],
      "source": [
        "!python -m pip install --upgrade pip\n",
        "!python -m pip install \"torch==2.3.1\" \"torchaudio==2.3.1\" --index-url https://download.pytorch.org/whl/cu121\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXzpaHhEKx9W"
      },
      "outputs": [],
      "source": [
        "!python -m pip install --user pyyaml torchmetrics neurokit2 \"transformers==4.41.2\" \"huggingface_hub>=0.20,<0.26\" safetensors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_eQ6f_MKzYR"
      },
      "outputs": [],
      "source": [
        "import sys, site, sysconfig\n",
        "print(\"Python:\", sys.executable)\n",
        "print(\"site-packages:\", site.getsitepackages() if hasattr(site, \"getsitepackages\") else sysconfig.get_paths()[\"purelib\"])\n",
        "\n",
        "# install into THIS interpreter\n",
        "import sys\n",
        "!{sys.executable} -m pip install --upgrade pip\n",
        "!{sys.executable} -m pip install pytorch-lightning\n",
        "# (or) !{sys.executable} -m pip install lightning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksg0SahDK0uu"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "print(\"pl:\", pl.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tm7K5QuFK4iP",
        "outputId": "4669116c-d1ef-4720-e95b-058b3505780c"
      },
      "outputs": [],
      "source": [
        "!OMP_NUM_THREADS=1 MKL_NUM_THREADS=1 \\\n",
        "python code/finetune.py \\\n",
        "  1 \\\n",
        "  reproducibility/chapman/chapman_train0.csv \\\n",
        "  reproducibility/chapman/chapman_val0.csv \\\n",
        "  500 \\\n",
        "  3 \\\n",
        "  16 \\\n",
        "  f1_score \\\n",
        "  --epochs 2 \\\n",
        "  --largeness small \\\n",
        "  --lr 1e-3 \\\n",
        "  --wandb_run_name cpu_smoke_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mi1I3gY8RU1H"
      },
      "outputs": [],
      "source": [
        "# @title Stage 1: GPU Hubert-ECG\n",
        "!pip install -q \"transformers>=4.44\" huggingface_hub \"torch>=2.1\" torchaudio \\pytorch-lightning einops scikit-learn pandas numpy scipy neurokit2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5bed995149a0440a95d1ef0ad716a906",
            "43452960f1fa43ba8713767a711c8664",
            "afbc31723a5e48d686a8accb8f050802",
            "282b545db5504ed2b83af4f3be132274",
            "b13c364fced4488b92eeafbfa59adb55",
            "243981cc97e14c0caacf2f84a681d7d5",
            "332fd3d7ccb54ea7889fce1bdc6ff24a",
            "6fee8fbd1da9408a93aa319260469321",
            "7a47595834134ee4ab62e5556d4d3599",
            "0ebbc734d2b04ad4864809e2831ca4d4",
            "151b6650398a4f6a8cb4646569e8333e",
            "32f663c4149d46d0915db39df0511e99",
            "f2ff8e5c08d44ea8865691a6db47088c",
            "ee0704dc70eb4cbf854f75e90b048ad9",
            "7d4d677896e9468881d72bfe8574b864",
            "c188ccf3b4e54278ad190fa39ee381e1",
            "e04a77f0380346bd9a10b999d0376f29",
            "3924349dafd74d008d634488aa800371",
            "584cc7a8235c4de183c92d731e5cc868",
            "f723dc87836d4e75b40019e0e482a095",
            "96d202d918844c3388f9710d5a6a08b0",
            "7a497d2b2a3a46e58159e63a1fc91ffc",
            "33541973fce7430fbb8bc8e822534dec",
            "5ca7ef085e264155b922226c368aa38b",
            "611a073da6ae4bde9e5065f913a762a2",
            "bbc7adaa932644dab4851db28a782869",
            "bafcfec6b23a47619d7884fb428e0df2",
            "82f737ef8d68401d8352f211f8ccd584",
            "583f013494554c34a81be875b0fa4c30",
            "77a5950bfbc2446286295790ad7e64d7",
            "2a856a402963424292ffd655cb85d0cc",
            "2641a1f1eadc4d149777900ed619385e",
            "3ea6b2edc9a14b22afa613c41d145093"
          ]
        },
        "id": "_u2jCpQCWxIT",
        "outputId": "e1b52959-b1a0-4fab-efff-a2f89acfee99"
      },
      "outputs": [],
      "source": [
        "# ==== RESUMEABLE EMBEDDING + TRAIN (HuBERT-ECG, per-record cache) ====\n",
        "import os, json, time, warnings\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModel\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, recall_score, precision_recall_curve, average_precision_score\n",
        "import joblib\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ----- paths (reuse your existing OUT_DIR_ABS / SPLITS if you already defined them) -----\n",
        "OUT_DIR_ABS = Path(\"/content/drive/MyDrive/HuBERT-ECG/processed_ptbxl_5s_100hz_fast\").resolve()\n",
        "SPLITS  = {\n",
        "    \"train\": OUT_DIR_ABS / \"split_train.csv\",\n",
        "    \"val\":   OUT_DIR_ABS / \"split_val.csv\",\n",
        "    \"test\":  OUT_DIR_ABS / \"split_test.csv\",\n",
        "}\n",
        "for s,p in SPLITS.items():\n",
        "    assert p.exists(), f\"Missing {p}\"\n",
        "\n",
        "# ----- model config -----\n",
        "HUBERT_SIZE = \"base\"  # \"small\" | \"base\" | \"large\"\n",
        "HF_MODEL_NAME = {\n",
        "    \"small\": \"Edoardo-BS/hubert-ecg-small\",\n",
        "    \"base\":  \"Edoardo-BS/hubert-ecg-base\",\n",
        "    \"large\": \"Edoardo-BS/hubert-ecg-large\",\n",
        "}[HUBERT_SIZE]\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ----- thresholds -----\n",
        "AF_RECALL_TARGET  = 0.98\n",
        "AFL_MIN_PRECISION = 0.15\n",
        "AFL_MIN_RECALL    = 0.60\n",
        "\n",
        "# ----- per-record embedding cache -----\n",
        "EMB_ROOT = Path(f\"/content/_emb_perrec_hubert_{HUBERT_SIZE}\")\n",
        "(EMB_ROOT/\"train\").mkdir(parents=True, exist_ok=True)\n",
        "(EMB_ROOT/\"val\").mkdir(parents=True, exist_ok=True)\n",
        "(EMB_ROOT/\"test\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ----- load model (safe to call again) -----\n",
        "@torch.no_grad()\n",
        "def load_hubert():\n",
        "    print(f\"Loading HuBERT-ECG: {HF_MODEL_NAME}\")\n",
        "    model = AutoModel.from_pretrained(HF_MODEL_NAME, trust_remote_code=True)\n",
        "    model.to(device).eval()\n",
        "    return model\n",
        "\n",
        "model = load_hubert()\n",
        "\n",
        "# ----- helpers -----\n",
        "def load_split(name: str) -> pd.DataFrame:\n",
        "    df = pd.read_csv(SPLITS[name])\n",
        "    df[\"target\"] = df[\"target\"].astype(\"string\")  # NaN -> REST\n",
        "    return df\n",
        "\n",
        "from scipy.signal import welch\n",
        "\n",
        "def _bandpower_ratio_simple(sig, fs, band, refband):\n",
        "    # quick PSD ratios without QRS suppression (fast & good enough for lead pick)\n",
        "    f, Pxx = welch(sig.astype(np.float32), fs=fs, nperseg=min(256, len(sig)))\n",
        "    m1 = (f>=band[0]) & (f<=band[1])\n",
        "    m2 = (f>=refband[0]) & (f<=refband[1])\n",
        "    bp1 = float(np.trapz(Pxx[m1], f[m1])) if m1.any() else 0.0\n",
        "    bp2 = float(np.trapz(Pxx[m2], f[m2])) if m2.any() else 1e-12\n",
        "    return bp1 / bp2\n",
        "\n",
        "def _flutter_score_for_lead(sig, fs=100):\n",
        "    # sawtooth band (3.5–7 Hz) + harmonic (8–12 Hz)\n",
        "    fr = _bandpower_ratio_simple(sig, fs, (3.5,7.0), (1.0,20.0))\n",
        "    h2 = _bandpower_ratio_simple(sig, fs, (8.0,12.0), (1.0,20.0))\n",
        "    return fr + 0.5*h2\n",
        "\n",
        "def mono_from_12lead(x_500x12: np.ndarray, policy=\"lead_II\") -> np.ndarray:\n",
        "    # returns [500] float32\n",
        "    if x_500x12.ndim != 2 or x_500x12.shape[1] != 12 or x_500x12.shape[0] != 500:\n",
        "        raise ValueError(f\"bad shape {getattr(x_500x12,'shape',None)}\")\n",
        "    if policy == \"mean12\":\n",
        "        return x_500x12.mean(axis=1, dtype=np.float32)\n",
        "    if policy == \"best_flutter\":\n",
        "        # evaluate classic AFL-friendly leads and pick the max flutter score\n",
        "        cand = ['V1','II','V2','III','aVF']\n",
        "        best_name, best_sig, best_s = 'II', x_500x12[:,1], -1.0\n",
        "        for name in cand:\n",
        "            idx = {'I':0,'II':1,'III':2,'aVR':3,'aVL':4,'aVF':5,'V1':6,'V2':7,'V3':8,'V4':9,'V5':10,'V6':11}[name]\n",
        "            s = _flutter_score_for_lead(x_500x12[:, idx], fs=100)\n",
        "            if s > best_s and np.std(x_500x12[:, idx]) > 1e-4:\n",
        "                best_s, best_name, best_sig = s, name, x_500x12[:, idx]\n",
        "        return best_sig.astype(np.float32, copy=False)\n",
        "    # default: Lead II\n",
        "    return x_500x12[:, 1].astype(np.float32, copy=False)\n",
        "\n",
        "@torch.no_grad()\n",
        "def embed_batch(model, batch_1d):\n",
        "    \"\"\"\n",
        "    batch_1d: torch.FloatTensor [B, 500]\n",
        "    returns: torch.FloatTensor [B, D]\n",
        "    \"\"\"\n",
        "    with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=(device==\"cuda\")):\n",
        "        out = model(batch_1d)\n",
        "    hs = getattr(out, \"last_hidden_state\", None)\n",
        "    if hs is None:\n",
        "        if torch.is_tensor(out):\n",
        "            hs = out\n",
        "        else:\n",
        "            for v in vars(out).values():\n",
        "                if torch.is_tensor(v):\n",
        "                    hs = v; break\n",
        "    return hs.mean(dim=1) if hs.ndim == 3 else hs  # [B,D]\n",
        "\n",
        "def resume_embed_split(model, df: pd.DataFrame, split_name: str, batch_size=128, lead_policy=\"lead_II\"):\n",
        "    \"\"\"\n",
        "    Resume-able: writes per-record embeddings to {EMB_ROOT}/{split}/{record_id}.npy\n",
        "    Only computes missing ones.\n",
        "    \"\"\"\n",
        "    emb_dir = EMB_ROOT / split_name\n",
        "    done = set(p.stem for p in emb_dir.glob(\"*.npy\"))\n",
        "    todo_idx = [i for i, rid in enumerate(df[\"record_id\"].astype(str)) if rid not in done]\n",
        "\n",
        "    print(f\"[{split_name}] already have {len(done)} / {len(df)} ; embedding {len(todo_idx)} missing…\")\n",
        "    buf = []; buf_rids = []\n",
        "    pbar = tqdm(total=len(todo_idx), desc=f\"Embed-{split_name}\", ncols=100)\n",
        "\n",
        "    for i in todo_idx:\n",
        "        rid = str(df.iloc[i][\"record_id\"])\n",
        "        try:\n",
        "            x = np.load(df.iloc[i][\"out_path\"])   # [500,12]\n",
        "            mono = mono_from_12lead(x, policy=lead_policy)  # [500]\n",
        "            buf.append(mono); buf_rids.append(rid)\n",
        "            if len(buf) >= batch_size:\n",
        "                xt = torch.from_numpy(np.stack(buf)).to(device)  # [B,500]\n",
        "                z  = embed_batch(model, xt).detach().cpu().numpy()\n",
        "                for r, e in zip(buf_rids, z):\n",
        "                    np.save(emb_dir/f\"{r}.npy\", e.astype(np.float32))\n",
        "                buf.clear(); buf_rids.clear()\n",
        "                pbar.update(batch_size)\n",
        "        except Exception as e:\n",
        "            # skip this record, but continue\n",
        "            pbar.update(1)\n",
        "            continue\n",
        "\n",
        "    # flush\n",
        "    if buf:\n",
        "        xt = torch.from_numpy(np.stack(buf)).to(device)\n",
        "        z  = embed_batch(model, xt).detach().cpu().numpy()\n",
        "        for r, e in zip(buf_rids, z):\n",
        "            np.save(emb_dir/f\"{r}.npy\", e.astype(np.float32))\n",
        "        pbar.update(len(buf))\n",
        "        buf.clear(); buf_rids.clear()\n",
        "    pbar.close()\n",
        "\n",
        "def assemble_X_for_split(df: pd.DataFrame, split_name: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Load per-record embeddings back in CSV order. If any missing, drop those rows to keep alignment.\n",
        "    \"\"\"\n",
        "    emb_dir = EMB_ROOT / split_name\n",
        "    embs, keep_idx = [], []\n",
        "    for i, rid in enumerate(df[\"record_id\"].astype(str).tolist()):\n",
        "        p = emb_dir / f\"{rid}.npy\"\n",
        "        if p.exists():\n",
        "            embs.append(np.load(p))\n",
        "            keep_idx.append(i)\n",
        "    if not embs:\n",
        "        return np.zeros((0,1), dtype=np.float32), df.iloc[:0]\n",
        "    X = np.stack(embs).astype(np.float32)\n",
        "    return X, df.iloc[keep_idx].reset_index(drop=True)\n",
        "\n",
        "def choose_tau_for_recall(y_true, scores, target_recall=0.98):\n",
        "    thrs = np.unique(scores)[::-1]\n",
        "    best = 0.5\n",
        "    for t in thrs:\n",
        "        rec = recall_score(y_true, (scores >= t).astype(int), zero_division=0)\n",
        "        if rec >= target_recall:\n",
        "            best = float(t)\n",
        "        else:\n",
        "            break\n",
        "    return best\n",
        "\n",
        "def choose_tau_min_precision_with_recall(y_true, scores, min_precision=0.30, min_recall=0.60):\n",
        "    P, R, T = precision_recall_curve(y_true, scores)\n",
        "    # 1) meet both if possible: among those, MAXIMIZE recall, then precision\n",
        "    both = [(p, r, t) for p, r, t in zip(P[:-1], R[:-1], T) if (p >= min_precision and r >= min_recall)]\n",
        "    if both:\n",
        "        both.sort(key=lambda z: (z[1], z[0]))  # recall then precision\n",
        "        return float(both[-1][2])\n",
        "    # 2) if can't meet both, MEET RECALL first: among those, maximize precision\n",
        "    byR = [(p, r, t) for p, r, t in zip(P[:-1], R[:-1], T) if r >= min_recall]\n",
        "    if byR:\n",
        "        byR.sort(key=lambda z: (z[0], z[1]))   # precision then recall\n",
        "        return float(byR[-1][2])\n",
        "    # 3) last: meet precision and maximize recall (ONLY if recall set is empty)\n",
        "    byP = [(p, r, t) for p, r, t in zip(P[:-1], R[:-1], T) if p >= min_precision]\n",
        "    if byP:\n",
        "        byP.sort(key=lambda z: (z[1], z[0]))   # recall then precision\n",
        "        return float(byP[-1][2])\n",
        "    # 4) fallback: tightest τ\n",
        "    return float(T[-1]) if len(T) else 0.5\n",
        "\n",
        "def train_head(task_name, pos_label, Xtr, dtr, Xva, dva, Xte, dte, out_root: Path):\n",
        "    ytr = dtr[\"target\"].eq(pos_label).fillna(False).astype(int).to_numpy()\n",
        "    yva = dva[\"target\"].eq(pos_label).fillna(False).astype(int).to_numpy()\n",
        "    yte = dte[\"target\"].eq(pos_label).fillna(False).astype(int).to_numpy()\n",
        "\n",
        "    clf = make_pipeline(\n",
        "        StandardScaler(with_mean=True),\n",
        "        LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42)\n",
        "    ).fit(Xtr, ytr)\n",
        "\n",
        "    # context @0.5\n",
        "    if len(Xva):\n",
        "        base_pred = clf.predict(Xva)\n",
        "        print(f\"\\n[{task_name}] VAL @0.50:\")\n",
        "        print(classification_report(yva, base_pred, target_names=[f\"REST(not {pos_label})\", pos_label], digits=4))\n",
        "        print(\"CM:\\n\", confusion_matrix(yva, base_pred))\n",
        "        ap = average_precision_score(yva, clf.predict_proba(Xva)[:,1])\n",
        "        print(f\"VAL PR-AUC: {ap:.4f}\")\n",
        "\n",
        "    # tune τ\n",
        "    proba_va = clf.predict_proba(Xva)[:,1] if len(Xva) else np.array([])\n",
        "    if pos_label == \"AF\":\n",
        "        tau = choose_tau_for_recall(yva, proba_va, target_recall=AF_RECALL_TARGET)\n",
        "        tune_note = f\"recall≥{AF_RECALL_TARGET:.2f}\"\n",
        "    else:\n",
        "        tau = choose_tau_min_precision_with_recall(yva, proba_va, min_precision=AFL_MIN_PRECISION, min_recall=AFL_MIN_RECALL)\n",
        "        tune_note = f\"precision≥{AFL_MIN_PRECISION:.2f} & recall≥{AFL_MIN_RECALL:.2f}\"\n",
        "\n",
        "    # report at τ\n",
        "    for split_name, X, y in [(\"VAL\", Xva, yva), (\"TEST\", Xte, yte)]:\n",
        "        if len(X)==0: continue\n",
        "        pred = (clf.predict_proba(X)[:,1] >= tau).astype(int)\n",
        "        print(f\"\\n[{task_name}] {split_name} @τ={tau:.3f} ({tune_note})\")\n",
        "        print(classification_report(y, pred, target_names=[f\"REST(not {pos_label})\", pos_label], digits=4))\n",
        "        print(\"CM:\\n\", confusion_matrix(y, pred))\n",
        "\n",
        "    # export positives @τ\n",
        "    out_dir = out_root / f\"stage1_{task_name}\"\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    for split_name, X, y, D in [(\"val\", Xva, yva, dva), (\"test\", Xte, yte, dte)]:\n",
        "        if len(X)==0: continue\n",
        "        proba = clf.predict_proba(X)[:,1]\n",
        "        idx = np.where(proba >= tau)[0]\n",
        "        pd.DataFrame({\"record_id\": D.iloc[idx][\"record_id\"]}).to_csv(out_dir/f\"positives_{split_name}.csv\", index=False)\n",
        "        pd.DataFrame({\"record_id\": D.iloc[idx][\"record_id\"], \"is_true_positive\": y[idx].astype(int)}).to_csv(out_dir/f\"positives_{split_name}_with_labels.csv\", index=False)\n",
        "        print(f\"[{task_name}] {split_name.upper()} positives @τ={tau:.3f}: {len(idx)}\")\n",
        "\n",
        "    joblib.dump(clf, out_dir/f\"{task_name}_logreg_head.joblib\")\n",
        "    with open(out_dir/f\"{task_name}_meta.json\",\"w\") as f:\n",
        "        json.dump({\n",
        "            \"task\": task_name, \"pos_label\": pos_label,\n",
        "            \"threshold\": float(tau), \"tuning\": tune_note,\n",
        "            \"created_at\": time.ctime(), \"embedding\": f\"{HF_MODEL_NAME} last-layer mean-pooled\"\n",
        "        }, f, indent=2)\n",
        "    return out_dir\n",
        "\n",
        "# ====== RUN (resume embedding for each split, then assemble & train) ======\n",
        "dtr = load_split(\"train\"); dva = load_split(\"val\"); dte = load_split(\"test\")\n",
        "\n",
        "# Resume-only: this computes ONLY missing per-record files\n",
        "resume_embed_split(model, dtr, \"train\", batch_size=128, lead_policy=\"lead_II\")\n",
        "resume_embed_split(model, dva, \"val\",   batch_size=128, lead_policy=\"lead_II\")\n",
        "resume_embed_split(model, dte, \"test\",  batch_size=128, lead_policy=\"lead_II\")\n",
        "\n",
        "# Assemble matrices in CSV order (dropping rows whose embedding file is missing)\n",
        "Xtr, dtr_fixed = assemble_X_for_split(dtr, \"train\")\n",
        "Xva, dva_fixed = assemble_X_for_split(dva, \"val\")\n",
        "Xte, dte_fixed = assemble_X_for_split(dte, \"test\")\n",
        "print(\"Embed sizes ->\", \"train:\", Xtr.shape, \"val:\", Xva.shape, \"test:\", Xte.shape)\n",
        "\n",
        "# Safety\n",
        "if min(len(Xtr), len(Xva), len(Xte)) == 0:\n",
        "    raise RuntimeError(\"Some split has 0 embeddings after resume; check cache and split paths.\")\n",
        "\n",
        "# Train AF & AFL heads\n",
        "dir_af  = train_head(\"AFvsREST\",  \"AF\",  Xtr, dtr_fixed, Xva, dva_fixed, Xte, dte_fixed, OUT_DIR_ABS)\n",
        "dir_afl = train_head(\"AFLvsREST\", \"AFL\", Xtr, dtr_fixed, Xva, dva_fixed, Xte, dte_fixed, OUT_DIR_ABS)\n",
        "print(\"\\nDone. Outputs:\")\n",
        "print(\" \", dir_af)\n",
        "print(\" \", dir_afl)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xg-7xG0pOSPC",
        "outputId": "ceded4a6-0023-44da-cd14-532cd57da14f"
      },
      "outputs": [],
      "source": [
        "# --- DIAGNOSE + AUTO-FIX MISSING positives_val_with_labels.csv ---\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Make sure OUT points to the SAME directory you used in Stage-1\n",
        "OUT = Path(\"/content/drive/MyDrive/HuBERT-ECG/processed_ptbxl_5s_100hz_fast\").resolve()\n",
        "print(\"Using OUT =\", OUT)\n",
        "\n",
        "def _list_stage1(task):\n",
        "    d = OUT / f\"stage1_{task}\"\n",
        "    print(f\"\\nListing {d}  (exists={d.exists()}):\")\n",
        "    if d.exists():\n",
        "        for p in sorted(d.glob(\"*\")):\n",
        "            print(\"  -\", p.name)\n",
        "\n",
        "def _ensure_with_labels(task_name, pos_label):\n",
        "    stage1_dir = OUT / f\"stage1_{task_name}\"\n",
        "    pos_val_csv  = stage1_dir / \"positives_val.csv\"\n",
        "    pos_val_lbls = stage1_dir / \"positives_val_with_labels.csv\"\n",
        "\n",
        "    # Already present?\n",
        "    if pos_val_lbls.exists():\n",
        "        print(f\"[OK] {pos_val_lbls} already exists.\")\n",
        "        return pos_val_lbls\n",
        "\n",
        "    # If we have positives_val.csv, build the labeled file now\n",
        "    if pos_val_csv.exists():\n",
        "        sv = pd.read_csv(OUT/\"split_val.csv\").drop_duplicates(\"record_id\").set_index(\"record_id\")\n",
        "        sv[\"target\"] = sv[\"target\"].astype(\"string\")\n",
        "\n",
        "        pv = pd.read_csv(pos_val_csv)\n",
        "        # keep only ids that exist in split\n",
        "        pv = pv[pv[\"record_id\"].isin(sv.index)].copy()\n",
        "        pv[\"is_true_positive\"] = pv[\"record_id\"].map(sv[\"target\"].eq(pos_label)).fillna(False).astype(int)\n",
        "        pv.to_csv(pos_val_lbls, index=False)\n",
        "\n",
        "        print(f\"[BUILT] {pos_val_lbls}  (TP={pv['is_true_positive'].sum()}/{len(pv)})\")\n",
        "        return pos_val_lbls\n",
        "\n",
        "    # Nothing to build from → show what exists to help you spot the mismatch\n",
        "    _list_stage1(task_name)\n",
        "    raise FileNotFoundError(\n",
        "        f\"Missing {pos_val_lbls} and {pos_val_csv}. \"\n",
        "        f\"Re-run Stage-1 export for {task_name} or fix OUT.\"\n",
        "    )\n",
        "\n",
        "# 2) Show what’s actually on disk\n",
        "_list_stage1(\"AFvsREST\")\n",
        "_list_stage1(\"AFLvsREST\")\n",
        "\n",
        "# 3) Build/make sure the labeled files exist where Stage-2 expects them\n",
        "af_lbl  = _ensure_with_labels(\"AFvsREST\",  \"AF\")\n",
        "afl_lbl = _ensure_with_labels(\"AFLvsREST\", \"AFL\")\n",
        "\n",
        "# 4) Sanity echo\n",
        "print(\"\\nResolved files:\")\n",
        "print(\" AF labels:\", af_lbl, \"exists?\", af_lbl.exists())\n",
        "print(\" AFL labels:\", afl_lbl, \"exists?\", afl_lbl.exists())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8LAoJT7ZfP0",
        "outputId": "eda85aba-e343-4495-f83a-4c5a1437ae88"
      },
      "outputs": [],
      "source": [
        "# --- Pre-flight: unify OUT, fix double extensions, and get label paths ---\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "OUT = Path(\"/content/drive/MyDrive/HuBERT-ECG/processed_ptbxl_5s_100hz_fast\").resolve()\n",
        "print(\"Using OUT =\", OUT)\n",
        "\n",
        "def _fix_double_ext(stage1_dir: Path):\n",
        "    \"\"\"Rename ..._with_labels.csv.csv -> ..._with_labels.csv if present.\"\"\"\n",
        "    bad = stage1_dir / \"positives_val_with_labels.csv.csv\"\n",
        "    good = stage1_dir / \"positives_val_with_labels.csv\"\n",
        "    if bad.exists() and not good.exists():\n",
        "        bad.rename(good)\n",
        "        print(f\"[FIXED] Renamed {bad.name} -> {good.name}\")\n",
        "\n",
        "def _ensure_labeled(task_name: str, pos_label: str) -> Path:\n",
        "    \"\"\"Return path to positives_val_with_labels.csv, building it if needed.\"\"\"\n",
        "    stage1 = OUT / f\"stage1_{task_name}\"\n",
        "    assert stage1.exists(), f\"Missing folder: {stage1}\"\n",
        "\n",
        "    # fix common typo\n",
        "    _fix_double_ext(stage1)\n",
        "\n",
        "    lbl = stage1 / \"positives_val_with_labels.csv\"\n",
        "    raw = stage1 / \"positives_val.csv\"\n",
        "\n",
        "    if lbl.exists():\n",
        "        return lbl\n",
        "\n",
        "    if raw.exists():\n",
        "        # build labeled file from split_val.csv\n",
        "        sv = pd.read_csv(OUT/\"split_val.csv\").drop_duplicates(\"record_id\").set_index(\"record_id\")\n",
        "        sv[\"target\"] = sv[\"target\"].astype(\"string\")\n",
        "        pv = pd.read_csv(raw)\n",
        "        pv = pv[pv[\"record_id\"].isin(sv.index)].copy()\n",
        "        pv[\"is_true_positive\"] = pv[\"record_id\"].map(sv[\"target\"].eq(pos_label)).fillna(False).astype(int)\n",
        "        pv.to_csv(lbl, index=False)\n",
        "        print(f\"[BUILT] {lbl}  (TP={pv['is_true_positive'].sum()}/{len(pv)})\")\n",
        "        return lbl\n",
        "\n",
        "    # last resort: any file that matches the pattern\n",
        "    cand = list(stage1.glob(\"positives_val_with_labels*.csv\"))\n",
        "    if cand:\n",
        "        print(f\"[WARN] Using alternative file {cand[0].name}\")\n",
        "        return cand[0]\n",
        "\n",
        "    raise FileNotFoundError(\n",
        "        f\"Need {lbl} or {raw}. Re-run Stage-1 for {task_name} or fix OUT.\"\n",
        "    )\n",
        "\n",
        "af_lbl  = _ensure_labeled(\"AFvsREST\",  \"AF\")\n",
        "afl_lbl = _ensure_labeled(\"AFLvsREST\", \"AFL\")\n",
        "\n",
        "print(\"\\nResolved label files:\")\n",
        "print(\" AF :\", af_lbl,  \"exists?\", af_lbl.exists())\n",
        "print(\" AFL:\", afl_lbl, \"exists?\", afl_lbl.exists())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ko8p_mw3vpl",
        "outputId": "a394948d-d364-4b56-c8a6-3e57e4ac281f"
      },
      "outputs": [],
      "source": [
        "# @title Stage-2 Validator (τ-only; print CM for AFL)\n",
        "# ===========================\n",
        "# - AF: recall-targeted validator (τ-only)\n",
        "# - AFL: per-lead flutter features, recall-first τ (τ-only) + CM print\n",
        "# ===========================\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd, json, time, warnings\n",
        "from scipy.signal import butter, sosfiltfilt, welch, find_peaks\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, recall_score, precision_recall_curve\n",
        "import joblib\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "OUT = Path(\"/content/drive/MyDrive/HuBERT-ECG/processed_ptbxl_5s_100hz_fast\")\n",
        "FS = 100\n",
        "LEADS = {'I':0,'II':1,'III':2,'aVR':3,'aVL':4,'aVF':5,'V1':6,'V2':7,'V3':8,'V4':9,'V5':10,'V6':11}\n",
        "\n",
        "# ---------- AFL τ targets ----------\n",
        "AFL_MIN_PREC = 0.30\n",
        "AFL_MIN_RECALL = 0.85\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def bandpass(sig, fs, lo, hi, order=4):\n",
        "    ny = fs/2; hi = min(hi, ny*0.98)\n",
        "    sos = butter(order, [lo/ny, hi/ny], btype='band', output='sos')\n",
        "    return sosfiltfilt(sos, sig)\n",
        "\n",
        "def safe_load_np(path):\n",
        "    x = np.load(path)  # (T,12)\n",
        "    if x.ndim!=2 or x.shape[1]!=12:\n",
        "        raise ValueError(f\"bad shape {getattr(x,'shape',None)}\")\n",
        "    return x\n",
        "\n",
        "def pick_lead_for_rhythm(x):\n",
        "    for name in ['II','V1','V2']:\n",
        "        idx = LEADS[name]\n",
        "        if np.std(x[:,idx]) > 1e-4:\n",
        "            return x[:,idx]\n",
        "    return x[:, np.argmax(np.std(x, axis=0))]\n",
        "\n",
        "def detect_qrs(sig, fs=FS):\n",
        "    y = bandpass(sig, fs, 5, 20)\n",
        "    diff = np.diff(y, prepend=y[0])\n",
        "    sq = diff**2\n",
        "    win = max(int(0.12*fs), 1)\n",
        "    integ = np.convolve(sq, np.ones(win)/win, mode='same')\n",
        "    peaks, _ = find_peaks(integ, distance=int(0.25*fs), prominence=np.percentile(integ, 85))\n",
        "    return peaks\n",
        "\n",
        "def rr_features(qrs_idx, fs=FS):\n",
        "    if len(qrs_idx) < 3:\n",
        "        return dict(rr_mean=np.nan, rr_std=np.nan, rr_cv=np.nan, rr_rmssd=np.nan, rr_reg=np.nan)\n",
        "    rr = np.diff(qrs_idx)/fs\n",
        "    mean = float(np.mean(rr))\n",
        "    std = float(np.std(rr))\n",
        "    cv = float(std/(mean+1e-8))\n",
        "    rmssd = float(np.sqrt(np.mean(np.diff(rr)**2))) if len(rr)>2 else np.nan\n",
        "    reg = float(1.0/(1.0+cv))\n",
        "    return dict(rr_mean=mean, rr_std=std, rr_cv=cv, rr_rmssd=rmssd, rr_reg=reg)\n",
        "\n",
        "def pwave_metrics(sig, qrs_idx, fs=FS):\n",
        "    if len(qrs_idx) < 2:\n",
        "        return dict(p_en=np.nan, p_present=np.nan, p_to_qrs=np.nan)\n",
        "    bp = bandpass(sig, fs, 4, 15)\n",
        "    wins = []\n",
        "    for q in qrs_idx:\n",
        "        a = max(0, q - int(0.20*fs)); b = max(0, q - int(0.05*fs))\n",
        "        if b > a:\n",
        "            wins.append(bp[a:b])\n",
        "    if not wins:\n",
        "        return dict(p_en=np.nan, p_present=np.nan, p_to_qrs=np.nan)\n",
        "    en = np.array([np.mean(w**2) for w in wins])\n",
        "    p_en = float(np.median(en))\n",
        "    thr = max(np.percentile(en, 60), 1e-6)\n",
        "    p_present = float(np.mean(en > thr))\n",
        "    pband = bandpass(sig, fs, 3, 9)\n",
        "    p_peaks, _ = find_peaks(pband, distance=int(0.18*fs), prominence=np.percentile(np.abs(pband), 80))\n",
        "    p_to_qrs = float(len(p_peaks)/(len(qrs_idx)+1e-6))\n",
        "    return dict(p_en=p_en, p_present=p_present, p_to_qrs=p_to_qrs)\n",
        "\n",
        "def flutter_bandpower(sig, fs=FS):\n",
        "    f, Pxx = welch(bandpass(sig, fs, 1, 20), fs=fs, nperseg=min(256, len(sig)))\n",
        "    mask_lo = (f>=3) & (f<=8)\n",
        "    mask_all = (f>=1) & (f<=20)\n",
        "    bp_3_8 = float(np.trapz(Pxx[mask_lo], f[mask_lo])) if mask_lo.any() else 0.0\n",
        "    bp_1_20 = float(np.trapz(Pxx[mask_all], f[mask_all])) if mask_all.any() else 1e-12\n",
        "    return dict(flutter_ratio = bp_3_8 / bp_1_20)\n",
        "\n",
        "def extract_features(npy_path):\n",
        "    x = safe_load_np(npy_path)\n",
        "    lead = pick_lead_for_rhythm(x)\n",
        "    qrs = detect_qrs(lead, FS)\n",
        "    feats = {}\n",
        "    feats.update(rr_features(qrs, FS))\n",
        "    feats.update(pwave_metrics(lead, qrs, FS))\n",
        "    feats.update(flutter_bandpower(lead, FS))\n",
        "    feats.update({\n",
        "        \"amp_med\": float(np.median(lead)),\n",
        "        \"amp_iqr\": float(np.percentile(lead,75)-np.percentile(lead,25))\n",
        "    })\n",
        "    return feats\n",
        "\n",
        "# ----- AFL: per-lead flutter + harmonic + combined score + atrial peak -----\n",
        "def pwave_perbeat_metrics(sig, qrs_idx, fs=FS):\n",
        "    if len(qrs_idx) < 2:\n",
        "        return dict(p_en_med=np.nan, p_present_frac=np.nan, p_per_qrs_med=np.nan)\n",
        "    pband = bandpass(sig, fs, 3, 9)\n",
        "    en, p_perq = [], []\n",
        "    for q in qrs_idx:\n",
        "        a = max(0, q - int(0.20*fs)); b = max(0, q - int(0.06*fs))\n",
        "        if b <= a: continue\n",
        "        w = pband[a:b]\n",
        "        en.append(np.mean(w**2))\n",
        "        peaks, _ = find_peaks(w, distance=int(0.08*fs), prominence=np.percentile(np.abs(w), 80))\n",
        "        p_perq.append(len(peaks))\n",
        "    if not en:\n",
        "        return dict(p_en_med=np.nan, p_present_frac=np.nan, p_per_qrs_med=np.nan)\n",
        "    en = np.array(en)\n",
        "    p_en_med = float(np.median(en))\n",
        "    thr = max(np.percentile(en, 60), 1e-6)\n",
        "    p_present_frac = float(np.mean(en > thr))\n",
        "    p_per_qrs_med = float(np.median(p_perq)) if len(p_perq) else 0.0\n",
        "    return dict(p_en_med=p_en_med, p_present_frac=p_present_frac, p_per_qrs_med=p_per_qrs_med)\n",
        "\n",
        "def bandpower_ratio(sig, fs, band, refband):\n",
        "    f, Pxx = welch(bandpass(sig, fs, 1, 20), fs=fs, nperseg=min(256, len(sig)))\n",
        "    m1 = (f>=band[0]) & (f<=band[1])\n",
        "    m2 = (f>=refband[0]) & (f<=refband[1])\n",
        "    bp1 = float(np.trapz(Pxx[m1], f[m1])) if m1.any() else 0.0\n",
        "    bp2 = float(np.trapz(Pxx[m2], f[m2])) if m2.any() else 1e-12\n",
        "    return bp1 / bp2\n",
        "\n",
        "def pick_lead_for_qrs(x):\n",
        "    for name in ['II','V1','V2']:\n",
        "        idx = LEADS[name]\n",
        "        if np.std(x[:, idx]) > 1e-4:\n",
        "            return x[:, idx], name\n",
        "    return x[:, np.argmax(np.std(x, axis=0))], 'auto'\n",
        "\n",
        "def extract_features_AFL(npy_path):\n",
        "    x = safe_load_np(npy_path)\n",
        "    qrs_lead, _ = pick_lead_for_qrs(x)\n",
        "    qrs = detect_qrs(qrs_lead, FS)\n",
        "\n",
        "    feats = {}\n",
        "    feats.update(rr_features(qrs, FS))  # will be dropped later (de-emphasize) before modeling\n",
        "\n",
        "    CANDIDATE_LEADS = ['V1','II','V2','III','aVF']\n",
        "    perlead = []\n",
        "    for name in CANDIDATE_LEADS:\n",
        "        sig = x[:, LEADS[name]].astype(float).copy()\n",
        "        if len(qrs) > 0:\n",
        "            for q in qrs:\n",
        "                a = max(0, q - int(0.06*FS)); b = min(len(sig), q + int(0.06*FS))\n",
        "                sig[a:b] = 0.0\n",
        "        fratio = bandpower_ratio(sig, FS, (3, 8), (1, 20))\n",
        "        h812   = bandpower_ratio(sig, FS, (8, 12), (1, 20))\n",
        "        score  = fratio + 0.5*h812\n",
        "        perlead.append((name, fratio, h812, score))\n",
        "\n",
        "    best_name, best_fr, best_h2, best_score = max(perlead, key=lambda z: z[3])\n",
        "    best_sig = x[:, LEADS[best_name]]\n",
        "    feats.update(pwave_perbeat_metrics(best_sig, qrs, FS))\n",
        "    feats.update({\n",
        "        \"flutter_ratio_qrs_supp\": float(best_fr),\n",
        "        \"harmonic_8_12\": float(best_h2),\n",
        "        \"flutter_score\": float(best_score),\n",
        "        \"amp_med\": float(np.median(best_sig)),\n",
        "        \"amp_iqr\": float(np.percentile(best_sig,75)-np.percentile(best_sig,25)),\n",
        "    })\n",
        "    f, Pxx = welch(bandpass(best_sig, FS, 1, 20), fs=FS, nperseg=min(256,len(best_sig)))\n",
        "    mask = (f>=3.5) & (f<=7.0)\n",
        "    if mask.any():\n",
        "        i = np.argmax(Pxx[mask]); f_peak = float(f[mask][i]); pk = float(Pxx[mask][i])\n",
        "        lo = max(0, np.where(mask)[0][i]-5); hi = min(len(Pxx), np.where(mask)[0][i]+6)\n",
        "        baseline = 1e-12 + float(np.median(Pxx[lo:hi])); atrial_peak_h = pk / baseline\n",
        "    else:\n",
        "        f_peak, atrial_peak_h = 0.0, 0.0\n",
        "    feats.update({\"atrial_peak_h\": atrial_peak_h, \"atrial_peak_hz\": f_peak})\n",
        "    return feats\n",
        "\n",
        "# ---------- thresholds ----------\n",
        "def choose_tau_for_recall(y_true_bin, scores, target_recall=0.99):\n",
        "    thrs = np.unique(scores)[::-1]\n",
        "    best = 0.5\n",
        "    for t in thrs:\n",
        "        rec = recall_score(y_true_bin, (scores>=t).astype(int), zero_division=0)\n",
        "        if rec >= target_recall: best = float(t)\n",
        "        else: break\n",
        "    return best\n",
        "\n",
        "def choose_tau_for_precision_and_recall(y_true, scores, min_precision=0.25, min_recall=0.99):\n",
        "    P, R, T = precision_recall_curve(y_true, scores)\n",
        "    cands = [(p, r, t) for p, r, t in zip(P[:-1], R[:-1], T) if (p >= min_precision and r >= min_recall)]\n",
        "    if not cands:\n",
        "        cand_recall = [(p, r, t) for p, r, t in zip(P[:-1], R[:-1], T) if r >= min_recall]\n",
        "        if cand_recall:\n",
        "            cand_recall.sort(key=lambda z: (z[0], z[1])); return float(cand_recall[-1][2])\n",
        "        cand_prec = [(p, r, t) for p, r, t in zip(P[:-1], R[:-1], T) if p >= min_precision]\n",
        "        if cand_prec:\n",
        "            cand_prec.sort(key=lambda z: (z[1], z[0])); return float(cand_prec[-1][2])\n",
        "        return float(T[-1]) if len(T) else 0.5\n",
        "    cands.sort(key=lambda z: (2*z[0]*z[1]/(z[0]+z[1]+1e-9), z[1], z[0]))\n",
        "    return float(cands[-1][2])\n",
        "\n",
        "def choose_tau_recall_first(y_true, scores, min_precision=0.30, min_recall=0.85):\n",
        "    P, R, T = precision_recall_curve(y_true, scores)\n",
        "    both = [(p, r, t) for p, r, t in zip(P[:-1], R[:-1], T) if (p >= min_precision and r >= min_recall)]\n",
        "    if both:\n",
        "        both.sort(key=lambda z: (z[1], z[0])); return float(both[-1][2])\n",
        "    byR = [(p, r, t) for p, r, t in zip(P[:-1], R[:-1], T) if r >= min_recall]\n",
        "    if byR:\n",
        "        byR.sort(key=lambda z: (z[0], z[1])); return float(byR[-1][2])\n",
        "    byP = [(p, r, t) for p, r, t in zip(P[:-1], R[:-1], T) if p >= min_precision]\n",
        "    if byP:\n",
        "        byP.sort(key=lambda z: (z[1], z[0])); return float(byP[-1][2])\n",
        "    return float(T[-1]) if len(T) else 0.5\n",
        "\n",
        "def debug_coefficients(pipeline, feat_cols, title=\"AFL validator\"):\n",
        "    lr = pipeline.named_steps[\"logisticregression\"]; w = lr.coef_.ravel()\n",
        "    order = np.argsort(-np.abs(w))\n",
        "    print(f\"\\n[{title}] top-weighted features:\")\n",
        "    for i in order[:12]:\n",
        "        print(f\"  {feat_cols[i]:28s}  weight={w[i]: .4f}\")\n",
        "\n",
        "# ---------- core ----------\n",
        "def train_validator(task_name):\n",
        "    print(f\"\\n=== Train {task_name} validator ===\")\n",
        "    pos_val = OUT/f\"stage1_{task_name}/positives_val_with_labels.csv\"\n",
        "    assert pos_val.exists(), f\"Missing {pos_val}. Run Stage-1 first.\"\n",
        "    df = pd.read_csv(pos_val)\n",
        "    print(\"Label counts (VAL positives set):\", df[\"is_true_positive\"].value_counts(dropna=False).to_dict())\n",
        "\n",
        "    sv = pd.read_csv(OUT/\"split_val.csv\").set_index(\"record_id\")\n",
        "    df[\"out_path\"] = df[\"record_id\"].map(sv[\"out_path\"])\n",
        "    df = df.dropna(subset=[\"out_path\"]).reset_index(drop=True)\n",
        "\n",
        "    rows = []\n",
        "    use_afl = (task_name == \"AFLvsREST\")\n",
        "    for rid, p, y in zip(df[\"record_id\"], df[\"out_path\"], df[\"is_true_positive\"]):\n",
        "        try:\n",
        "            feats = extract_features_AFL(p) if use_afl else extract_features(p)\n",
        "            feats[\"record_id\"] = str(rid); feats[\"y\"] = int(y)\n",
        "            rows.append(feats)\n",
        "        except Exception as e:\n",
        "            print(\"Skip (VAL)\", rid, \":\", e)\n",
        "    feat = pd.DataFrame(rows).fillna(0.0)\n",
        "    if feat.empty: raise RuntimeError(\"No features extracted on VAL positives.\")\n",
        "\n",
        "    # AFL: de-emphasize RR by dropping them from the validator\n",
        "    if use_afl:\n",
        "        rr_drop = [\"rr_mean\",\"rr_std\",\"rr_cv\",\"rr_rmssd\",\"rr_reg\"]\n",
        "        feat = feat.drop(columns=[c for c in rr_drop if c in feat.columns])\n",
        "\n",
        "    yv = feat[\"y\"].to_numpy()\n",
        "    Xv = feat.drop(columns=[\"record_id\",\"y\"]).to_numpy()\n",
        "    feat_cols = [c for c in feat.columns if c not in [\"record_id\",\"y\"]]\n",
        "\n",
        "    clf = make_pipeline(\n",
        "        StandardScaler(with_mean=True),\n",
        "        LogisticRegression(max_iter=1000, penalty=\"l1\", solver=\"liblinear\",\n",
        "                           C=0.7, class_weight={0:1.0, 1:4.0}, random_state=42)\n",
        "    )\n",
        "    clf.fit(Xv, yv)\n",
        "    debug_coefficients(clf, feat_cols, title=task_name)\n",
        "\n",
        "    pv = clf.predict_proba(Xv)[:,1]\n",
        "    if use_afl:\n",
        "        tau = choose_tau_recall_first(yv, pv, min_precision=AFL_MIN_PREC, min_recall=AFL_MIN_RECALL)\n",
        "    else:\n",
        "        tau = choose_tau_for_precision_and_recall(yv, pv, min_precision=0.25, min_recall=0.99)\n",
        "\n",
        "    print(\"VAL (validator @τ):\")\n",
        "    print(classification_report(yv, (pv>=tau).astype(int), digits=4))\n",
        "    print(\"CM:\\n\", confusion_matrix(yv, (pv>=tau).astype(int)))\n",
        "    print(f\"Chosen τ: {tau:.4f}\")\n",
        "\n",
        "    joblib.dump(clf, OUT/f\"stage2_{task_name}_validator.joblib\")\n",
        "    with open(OUT/f\"stage2_{task_name}_meta.json\",\"w\") as f:\n",
        "        json.dump({\"threshold\": float(tau),\n",
        "                   \"features\": feat_cols,\n",
        "                   \"strategy\": (f\"precision≥{AFL_MIN_PREC}, recall≥{AFL_MIN_RECALL}\") if use_afl else \"recall≥0.99 (AF unchanged)\",\n",
        "                   \"created_at\": time.ctime()}, f, indent=2)\n",
        "    return clf, tau, feat_cols\n",
        "\n",
        "def _load_candidate_labels(task_name, split_name):\n",
        "    path = OUT/f\"stage1_{task_name}/positives_{split_name}_with_labels.csv\"\n",
        "    if not path.exists(): return {}\n",
        "    df = pd.read_csv(path)\n",
        "    return dict(zip(df[\"record_id\"].astype(str), df[\"is_true_positive\"].astype(int)))\n",
        "\n",
        "def apply_validator(task_name, clf, tau, feat_cols, split_name):\n",
        "    \"\"\"\n",
        "    τ-only for both AF and AFL.\n",
        "    For AFL, also print CM on the Stage-1 candidate set after τ.\n",
        "    \"\"\"\n",
        "    pos = OUT/f\"stage1_{task_name}/positives_{split_name}.csv\"\n",
        "    assert pos.exists(), f\"Missing {pos}\"\n",
        "    pos_ids = pd.read_csv(pos)\n",
        "    pos_ids[\"record_id\"] = pos_ids[\"record_id\"].astype(str)\n",
        "\n",
        "    split = pd.read_csv(OUT/f\"split_{split_name}.csv\").set_index(\"record_id\")\n",
        "    split.index = split.index.astype(str)\n",
        "\n",
        "    rows = []\n",
        "    use_afl = (task_name == \"AFLvsREST\")\n",
        "    for rid in pos_ids[\"record_id\"]:\n",
        "        if rid not in split.index: continue\n",
        "        p = split.loc[rid, \"out_path\"]\n",
        "        try:\n",
        "            feats = extract_features_AFL(p) if use_afl else extract_features(p)\n",
        "            feats[\"record_id\"] = str(rid)\n",
        "            rows.append(feats)\n",
        "        except Exception as e:\n",
        "            print(f\"Skip ({task_name} {split_name}) {rid}: {e}\")\n",
        "\n",
        "    out_dir = OUT/f\"stage2_{task_name}\"\n",
        "    Path(out_dir).mkdir(exist_ok=True)\n",
        "\n",
        "    if not rows:\n",
        "        pd.DataFrame(columns=[\"record_id\"]).to_csv(Path(out_dir)/f\"validator_passed_{split_name}.csv\", index=False)\n",
        "        print(f\"{task_name} {split_name.upper()}: kept 0/0 after τ.\")\n",
        "        if use_afl:\n",
        "            print(f\"{task_name} {split_name.upper()} CM after τ: empty.\")\n",
        "        return\n",
        "\n",
        "    F = pd.DataFrame(rows).fillna(0.0)\n",
        "    X = F.drop(columns=[\"record_id\"]).reindex(columns=feat_cols, fill_value=0).to_numpy()\n",
        "    prob = clf.predict_proba(X)[:,1]\n",
        "    keep_idx = np.where(prob >= tau)[0]\n",
        "    kept_ids = F.iloc[keep_idx][\"record_id\"].astype(str)\n",
        "\n",
        "    # save τ-only kept list\n",
        "    pd.DataFrame({\"record_id\": kept_ids}).to_csv(Path(out_dir)/f\"validator_passed_{split_name}.csv\", index=False)\n",
        "    print(f\"{task_name} {split_name.upper()}: kept {len(keep_idx)}/{len(F)} after τ (τ={tau:.3f}).\")\n",
        "\n",
        "    # For AFL, print CM on the candidate set after τ\n",
        "    if use_afl:\n",
        "        lab_map = _load_candidate_labels(task_name, split_name)\n",
        "        cand_ids = F[\"record_id\"].astype(str).tolist()\n",
        "        kept_set = set(kept_ids.tolist())\n",
        "        y_true = np.array([lab_map.get(r, 0) for r in cand_ids], dtype=int)\n",
        "        y_pred = np.array([1 if r in kept_set else 0 for r in cand_ids], dtype=int)\n",
        "        print(f\"{task_name} {split_name.upper()} CM after τ (on candidate set):\")\n",
        "        print(confusion_matrix(y_true, y_pred))\n",
        "        print(classification_report(y_true, y_pred, digits=4))\n",
        "\n",
        "# ===== run both tasks =====\n",
        "clf_af,  tau_af,  cols_af  = train_validator(\"AFvsREST\")\n",
        "clf_afl, tau_afl, cols_afl = train_validator(\"AFLvsREST\")\n",
        "\n",
        "apply_validator(\"AFvsREST\",  clf_af,  tau_af,  cols_af,  \"val\")\n",
        "apply_validator(\"AFvsREST\",  clf_af,  tau_af,  cols_af,  \"test\")\n",
        "apply_validator(\"AFLvsREST\", clf_afl, tau_afl, cols_afl, \"val\")\n",
        "apply_validator(\"AFLvsREST\", clf_afl, tau_afl, cols_afl, \"test\")\n",
        "\n",
        "print(\"\\nDone. Outputs:\")\n",
        "print(\" -\", (OUT/'stage2_AFvsREST').as_posix())\n",
        "print(\" -\", (OUT/'stage2_AFLvsREST').as_posix())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMWuJdCZh1Tt",
        "outputId": "c6a4b9c6-ec2d-4a45-d9c2-c19501f915ff"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === 00_export_predictions (run AFTER Stage-2) ===\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd, json, joblib\n",
        "from scipy.signal import butter, sosfiltfilt, welch, find_peaks\n",
        "\n",
        "# ---------- config ----------\n",
        "OUT = Path(\"/content/drive/MyDrive/HuBERT-ECG/processed_ptbxl_5s_100hz_fast\")\n",
        "FS = 100\n",
        "TASKS = [\"AFvsREST\", \"AFLvsREST\"]\n",
        "SPLITS = [\"val\", \"test\"]\n",
        "RNG = np.random.default_rng(42)\n",
        "\n",
        "LEADS = {'I':0,'II':1,'III':2,'aVR':3,'aVL':4,'aVF':5,'V1':6,'V2':7,'V3':8,'V4':9,'V5':10,'V6':11}\n",
        "\n",
        "# ---------- helpers (self-contained; no Stage-2 patch needed) ----------\n",
        "def bandpass(sig, fs, lo, hi, order=4):\n",
        "    ny = fs/2; hi = min(hi, ny*0.98)\n",
        "    sos = butter(order, [lo/ny, hi/ny], btype='band', output='sos')\n",
        "    return sosfiltfilt(sos, sig)\n",
        "\n",
        "def safe_load_np(path):\n",
        "    x = np.load(path)  # (T,12)\n",
        "    if x.ndim!=2 or x.shape[1]!=12:\n",
        "        raise ValueError(f\"bad shape {getattr(x,'shape',None)}\")\n",
        "    return x\n",
        "\n",
        "def pick_lead_for_rhythm_with_name(x):\n",
        "    for name in ['II','V1','V2']:\n",
        "        idx = LEADS[name]\n",
        "        if np.std(x[:,idx]) > 1e-4:\n",
        "            return x[:,idx], name\n",
        "    j = int(np.argmax(np.std(x, axis=0)))\n",
        "    name = list(LEADS.keys())[j]\n",
        "    return x[:, j], name\n",
        "\n",
        "def detect_qrs(sig, fs=FS):\n",
        "    y = bandpass(sig, fs, 5, 20)\n",
        "    diff = np.diff(y, prepend=y[0])\n",
        "    sq = diff**2\n",
        "    win = max(int(0.12*fs), 1)\n",
        "    integ = np.convolve(sq, np.ones(win)/win, mode='same')\n",
        "    peaks, _ = find_peaks(integ, distance=int(0.25*fs), prominence=np.percentile(integ, 85))\n",
        "    return peaks\n",
        "\n",
        "def rr_features(qrs_idx, fs=FS):\n",
        "    if len(qrs_idx) < 3:\n",
        "        return dict(rr_mean=np.nan, rr_std=np.nan, rr_cv=np.nan, rr_rmssd=np.nan, rr_reg=np.nan)\n",
        "    rr = np.diff(qrs_idx)/fs\n",
        "    mean = float(np.mean(rr))\n",
        "    std = float(np.std(rr))\n",
        "    cv = float(std/(mean+1e-8))\n",
        "    rmssd = float(np.sqrt(np.mean(np.diff(rr)**2))) if len(rr)>2 else np.nan\n",
        "    reg = float(1.0/(1.0+cv))\n",
        "    return dict(rr_mean=mean, rr_std=std, rr_cv=cv, rr_rmssd=rmssd, rr_reg=reg)\n",
        "\n",
        "def pwave_metrics(sig, qrs_idx, fs=FS):\n",
        "    if len(qrs_idx) < 2:\n",
        "        return dict(p_en=np.nan, p_present=np.nan, p_to_qrs=np.nan)\n",
        "    bp = bandpass(sig, fs, 4, 15)\n",
        "    wins = []\n",
        "    for q in qrs_idx:\n",
        "        a = max(0, q - int(0.20*fs)); b = max(0, q - int(0.05*fs))\n",
        "        if b > a: wins.append(bp[a:b])\n",
        "    if not wins:\n",
        "        return dict(p_en=np.nan, p_present=np.nan, p_to_qrs=np.nan)\n",
        "    en = np.array([np.mean(w**2) for w in wins])\n",
        "    p_en = float(np.median(en))\n",
        "    thr = max(np.percentile(en, 60), 1e-6)\n",
        "    p_present = float(np.mean(en > thr))\n",
        "    pband = bandpass(sig, fs, 3, 9)\n",
        "    p_peaks, _ = find_peaks(pband, distance=int(0.18*fs), prominence=np.percentile(np.abs(pband), 80))\n",
        "    p_to_qrs = float(len(p_peaks)/(len(qrs_idx)+1e-6))\n",
        "    return dict(p_en=p_en, p_present=p_present, p_to_qrs=p_to_qrs)\n",
        "\n",
        "def flutter_bandpower(sig, fs=FS):\n",
        "    f, Pxx = welch(bandpass(sig, fs, 1, 20), fs=fs, nperseg=min(256, len(sig)))\n",
        "    mask_lo = (f>=3) & (f<=8)\n",
        "    mask_all = (f>=1) & (f<=20)\n",
        "    bp_3_8 = float(np.trapz(Pxx[mask_lo], f[mask_lo])) if mask_lo.any() else 0.0\n",
        "    bp_1_20 = float(np.trapz(Pxx[mask_all], f[mask_all])) if mask_all.any() else 1e-12\n",
        "    return dict(flutter_ratio = bp_3_8 / bp_1_20)\n",
        "\n",
        "def bandpower_ratio(sig, fs, band, refband):\n",
        "    f, Pxx = welch(bandpass(sig, fs, 1, 20), fs=fs, nperseg=min(256, len(sig)))\n",
        "    m1 = (f>=band[0]) & (f<=band[1])\n",
        "    m2 = (f>=refband[0]) & (f<=refband[1])\n",
        "    bp1 = float(np.trapz(Pxx[m1], f[m1])) if m1.any() else 0.0\n",
        "    bp2 = float(np.trapz(Pxx[m2], f[m2])) if m2.any() else 1e-12\n",
        "    return bp1 / bp2\n",
        "\n",
        "def extract_features_AF(npy_path):\n",
        "    x = safe_load_np(npy_path)\n",
        "    lead, lead_name = pick_lead_for_rhythm_with_name(x)\n",
        "    qrs = detect_qrs(lead, FS)\n",
        "    feats = {}\n",
        "    feats.update(rr_features(qrs, FS))\n",
        "    feats.update(pwave_metrics(lead, qrs, FS))\n",
        "    feats.update(flutter_bandpower(lead, FS))  # carried for completeness\n",
        "    feats.update({\n",
        "        \"amp_med\": float(np.median(lead)),\n",
        "        \"amp_iqr\": float(np.percentile(lead,75)-np.percentile(lead,25)),\n",
        "        \"lead_name\": lead_name\n",
        "    })\n",
        "    return feats\n",
        "\n",
        "def extract_features_AFL(npy_path):\n",
        "    x = safe_load_np(npy_path).astype(float, copy=False)\n",
        "    # find QRS on rhythm lead, suppress around QRS, then evaluate flutter per lead\n",
        "    qrs_lead, _ = pick_lead_for_rhythm_with_name(x)\n",
        "    qrs = detect_qrs(qrs_lead, FS)\n",
        "    feats = {}\n",
        "    feats.update(rr_features(qrs, FS))  # may be dropped in your model; ok to keep here\n",
        "    CANDIDATE_LEADS = ['V1','II','V2','III','aVF']\n",
        "    perlead = []\n",
        "    for name in CANDIDATE_LEADS:\n",
        "        sig = x[:, LEADS[name]].copy()\n",
        "        if len(qrs) > 0:\n",
        "            for q in qrs:\n",
        "                a = max(0, q - int(0.06*FS)); b = min(len(sig), q + int(0.06*FS))\n",
        "                sig[a:b] = 0.0\n",
        "        fr = bandpower_ratio(sig, FS, (3, 8), (1, 20))\n",
        "        h2 = bandpower_ratio(sig, FS, (8, 12), (1, 20))\n",
        "        sc = fr + 0.5*h2\n",
        "        perlead.append((name, fr, h2, sc))\n",
        "    best_name, best_fr, best_h2, best_sc = max(perlead, key=lambda z: z[3])\n",
        "    best_sig = x[:, LEADS[best_name]]\n",
        "    feats.update({\n",
        "        \"flutter_ratio_qrs_supp\": float(best_fr),\n",
        "        \"harmonic_8_12\": float(best_h2),\n",
        "        \"flutter_score\": float(best_sc),\n",
        "        \"amp_med\": float(np.median(best_sig)),\n",
        "        \"amp_iqr\": float(np.percentile(best_sig,75)-np.percentile(best_sig,25)),\n",
        "        \"best_lead\": best_name\n",
        "    })\n",
        "    return feats\n",
        "\n",
        "def _load_candidate_labels(task_name, split_name):\n",
        "    path = OUT/f\"stage1_{task_name}/positives_{split_name}_with_labels.csv\"\n",
        "    if not path.exists(): return {}\n",
        "    df = pd.read_csv(path)\n",
        "    return dict(zip(df[\"record_id\"].astype(str), df[\"is_true_positive\"].astype(int)))\n",
        "\n",
        "def export_predictions_for_task(task_name):\n",
        "    save_dir = OUT/f\"analysis_exports/{task_name}\"\n",
        "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    clf = joblib.load(OUT/f\"stage2_{task_name}_validator.joblib\")\n",
        "    with open(OUT/f\"stage2_{task_name}_meta.json\",\"r\") as f:\n",
        "        meta = json.load(f)\n",
        "    tau = float(meta[\"threshold\"])\n",
        "    feat_cols = meta[\"features\"]\n",
        "\n",
        "    all_rows = []\n",
        "    for split_name in SPLITS:\n",
        "        pos_csv = OUT/f\"stage1_{task_name}/positives_{split_name}.csv\"\n",
        "        if not pos_csv.exists():\n",
        "            print(f\"[{task_name}] Missing {pos_csv}, skip.\")\n",
        "            continue\n",
        "        pos_ids = pd.read_csv(pos_csv)\n",
        "        pos_ids[\"record_id\"] = pos_ids[\"record_id\"].astype(str)\n",
        "\n",
        "        split = pd.read_csv(OUT/f\"split_{split_name}.csv\").set_index(\"record_id\")\n",
        "        split.index = split.index.astype(str)\n",
        "\n",
        "        lab_map = _load_candidate_labels(task_name, split_name)\n",
        "\n",
        "        rows = []\n",
        "        for rid in pos_ids[\"record_id\"]:\n",
        "            if rid not in split.index:\n",
        "                continue\n",
        "            npy_path = split.loc[rid, \"out_path\"]\n",
        "            try:\n",
        "                if task_name == \"AFLvsREST\":\n",
        "                    feats = extract_features_AFL(npy_path); lead = feats.get(\"best_lead\",\"NA\")\n",
        "                else:\n",
        "                    feats = extract_features_AF(npy_path);  lead = feats.get(\"lead_name\",\"NA\")\n",
        "                row = {\"record_id\": str(rid), \"split\": split_name, \"task\": task_name, \"lead\": lead}\n",
        "                row.update(feats)\n",
        "                rows.append(row)\n",
        "            except Exception as e:\n",
        "                print(f\"Skip ({task_name} {split_name}) {rid}: {e}\")\n",
        "\n",
        "        if not rows:\n",
        "            continue\n",
        "\n",
        "        F = pd.DataFrame(rows).fillna(0.0)\n",
        "        X = F.drop(columns=[\"record_id\",\"split\",\"task\",\"lead\"], errors=\"ignore\") \\\n",
        "             .reindex(columns=feat_cols, fill_value=0.0).to_numpy()\n",
        "        prob = clf.predict_proba(X)[:,1]\n",
        "        pred = (prob >= tau).astype(int)\n",
        "        F[\"score\"] = prob\n",
        "        F[\"pred_label\"] = pred\n",
        "        F[\"true_label\"] = F[\"record_id\"].map(lab_map).fillna(0).astype(int)\n",
        "        F.to_csv(Path(save_dir)/f\"predictions_{split_name}.csv\", index=False)\n",
        "        all_rows.append(F)\n",
        "\n",
        "    if not all_rows:\n",
        "        print(f\"[{task_name}] No rows exported.\"); return\n",
        "\n",
        "    DF = pd.concat(all_rows, ignore_index=True)\n",
        "    DF.to_csv(Path(save_dir)/\"predictions_ALL.csv\", index=False)\n",
        "\n",
        "    # tag TP/FP among predicted positives\n",
        "    PP = DF[DF[\"pred_label\"]==1].copy()\n",
        "    PP[\"tp_flag\"] = (PP[\"true_label\"]==1).astype(int)\n",
        "    PP[\"fp_flag\"] = (PP[\"true_label\"]==0).astype(int)\n",
        "\n",
        "    def stratified_sample(df, n_total=20):\n",
        "        if df.empty: return df\n",
        "        df = df.copy()\n",
        "        # quartiles may collapse if few rows ⇒ use duplicates=\"drop\"\n",
        "        df[\"quartile\"] = pd.qcut(df[\"score\"], q=4, labels=[1,2,3,4], duplicates=\"drop\")\n",
        "        want = min(n_total, len(df))\n",
        "        parts = []\n",
        "        if \"quartile\" in df:\n",
        "            per_q = max(1, want // df[\"quartile\"].nunique())\n",
        "            for _, sub in df.groupby(\"quartile\"):\n",
        "                parts.append(sub.sample(n=min(per_q, len(sub)), random_state=42))\n",
        "        S = pd.concat(parts, ignore_index=True) if parts else df.sample(n=want, random_state=42)\n",
        "        if len(S) < want:\n",
        "            S = pd.concat([S, df.drop(S.index).sample(n=want-len(S), random_state=42)], ignore_index=True)\n",
        "        return S\n",
        "\n",
        "    samp_dir = OUT/\"analysis\"/\"samples\"; samp_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if \"AFvsREST\" in DF[\"task\"].unique():\n",
        "        af_pos = PP[PP[\"task\"]==\"AFvsREST\"]\n",
        "        af_TP = stratified_sample(af_pos[af_pos[\"true_label\"]==1], n_total=20)\n",
        "        af_FP = stratified_sample(af_pos[af_pos[\"true_label\"]==0], n_total=20)\n",
        "        af_TP.to_csv(samp_dir/\"AF_TP.csv\", index=False)\n",
        "        af_FP.to_csv(samp_dir/\"AF_FP.csv\", index=False)\n",
        "\n",
        "    if \"AFLvsREST\" in DF[\"task\"].unique():\n",
        "        afl_pos = PP[PP[\"task\"]==\"AFLvsREST\"]\n",
        "        afl_TP = stratified_sample(afl_pos[afl_pos[\"true_label\"]==1], n_total=20)\n",
        "        afl_FP = stratified_sample(afl_pos[afl_pos[\"true_label\"]==0], n_total=20)\n",
        "        afl_TP.to_csv(samp_dir/\"AFL_TP.csv\", index=False)\n",
        "        afl_FP.to_csv(samp_dir/\"AFL_FP.csv\", index=False)\n",
        "\n",
        "    print(\"✓ Saved:\", (OUT/\"analysis\"/\"samples\").as_posix())\n",
        "    print(\"  - AF_TP.csv, AF_FP.csv, AFL_TP.csv, AFL_FP.csv\")\n",
        "    print(\"  - Full per-split exports in:\", (OUT/\"analysis_exports\").as_posix())\n",
        "\n",
        "# run both tasks\n",
        "for task in TASKS:\n",
        "    export_predictions_for_task(task)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOdPOkmbE0DV",
        "outputId": "efb04d23-c30a-4c2d-dce4-3ec155509f4d"
      },
      "outputs": [],
      "source": [
        "!pip -q install neurokit2 biosppy shap scipy scikit-learn matplotlib pandas numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sr71mqR6E8vI"
      },
      "outputs": [],
      "source": [
        "# === common_config.py (cell) ===\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "from scipy.signal import butter, sosfiltfilt, welch, find_peaks\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings, json, joblib, math\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ---- paths ----\n",
        "OUT = Path(\"/content/drive/MyDrive/HuBERT-ECG/processed_ptbxl_5s_100hz_fast\")\n",
        "SAMPLES_DIR = OUT/\"analysis\"/\"samples\"\n",
        "FIGS_DIR = OUT/\"figs\"\n",
        "AN_DIR = OUT/\"analysis\"\n",
        "FS = 100\n",
        "\n",
        "LEADS = {'I':0,'II':1,'III':2,'aVR':3,'aVL':4,'aVF':5,'V1':6,'V2':7,'V3':8,'V4':9,'V5':10,'V6':11}\n",
        "\n",
        "FIGS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "(AN_DIR/\"outputs\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---- basic helpers ----\n",
        "def safe_load_np(path):\n",
        "    x = np.load(path)  # (T,12)\n",
        "    if x.ndim!=2 or x.shape[1]!=12:\n",
        "        raise ValueError(f\"bad shape {getattr(x,'shape',None)} for {path}\")\n",
        "    return x.astype(float, copy=False)\n",
        "\n",
        "def bandpass(sig, fs, lo, hi, order=4):\n",
        "    ny = fs/2; hi = min(hi, ny*0.98)\n",
        "    sos = butter(order, [lo/ny, hi/ny], btype='band', output='sos')\n",
        "    return sosfiltfilt(sos, sig)\n",
        "\n",
        "def pick_lead_with_name(x, pref=('II','V1','V2')):\n",
        "    for name in pref:\n",
        "        idx = LEADS[name]\n",
        "        if np.std(x[:,idx]) > 1e-4:\n",
        "            return x[:,idx], name\n",
        "    j = int(np.argmax(np.std(x, axis=0)))\n",
        "    name = list(LEADS.keys())[j]\n",
        "    return x[:, j], name\n",
        "\n",
        "def find_rpeaks_dual(sig, fs=FS):\n",
        "    \"\"\"Return: best_r (np.array), aux dict with details from neurokit + pantompkins.\"\"\"\n",
        "    import neurokit2 as nk\n",
        "    # A) NeuroKit detector\n",
        "    r_nk = np.array([])\n",
        "    try:\n",
        "        _, info = nk.ecg_peaks(sig, sampling_rate=fs, method=\"neurokit\")\n",
        "        r_nk = info.get(\"ECG_R_Peaks\", np.array([]))\n",
        "    except Exception:\n",
        "        pass\n",
        "    # B) Pan-Tompkins (fallback)\n",
        "    r_pt = np.array([])\n",
        "    try:\n",
        "        _, info2 = nk.ecg_peaks(sig, sampling_rate=fs, method=\"pantompkins1985\")\n",
        "        r_pt = info2.get(\"ECG_R_Peaks\", np.array([]))\n",
        "    except Exception:\n",
        "        pass\n",
        "    # choose the one with more beats\n",
        "    best = r_nk if len(r_nk) >= len(r_pt) else r_pt\n",
        "    return np.asarray(best, dtype=int), {\"r_nk\": r_nk, \"r_pt\": r_pt}\n",
        "\n",
        "def rr_metrics(r, fs=FS):\n",
        "    if len(r) < 3:\n",
        "        return dict(meanRR=np.nan, sdRR=np.nan, cvRR=np.nan, rmssd=np.nan, reg_score=np.nan,\n",
        "                    n_rr=len(r)-1, pct_abnRR=np.nan, ectopics_removed=np.nan)\n",
        "    rr = np.diff(r)/fs\n",
        "    meanRR = float(np.mean(rr)); sdRR = float(np.std(rr, ddof=1))\n",
        "    cvRR   = float(sdRR/(meanRR+1e-9))\n",
        "    d = np.diff(rr); rmssd = float(np.sqrt(np.mean(d**2))) if len(d)>0 else np.nan\n",
        "    reg = float(1 - (rmssd/(meanRR+1e-9)))  # crude \"regularity\"\n",
        "    # abnormal RR flags (<=0.3s or >=2.0s)\n",
        "    abn = np.logical_or(rr<=0.3, rr>=2.0)\n",
        "    pct_abnRR = float(100*np.mean(abn))\n",
        "    return dict(meanRR=meanRR, sdRR=sdRR, cvRR=cvRR, rmssd=rmssd, reg_score=reg,\n",
        "                n_rr=int(len(rr)), pct_abnRR=pct_abnRR, ectopics_removed=np.nan)\n",
        "\n",
        "def p_window_proxies(sig, r, fs=FS, pre_ms=200, post_ms=60):\n",
        "    \"\"\"Compute P proxies around -200..-60ms of each R.\"\"\"\n",
        "    if len(r)<2:\n",
        "        return dict(p_energy=np.nan, p_presence=np.nan, p_to_qrs=np.nan, p_flatness=np.nan)\n",
        "    pband = bandpass(sig, fs, 4, 15)\n",
        "    wins = []\n",
        "    pre  = int(pre_ms*fs/1000); post = int(post_ms*fs/1000)\n",
        "    for q in r:\n",
        "        a = max(0, q-pre); b = max(0, q-post)\n",
        "        if b>a and b<=len(pband): wins.append(pband[a:b])\n",
        "    if not wins:\n",
        "        return dict(p_energy=np.nan, p_presence=np.nan, p_to_qrs=np.nan, p_flatness=np.nan)\n",
        "    # equalize\n",
        "    L = min(len(w) for w in wins)\n",
        "    W = np.vstack([w[:L] for w in wins])\n",
        "    avg = np.median(W, axis=0)\n",
        "    # spectral flatness proxy = var(avg) (simple, robust)\n",
        "    p_flat = float(np.var(avg))\n",
        "    # energy\n",
        "    p_energy = float(np.median(np.mean(W**2, axis=1)))\n",
        "    # presence ratio\n",
        "    peaks = np.max(np.abs(W), axis=1)\n",
        "    thr = np.percentile(peaks, 60)\n",
        "    p_presence = float(np.mean(peaks > thr))\n",
        "    # P:QRS ratio (use segmentwise QRS proxy from raw)\n",
        "    qrs_pk = np.percentile(np.abs(sig), 99) + 1e-9\n",
        "    p_to_qrs = float(np.median(peaks)/qrs_pk)\n",
        "    return dict(p_energy=p_energy, p_presence=p_presence, p_to_qrs=p_to_qrs, p_flatness=p_flat)\n",
        "\n",
        "def psd_features(sig, fs=FS):\n",
        "    \"\"\"3–8 Hz ratio, and ridge prominence ~4–5 Hz.\"\"\"\n",
        "    f, Pxx = welch(bandpass(sig, fs, 1, 20), fs=fs, nperseg=min(512, len(sig)))\n",
        "    band = (f>=3)&(f<=8); total = (f>=1)&(f<=20)\n",
        "    bandpow = float(np.trapz(Pxx[band], f[band])) if np.any(band) else 0.0\n",
        "    totpow  = float(np.trapz(Pxx[total], f[total])) if np.any(total) else 1e-12\n",
        "    ratio   = bandpow/totpow\n",
        "    ridge = (f>=4)&(f<=5.5)\n",
        "    peak = float(np.max(Pxx[ridge])) if np.any(ridge) else 0.0\n",
        "    # local baseline around peak:\n",
        "    if np.any(ridge):\n",
        "        i = np.argmax(Pxx[ridge]); idx = np.where(ridge)[0][i]\n",
        "        lo, hi = max(0, idx-5), min(len(Pxx), idx+6)\n",
        "        baseline = float(np.median(Pxx[lo:hi])) + 1e-12\n",
        "        prom = peak/baseline\n",
        "        fpk = float(f[idx])\n",
        "    else:\n",
        "        prom, fpk = 0.0, 0.0\n",
        "    return dict(flutter_ratio=ratio, ridge_4_5=prom, ridge_hz=fpk)\n",
        "\n",
        "def quality_metrics(sig, fs=FS):\n",
        "    f, Pxx = welch(sig, fs=fs, nperseg=min(512, len(sig)))\n",
        "    base = (f>=0.2)&(f<=0.5); mid=(f>=5)&(f<=15)\n",
        "    basepow = float(np.trapz(Pxx[base], f[base])) if np.any(base) else 0.0\n",
        "    midpow  = float(np.trapz(Pxx[mid], f[mid])) if np.any(mid) else 1e-12\n",
        "    baseline_wander = basepow/midpow\n",
        "    snr_proxy = float(np.var(sig)/(np.var(np.diff(sig))+1e-12))\n",
        "    sat = float(np.mean(np.isclose(sig, sig.max(), atol=1e-7) | np.isclose(sig, sig.min(), atol=1e-7)))\n",
        "    return dict(baseline_wander=baseline_wander, snr_proxy=snr_proxy, pct_saturation=sat)\n",
        "\n",
        "def load_split():\n",
        "    sv = pd.read_csv(OUT/\"split_val.csv\").set_index(\"record_id\"); sv.index = sv.index.astype(str)\n",
        "    st = pd.read_csv(OUT/\"split_test.csv\").set_index(\"record_id\"); st.index = st.index.astype(str)\n",
        "    return {\"val\": sv, \"test\": st}\n",
        "\n",
        "def locate_npy(record_id, split_maps):\n",
        "    for s, df in split_maps.items():\n",
        "        if str(record_id) in df.index:\n",
        "            return s, df.loc[str(record_id), \"out_path\"]\n",
        "    return None, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "DxQ5FP75FA2K",
        "outputId": "a1ebfd12-1a12-43a4-fc82-ee9d7668f8fe"
      },
      "outputs": [],
      "source": [
        "# === 01_recompute_features.py (cell) ===\n",
        "split_maps = load_split()\n",
        "\n",
        "def recompute_row(row, task):\n",
        "    rid = str(row[\"record_id\"])\n",
        "    _, npy_path = locate_npy(rid, split_maps)\n",
        "    if npy_path is None:\n",
        "        return None\n",
        "    x = safe_load_np(npy_path)\n",
        "    # Lead choice: rhythm lead for AF features; V1/II/V2 tested for AFL flutter\n",
        "    lead_sig, lead_name = pick_lead_with_name(x)\n",
        "    # R-peaks (dual)\n",
        "    r, aux = find_rpeaks_dual(lead_sig, FS)\n",
        "    feats = {}\n",
        "    feats.update(rr_metrics(r, FS))\n",
        "    feats.update(quality_metrics(lead_sig, FS))\n",
        "    # AF focus P-window proxies (still compute for AFL to inspect)\n",
        "    feats.update(p_window_proxies(lead_sig, r, FS))\n",
        "    # Flutter PSD: evaluate also on V1 for AFL evidence\n",
        "    feats.update(psd_features(lead_sig, FS))\n",
        "    # Best ridge on V1 if available (AFL helpful)\n",
        "    if \"V1\" in LEADS:\n",
        "        sig_v1 = x[:, LEADS[\"V1\"]]\n",
        "        v1_psd = psd_features(sig_v1, FS)\n",
        "        feats[\"flutter_ratio_v1\"] = v1_psd[\"flutter_ratio\"]\n",
        "        feats[\"ridge_4_5_v1\"]     = v1_psd[\"ridge_4_5\"]\n",
        "        feats[\"ridge_hz_v1\"]      = v1_psd[\"ridge_hz\"]\n",
        "    feats[\"lead_used\"] = lead_name\n",
        "    feats[\"record_id\"] = rid\n",
        "    feats[\"task\"] = task\n",
        "    feats[\"split_src\"] = row.get(\"split\", \"\")\n",
        "    feats[\"tp_flag\"] = int(row.get(\"true_label\", 0)==1 and row.get(\"pred_label\",1)==1)\n",
        "    feats[\"fp_flag\"] = int(row.get(\"true_label\", 0)==0 and row.get(\"pred_label\",1)==1)\n",
        "    feats[\"score\"]   = float(row.get(\"score\", np.nan))\n",
        "    return feats\n",
        "\n",
        "def recompute_for_task(task):\n",
        "    # Read the sampled lists\n",
        "    tp_csv = SAMPLES_DIR/f\"{'AF' if task=='AFvsREST' else 'AFL'}_TP.csv\"\n",
        "    fp_csv = SAMPLES_DIR/f\"{'AF' if task=='AFvsREST' else 'AFL'}_FP.csv\"\n",
        "    tp = pd.read_csv(tp_csv) if tp_csv.exists() else pd.DataFrame()\n",
        "    fp = pd.read_csv(fp_csv) if fp_csv.exists() else pd.DataFrame()\n",
        "    df = pd.concat([tp, fp], ignore_index=True)\n",
        "    rows = []\n",
        "    for _, r in df.iterrows():\n",
        "        try:\n",
        "            out = recompute_row(r, task)\n",
        "            if out is not None: rows.append(out)\n",
        "        except Exception as e:\n",
        "            print(\"skip\", r.get(\"record_id\"), e)\n",
        "    F = pd.DataFrame(rows)\n",
        "    outp = AN_DIR/\"outputs\"/f\"features_recomputed_{task}.csv\"\n",
        "    F.to_csv(outp, index=False)\n",
        "    print(\"Saved\", outp, F.shape)\n",
        "    return F\n",
        "\n",
        "F_AF  = recompute_for_task(\"AFvsREST\")\n",
        "F_AFL = recompute_for_task(\"AFLvsREST\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "137b2d75",
        "outputId": "90af2824-e6da-4eb3-cf4b-011c38d00bf0"
      },
      "outputs": [],
      "source": [
        "# === 01_recompute_features.py (cell) ===\n",
        "split_maps = load_split()\n",
        "\n",
        "def recompute_row(row, task):\n",
        "    rid = str(row[\"record_id\"])\n",
        "    _, npy_path = locate_npy(rid, split_maps)\n",
        "    if npy_path is None:\n",
        "        return None\n",
        "    x = safe_load_np(npy_path)\n",
        "    # Lead choice: rhythm lead for AF features; V1/II/V2 tested for AFL flutter\n",
        "    lead_sig, lead_name = pick_lead_with_name(x)\n",
        "    # R-peaks (dual)\n",
        "    r, aux = find_rpeaks_dual(lead_sig, FS)\n",
        "    feats = {}\n",
        "    feats.update(rr_metrics(r, FS))\n",
        "    feats.update(quality_metrics(lead_sig, FS))\n",
        "    # AF focus P-window proxies (still compute for AFL to inspect)\n",
        "    feats.update(p_window_proxies(lead_sig, r, FS))\n",
        "    # Flutter PSD: evaluate also on V1 for AFL evidence\n",
        "    feats.update(psd_features(lead_sig, FS))\n",
        "    # Best ridge on V1 if available (AFL helpful)\n",
        "    if \"V1\" in LEADS:\n",
        "        sig_v1 = x[:, LEADS[\"V1\"]]\n",
        "        v1_psd = psd_features(sig_v1, FS)\n",
        "        feats[\"flutter_ratio_v1\"] = v1_psd[\"flutter_ratio\"]\n",
        "        feats[\"ridge_4_5_v1\"]     = v1_psd[\"ridge_4_5\"]\n",
        "        feats[\"ridge_hz_v1\"]      = v1_psd[\"ridge_hz\"]\n",
        "    feats[\"lead_used\"] = lead_name\n",
        "    feats[\"record_id\"] = rid\n",
        "    feats[\"task\"] = task\n",
        "    feats[\"split_src\"] = row.get(\"split\", \"\")\n",
        "    feats[\"tp_flag\"] = int(row.get(\"true_label\", 0)==1 and row.get(\"pred_label\",1)==1)\n",
        "    feats[\"fp_flag\"] = int(row.get(\"true_label\", 0)==0 and row.get(\"pred_label\",1)==1)\n",
        "    feats[\"score\"]   = float(row.get(\"score\", np.nan))\n",
        "    return feats\n",
        "\n",
        "def recompute_for_task(task):\n",
        "    # Read the sampled lists\n",
        "    tp_csv = SAMPLES_DIR/f\"{'AF' if task=='AFvsREST' else 'AFL'}_TP.csv\"\n",
        "    fp_csv = SAMPLES_DIR/f\"{'AF' if task=='AFvsREST' else 'AFL'}_FP.csv\"\n",
        "    tp = pd.read_csv(tp_csv) if tp_csv.exists() else pd.DataFrame()\n",
        "    fp = pd.read_csv(fp_csv) if fp_csv.exists() else pd.DataFrame()\n",
        "    df = pd.concat([tp, fp], ignore_index=True)\n",
        "    rows = []\n",
        "    for _, r in df.iterrows():\n",
        "        try:\n",
        "            out = recompute_row(r, task)\n",
        "            if out is not None: rows.append(out)\n",
        "        except Exception as e:\n",
        "            print(\"skip\", r.get(\"record_id\"), e)\n",
        "    F = pd.DataFrame(rows)\n",
        "    outp = AN_DIR/\"outputs\"/f\"features_recomputed_{task}.csv\"\n",
        "    F.to_csv(outp, index=False)\n",
        "    print(\"Saved\", outp, F.shape)\n",
        "    return F\n",
        "\n",
        "F_AF  = recompute_for_task(\"AFvsREST\")\n",
        "F_AFL = recompute_for_task(\"AFLvsREST\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "Ln8AsA9ZG6aJ",
        "outputId": "0429f33f-83f4-412f-b282-9514dbd99b9c"
      },
      "outputs": [],
      "source": [
        "# === 02_case_panels.py (cell) ===\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.dpi\"]=120\n",
        "\n",
        "def plot_case_panel(record_id, task):\n",
        "    sname, npy_path = locate_npy(str(record_id), split_maps)\n",
        "    if npy_path is None:\n",
        "        print(\"record not found\", record_id); return\n",
        "    x = safe_load_np(npy_path)\n",
        "    lead_sig, lead_name = pick_lead_with_name(x)\n",
        "    r, aux = find_rpeaks_dual(lead_sig, FS)\n",
        "    rr = np.diff(r)/FS if len(r)>1 else np.array([])\n",
        "    # P-window overlay data\n",
        "    pre_ms, post_ms = 200, 60\n",
        "    pband = bandpass(lead_sig, FS, 4, 15)\n",
        "    wins = []\n",
        "    for q in r:\n",
        "        a = max(0, q-int(pre_ms*FS/1000)); b = max(0, q-int(post_ms*FS/1000))\n",
        "        if b>a and b<=len(pband): wins.append(pband[a:b])\n",
        "    avg = np.median(np.vstack([w[:min(map(len,wins))] for w in wins]), axis=0) if len(wins)>1 else None\n",
        "    # PSD\n",
        "    f, Pxx = welch(bandpass(lead_sig, FS, 1, 20), fs=FS, nperseg=min(512, len(lead_sig)))\n",
        "\n",
        "    # ---- plot ----\n",
        "    Tstrip = min(len(lead_sig)/FS, 12.0)  # up to 12 s\n",
        "    Nstrip = int(Tstrip*FS)\n",
        "    t = np.arange(Nstrip)/FS\n",
        "\n",
        "    fig = plt.figure(figsize=(10,8))\n",
        "    ax1 = plt.subplot(4,1,1)\n",
        "    ax1.plot(t, lead_sig[:Nstrip])\n",
        "    ax1.scatter(r[r<Nstrip]/FS, lead_sig[r[r<Nstrip]], s=12)\n",
        "    ax1.set_title(f\"{task} | {record_id} | Lead {lead_name} | strip+R\")\n",
        "    ax1.set_xlim(0, Tstrip)\n",
        "\n",
        "    # ax2 = plt.subplot(4,1,2)\n",
        "    # if rr.size:\n",
        "    #     ax2.plot(np.arange(len(rr)), rr)\n",
        "    # ax2.set_title(f\"RR tachogram (mean={rr.mean():.3f}s, sd={rr.std(ddof=1) if rr.size else float('nan'):.3f}, CV={rr.std(ddof=1)/rr.mean() if rr.size else float('nan'):.3f})\")\n",
        "    # ax2.set_xlabel(\"beat\"); ax2.set_ylabel(\"RR (s)\")\n",
        "\n",
        "    N = 2  # show every 2nd label\n",
        "    ticks = np.arange(0, len(rr), N)\n",
        "    ax2.set_xticks(ticks)\n",
        "    ax2.set_xticklabels([f\"{i+1}–{i+2}\" for i in ticks], rotation=0)\n",
        "\n",
        "    ax3 = plt.subplot(4,1,3)\n",
        "    if avg is not None:\n",
        "        tt = np.arange(len(avg))/FS\n",
        "        ax3.plot(tt, avg)\n",
        "        # shade P-window\n",
        "        ax3.axvspan((pre_ms-200)/1000, (pre_ms-60)/1000, alpha=0.2)\n",
        "    ax3.set_title(\"P-window overlay (median beat, 4–15 Hz)\")\n",
        "\n",
        "    ax4 = plt.subplot(4,1,4)\n",
        "    ax4.plot(f, Pxx)\n",
        "    ax4.axvspan(3,8, alpha=0.15)\n",
        "    ax4.axvline(4.5, ls=\"--\")\n",
        "    ax4.set_xlim(0, 20); ax4.set_xlabel(\"Hz\"); ax4.set_title(\"PSD (1–20 Hz) with 3–8 Hz band\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "    subdir = FIGS_DIR/\"casepanels\"/(\"AF\" if task==\"AFvsREST\" else \"AFL\")\n",
        "    subdir.mkdir(parents=True, exist_ok=True)\n",
        "    path = subdir/f\"{record_id}.png\"\n",
        "    fig.savefig(path, bbox_inches=\"tight\")\n",
        "    plt.close(fig)\n",
        "    return path\n",
        "\n",
        "def batch_panels(F, task, max_n=80):\n",
        "    ids = list(F[\"record_id\"].unique())[:max_n]\n",
        "    out = []\n",
        "    for rid in ids:\n",
        "        p = plot_case_panel(rid, task)\n",
        "        if p: out.append(p)\n",
        "    print(\"Saved\", len(out), \"panels to\", (FIGS_DIR/\"casepanels\").as_posix())\n",
        "\n",
        "batch_panels(F_AF, \"AFvsREST\")\n",
        "batch_panels(F_AFL, \"AFLvsREST\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpRZ1SAPIOy7"
      },
      "outputs": [],
      "source": [
        "# Show a few saved panels inline\n",
        "from pathlib import Path\n",
        "from IPython.display import display, Image\n",
        "\n",
        "base = Path(\"/content/drive/MyDrive/HuBERT-ECG/processed_ptbxl_5s_100hz_fast/figs/casepanels\")\n",
        "for sub in [\"AF\",\"AFL\"]:\n",
        "    paths = sorted((base/sub).glob(\"*.png\"))[:6]  # first 6\n",
        "    print(f\"\\n{sub} panels ({len(paths)} shown):\")\n",
        "    for p in paths:\n",
        "        display(Image(filename=str(p)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "ebyiJBWs9WN7",
        "outputId": "13e4fdaf-0802-49d3-8218-1b14988d1e54"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import butter, sosfiltfilt, welch, find_peaks\n",
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "# ====== CONFIG: set this to your project folder ======\n",
        "BASE = Path(\"/content/drive/MyDrive/HuBERT-ECG/processed_ptbxl_5s_100hz_fast\")\n",
        "\n",
        "SAMPLES_DIR = BASE/\"analysis\"/\"samples\"\n",
        "OUT_DIR     = BASE/\"analysis\"/\"outputs\"\n",
        "FIG_DIR     = BASE/\"analysis\"/\"figs_tp_fp_dists\"\n",
        "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "FS = 100\n",
        "LEADS = {'I':0,'II':1,'III':2,'aVR':3,'aVL':4,'aVF':5,'V1':6,'V2':7,'V3':8,'V4':9,'V5':10,'V6':11}\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def safe_load_np(p):\n",
        "    x = np.load(p)\n",
        "    if x.ndim!=2 or x.shape[1]!=12: raise ValueError(f\"bad shape {getattr(x,'shape',None)} for {p}\")\n",
        "    return x.astype(float, copy=False)\n",
        "\n",
        "def bandpass(sig, fs, lo, hi, order=4):\n",
        "    ny=fs/2; hi=min(hi, ny*0.98); sos=butter(order, [lo/ny, hi/ny], btype='band', output='sos')\n",
        "    return sosfiltfilt(sos, sig)\n",
        "\n",
        "def pick_lead_with_name(x, pref=('II','V1','V2')):\n",
        "    for name in pref:\n",
        "        i=LEADS[name]\n",
        "        if np.std(x[:,i])>1e-4: return x[:,i], name\n",
        "    j=int(np.argmax(np.std(x,axis=0))); name=list(LEADS.keys())[j]\n",
        "    return x[:,j], name\n",
        "\n",
        "def find_rpeaks_dual(sig, fs=FS):\n",
        "    import neurokit2 as nk\n",
        "    r1=r2=np.array([])\n",
        "    try: _,info = nk.ecg_peaks(sig, sampling_rate=fs, method=\"neurokit\"); r1=info.get(\"ECG_R_Peaks\", np.array([]))\n",
        "    except: pass\n",
        "    try: _,info = nk.ecg_peaks(sig, sampling_rate=fs, method=\"pantompkins1985\"); r2=info.get(\"ECG_R_Peaks\", np.array([]))\n",
        "    except: pass\n",
        "    return r1 if len(r1)>=len(r2) else r2\n",
        "\n",
        "def rr_metrics(r, fs=FS):\n",
        "    if len(r)<3: return dict(cvRR=np.nan, rmssd=np.nan, reg_score=np.nan)\n",
        "    rr=np.diff(r)/fs; sd=float(np.std(rr,ddof=1)); mean=float(np.mean(rr))\n",
        "    cv=sd/(mean+1e-9); d=np.diff(rr); rmssd=float(np.sqrt(np.mean(d**2))) if len(d)>0 else np.nan\n",
        "    reg=float(1-(rmssd/(mean+1e-9))); return dict(cvRR=cv, rmssd=rmssd, reg_score=reg)\n",
        "\n",
        "def p_window_proxies(sig, r, fs=FS, pre_ms=200, post_ms=60):\n",
        "    if len(r)<2: return dict(p_energy=np.nan, p_presence=np.nan, p_to_qrs=np.nan, p_flatness=np.nan)\n",
        "    pband=bandpass(sig, fs, 4, 15); wins=[]; pre=int(pre_ms*fs/1000); post=int(post_ms*fs/1000)\n",
        "    for q in r:\n",
        "        a=max(0, q-pre); b=max(0, q-post)\n",
        "        if b>a and b<=len(pband): wins.append(pband[a:b])\n",
        "    if not wins: return dict(p_energy=np.nan, p_presence=np.nan, p_to_qrs=np.nan, p_flatness=np.nan)\n",
        "    L=min(len(w) for w in wins); W=np.vstack([w[:L] for w in wins]); avg=np.median(W,axis=0)\n",
        "    p_flat=float(np.var(avg)); p_energy=float(np.median(np.mean(W**2,axis=1)))\n",
        "    peaks=np.max(np.abs(W),axis=1); thr=np.percentile(peaks,60); p_presence=float(np.mean(peaks>thr))\n",
        "    qrs_pk=np.percentile(np.abs(sig),99)+1e-9; p_to_qrs=float(np.median(peaks)/qrs_pk)\n",
        "    return dict(p_energy=p_energy, p_presence=p_presence, p_to_qrs=p_to_qrs, p_flatness=p_flat)\n",
        "\n",
        "def psd_features(sig, fs=FS):\n",
        "    f,Pxx=welch(bandpass(sig,fs,1,20), fs=fs, nperseg=min(512,len(sig)))\n",
        "    m38=(f>=3)&(f<=8); m120=(f>=1)&(f<=20); ratio=float(np.trapz(Pxx[m38],f[m38]))/(float(np.trapz(Pxx[m120],f[m120]))+1e-12)\n",
        "    ridge=(f>=4)&(f<=5.5)\n",
        "    if np.any(ridge):\n",
        "        i=np.argmax(Pxx[ridge]); idx=np.where(ridge)[0][i]\n",
        "        lo,hi=max(0,idx-5),min(len(Pxx),idx+6); baseline=float(np.median(Pxx[lo:hi]))+1e-12\n",
        "        prom=float(Pxx[idx]/baseline); fpk=float(f[idx])\n",
        "    else:\n",
        "        prom=0.0; fpk=0.0\n",
        "    return dict(flutter_ratio=ratio, ridge_4_5=prom, ridge_hz=fpk)\n",
        "\n",
        "def quality_metrics(sig, fs=FS):\n",
        "    f,Pxx=welch(sig,fs=fs,nperseg=min(512,len(sig)))\n",
        "    base=(f>=0.2)&(f<=0.5); mid=(f>=5)&(f<=15)\n",
        "    basepow=float(np.trapz(Pxx[base],f[base])) if np.any(base) else 0.0\n",
        "    midpow=float(np.trapz(Pxx[mid],f[mid])) if np.any(mid) else 1e-12\n",
        "    return dict(snr_proxy=float(np.var(sig)/(np.var(np.diff(sig))+1e-12)),\n",
        "                baseline_wander=basepow/midpow)\n",
        "\n",
        "def qrs_indices(sig, fs=FS):\n",
        "    y=bandpass(sig,fs,5,20); diff=np.diff(y,prepend=y[0]); sq=diff**2\n",
        "    win=max(int(0.12*fs),1); integ=np.convolve(sq, np.ones(win)/win, mode='same')\n",
        "    peaks,_=find_peaks(integ, distance=int(0.25*fs), prominence=np.percentile(integ,85))\n",
        "    return peaks\n",
        "\n",
        "def qrs_suppress(sig, r_idx, fs=FS, ms=60):\n",
        "    out=sig.copy(); w=int(ms*fs/1000)\n",
        "    for q in r_idx:\n",
        "        a=max(0,q-w); b=min(len(out),q+w); out[a:b]=0.0\n",
        "    return out\n",
        "\n",
        "def afl_bestlead_features(x):\n",
        "    # detect QRS on rhythm lead\n",
        "    rl,_=pick_lead_with_name(x)\n",
        "    r_idx=qrs_indices(rl, FS)\n",
        "    best=None; best_row=None\n",
        "    for name in ['V1','II','V2','III','aVF']:\n",
        "        sig=x[:,LEADS[name]].astype(float)\n",
        "        sig_sup=qrs_suppress(sig, r_idx, FS, ms=60)\n",
        "        f,Pxx=welch(bandpass(sig_sup,FS,1,20), fs=FS, nperseg=min(512,len(sig_sup)))\n",
        "        m38=(f>=3)&(f<=8); m120=(f>=1)&(f<=20); m812=(f>=8)&(f<=12); m357=(f>=3.5)&(f<=7.0)\n",
        "        def bp(m): return float(np.trapz(Pxx[m],f[m]))/(float(np.trapz(Pxx[m120],f[m120]))+1e-12) if np.any(m) else 0.0\n",
        "        fr =  bp(m38)\n",
        "        h  =  bp(m812)\n",
        "        score = fr + 0.5*h\n",
        "        if np.any(m357):\n",
        "            i=np.argmax(Pxx[m357]); idx=np.where(m357)[0][i]\n",
        "            lo,hi=max(0,idx-5),min(len(Pxx),idx+6); baseline=float(np.median(Pxx[lo:hi]))+1e-12\n",
        "            atrial_peak_h=float(Pxx[idx]/baseline); atrial_peak_hz=float(f[idx])\n",
        "        else:\n",
        "            atrial_peak_h=0.0; atrial_peak_hz=0.0\n",
        "        row=dict(best_lead=name, flutter_ratio=fr, harmonic_8_12=h, flutter_score=score,\n",
        "                 atrial_peak_h=atrial_peak_h, ridge_4_5=atrial_peak_h, ridge_hz=atrial_peak_hz,\n",
        "                 snr_proxy=float(np.var(sig)/(np.var(np.diff(sig))+1e-12)),\n",
        "                 baseline_wander=quality_metrics(sig, FS)[\"baseline_wander\"])\n",
        "        if best is None or score>best: best=score; best_row=row\n",
        "    return best_row\n",
        "\n",
        "def load_split_maps():\n",
        "    sv=pd.read_csv(BASE/\"split_val.csv\").set_index(\"record_id\"); sv.index=sv.index.astype(str)\n",
        "    st=pd.read_csv(BASE/\"split_test.csv\").set_index(\"record_id\"); st.index=st.index.astype(str)\n",
        "    return {\"val\": sv, \"test\": st}\n",
        "\n",
        "def locate_npy(record_id, split_maps):\n",
        "    for s,df in split_maps.items():\n",
        "        if str(record_id) in df.index:\n",
        "            return s, df.loc[str(record_id), \"out_path\"]\n",
        "    return None, None\n",
        "\n",
        "def cliffs_delta(a,b):\n",
        "    a=np.asarray(a); b=np.asarray(b)\n",
        "    a=a[~np.isnan(a)]; b=b[~np.isnan(b)]\n",
        "    n1,n2=len(a),len(b)\n",
        "    if n1==0 or n2==0: return np.nan\n",
        "    a=np.sort(a); b=np.sort(b)\n",
        "    i=j=0; greater=less=0\n",
        "    while i<n1 and j<n2:\n",
        "        if a[i]>b[j]:\n",
        "            greater+=(n1-i); j+=1\n",
        "        elif a[i]<b[j]:\n",
        "            less+=(n2-j); i+=1\n",
        "        else:\n",
        "            val=a[i]\n",
        "            while i<n1 and a[i]==val: i+=1\n",
        "            while j<n2 and b[j]==val: j+=1\n",
        "    return (greater-less)/(n1*n2)\n",
        "\n",
        "def plot_tp_fp_hist(df, feature, title_prefix, out_dir: Path):\n",
        "    TP = df[df[\"tp_flag\"]==1][feature].dropna()\n",
        "    FP = df[df[\"fp_flag\"]==1][feature].dropna()\n",
        "    if len(TP) and len(FP):\n",
        "        stat,p = mannwhitneyu(TP,FP,alternative=\"two-sided\")\n",
        "        d = cliffs_delta(TP.values, FP.values)\n",
        "        tp_med, fp_med = float(np.median(TP)), float(np.median(FP))\n",
        "    else:\n",
        "        p=d=tp_med=fp_med=np.nan\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.hist(TP, bins=24, alpha=0.6, density=True, label=\"TP\")\n",
        "    plt.hist(FP, bins=24, alpha=0.5, density=True, label=\"FP\")\n",
        "    plt.title(f\"{title_prefix}: {feature}\\nmedian(TP)={tp_med:.3g}, median(FP)={fp_med:.3g}; p={p:.3g}, CliffΔ={d:.2f}\")\n",
        "    plt.xlabel(feature); plt.ylabel(\"Density\"); plt.legend()\n",
        "    out = out_dir/f\"{title_prefix.replace(' ','_')}_{feature}.png\"\n",
        "    plt.tight_layout(); plt.savefig(out, bbox_inches=\"tight\"); plt.close()\n",
        "    return out, {\"feature\":feature, \"tp_median\":tp_med, \"fp_median\":fp_med, \"p\":p, \"cliffs_delta\":d, \"tp_n\":int(len(TP)), \"fp_n\":int(len(FP))}\n",
        "\n",
        "# ====== build recomputed tables if missing ======\n",
        "split_maps = load_split_maps()\n",
        "\n",
        "af_rec_path  = OUT_DIR/\"features_recomputed_AFvsREST.csv\"\n",
        "afl_rec_path = OUT_DIR/\"features_recomputed_AFLvsREST.csv\"\n",
        "\n",
        "if not af_rec_path.exists():\n",
        "    AF_S = pd.concat([pd.read_csv(SAMPLES_DIR/\"AF_TP.csv\"),\n",
        "                      pd.read_csv(SAMPLES_DIR/\"AF_FP.csv\")], ignore_index=True)\n",
        "    rows=[]\n",
        "    for _, r in AF_S.iterrows():\n",
        "        rid=str(r[\"record_id\"])\n",
        "        _, npy = locate_npy(rid, split_maps)\n",
        "        if not npy or not Path(npy).exists(): continue\n",
        "        x = safe_load_np(npy)\n",
        "        sig, _ = pick_lead_with_name(x)\n",
        "        rpeaks = find_rpeaks_dual(sig, FS)\n",
        "        feats = {}\n",
        "        feats.update(rr_metrics(rpeaks, FS))\n",
        "        feats.update(p_window_proxies(sig, rpeaks, FS))\n",
        "        feats.update(psd_features(sig, FS))\n",
        "        feats.update(quality_metrics(sig, FS))\n",
        "        feats.update(dict(record_id=rid, tp_flag=int(r.get(\"true_label\",0)==1 and r.get(\"pred_label\",1)==1),\n",
        "                          fp_flag=int(r.get(\"true_label\",0)==0 and r.get(\"pred_label\",1)==1)))\n",
        "        rows.append(feats)\n",
        "    pd.DataFrame(rows).to_csv(af_rec_path, index=False)\n",
        "\n",
        "if not afl_rec_path.exists():\n",
        "    AFL_S = pd.concat([pd.read_csv(SAMPLES_DIR/\"AFL_TP.csv\"),\n",
        "                       pd.read_csv(SAMPLES_DIR/\"AFL_FP.csv\")], ignore_index=True)\n",
        "    rows=[]\n",
        "    for _, r in AFL_S.iterrows():\n",
        "        rid=str(r[\"record_id\"])\n",
        "        _, npy = locate_npy(rid, split_maps)\n",
        "        if not npy or not Path(npy).exists(): continue\n",
        "        x = safe_load_np(npy)\n",
        "        feats = afl_bestlead_features(x)\n",
        "        feats.update(dict(record_id=rid, tp_flag=int(r.get(\"true_label\",0)==1 and r.get(\"pred_label\",1)==1),\n",
        "                          fp_flag=int(r.get(\"true_label\",0)==0 and r.get(\"pred_label\",1)==1)))\n",
        "        rows.append(feats)\n",
        "    pd.DataFrame(rows).to_csv(afl_rec_path, index=False)\n",
        "\n",
        "# ====== Load & plot ======\n",
        "AF  = pd.read_csv(af_rec_path)\n",
        "AFL = pd.read_csv(afl_rec_path)\n",
        "\n",
        "af_features  = [c for c in [\"cvRR\",\"rmssd\",\"reg_score\",\"p_energy\",\"p_presence\",\"p_to_qrs\",\"p_flatness\",\"snr_proxy\",\"baseline_wander\"] if c in AF.columns]\n",
        "afl_features = [c for c in [\"flutter_ratio\",\"ridge_4_5\",\"ridge_hz\",\"harmonic_8_12\",\"snr_proxy\",\"baseline_wander\"] if c in AFL.columns]\n",
        "\n",
        "summaries=[]\n",
        "af_dir = FIG_DIR/\"AF\"; afl_dir = FIG_DIR/\"AFL\"\n",
        "af_dir.mkdir(parents=True, exist_ok=True); afl_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for feat in af_features:\n",
        "    pth, summ = plot_tp_fp_hist(AF, feat, \"AF TP vs FP\", af_dir)\n",
        "    summaries.append({\"task\":\"AF\", **summ, \"path\": str(pth)})\n",
        "\n",
        "for feat in afl_features:\n",
        "    pth, summ = plot_tp_fp_hist(AFL, feat, \"AFL TP vs FP\", afl_dir)\n",
        "    summaries.append({\"task\":\"AFL\", **summ, \"path\": str(pth)})\n",
        "\n",
        "summary_df = pd.DataFrame(summaries)\n",
        "# from caas_jupyter_tools import display_dataframe_to_user\n",
        "display(summary_df)\n",
        "\n",
        "print(\"Saved AF plots to:\", af_dir.as_posix())\n",
        "print(\"Saved AFL plots to:\", afl_dir.as_posix())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "YA3JAhlmN2Oi",
        "outputId": "7ba11795-8b9b-4e61-8d38-72817a9b1f7a"
      },
      "outputs": [],
      "source": [
        "# === TP vs FP summary for AF & AFL ===\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "# ---- CONFIG: set to your project root ----\n",
        "BASE = Path(\"/content/drive/MyDrive/HuBERT-ECG/processed_ptbxl_5s_100hz_fast\")\n",
        "AF_PATH  = BASE/\"analysis\"/\"outputs\"/\"features_recomputed_AFvsREST.csv\"\n",
        "AFL_PATH = BASE/\"analysis\"/\"outputs\"/\"features_recomputed_AFLvsREST.csv\"\n",
        "\n",
        "# ---- helpers ----\n",
        "def cliffs_delta(a, b):\n",
        "    a = np.asarray(a); b = np.asarray(b)\n",
        "    a = a[~np.isnan(a)]; b = b[~np.isnan(b)]\n",
        "    n1, n2 = len(a), len(b)\n",
        "    if n1 == 0 or n2 == 0: return np.nan\n",
        "    a = np.sort(a); b = np.sort(b)\n",
        "    i = j = 0; greater = less = 0\n",
        "    while i < n1 and j < n2:\n",
        "        if a[i] > b[j]:\n",
        "            greater += (n1 - i); j += 1\n",
        "        elif a[i] < b[j]:\n",
        "            less += (n2 - j); i += 1\n",
        "        else:\n",
        "            val = a[i]\n",
        "            while i < n1 and a[i] == val: i += 1\n",
        "            while j < n2 and b[j] == val: j += 1\n",
        "    return (greater - less) / (n1 * n2)\n",
        "\n",
        "def effect_label(delta):\n",
        "    if np.isnan(delta): return \"—\"\n",
        "    ad = abs(delta)\n",
        "    if ad < 0.147: return \"Negligible\"\n",
        "    if ad < 0.33:  return \"Small\"\n",
        "    if ad < 0.474: return \"Medium\"\n",
        "    return \"Large\"\n",
        "\n",
        "def summarize_tp_fp(df, features, title):\n",
        "    rows = []\n",
        "    TP = df[df[\"tp_flag\"]==1]\n",
        "    FP = df[df[\"fp_flag\"]==1]\n",
        "    for f in features:\n",
        "        if f not in df.columns:\n",
        "            continue\n",
        "        x = TP[f].astype(float).dropna().values\n",
        "        y = FP[f].astype(float).dropna().values\n",
        "        tp_med = np.nanmedian(x) if x.size else np.nan\n",
        "        fp_med = np.nanmedian(y) if y.size else np.nan\n",
        "        if x.size and y.size:\n",
        "            _, p = mannwhitneyu(x, y, alternative=\"two-sided\")\n",
        "            d = cliffs_delta(x, y)\n",
        "        else:\n",
        "            p, d = np.nan, np.nan\n",
        "        rows.append(dict(Task=title, Feature=f,\n",
        "                         TP_Median=tp_med, FP_Median=fp_med,\n",
        "                         p_value=p, Cliffs_Delta=d, Effect_Size=effect_label(d),\n",
        "                         TP_N=x.size, FP_N=y.size))\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# ---- load and pick features ----\n",
        "tables = []\n",
        "if AF_PATH.exists():\n",
        "    AF = pd.read_csv(AF_PATH)\n",
        "    af_feats = [c for c in [\"cvRR\",\"rmssd\",\"reg_score\",\"p_presence\",\"p_energy\",\"p_flatness\",\"p_to_qrs\",\n",
        "                            \"baseline_wander\",\"snr_proxy\"] if c in AF.columns]\n",
        "    tables.append(summarize_tp_fp(AF, af_feats, \"AF\"))\n",
        "else:\n",
        "    print(\"Missing:\", AF_PATH)\n",
        "\n",
        "if AFL_PATH.exists():\n",
        "    AFL = pd.read_csv(AFL_PATH)\n",
        "    afl_feats = [c for c in [\"flutter_ratio\",\"atrial_peak_h\",\"ridge_4_5\",\"ridge_hz\",\"harmonic_8_12\",\n",
        "                             \"p_present_frac\",\"p_en_med\",\"p_per_qrs_med\",\n",
        "                             \"baseline_wander\",\"snr_proxy\"] if c in AFL.columns]\n",
        "    tables.append(summarize_tp_fp(AFL, afl_feats, \"AFL\"))\n",
        "else:\n",
        "    print(\"Missing:\", AFL_PATH)\n",
        "\n",
        "if not tables:\n",
        "    raise SystemExit(\"No input tables found. Recompute your qualitative features or fix the BASE path.\")\n",
        "\n",
        "summary = pd.concat(tables, ignore_index=True)\n",
        "\n",
        "# Save CSV and also print a Markdown-ready table per task\n",
        "OUT = BASE/\"analysis\"/\"outputs\"/\"tp_fp_feature_summary.csv\"\n",
        "summary.to_csv(OUT, index=False)\n",
        "print(\"Saved:\", OUT)\n",
        "\n",
        "# Pretty print per task\n",
        "for task in summary[\"Task\"].unique():\n",
        "    sub = summary[summary[\"Task\"]==task].copy()\n",
        "    sub = sub[[\"Feature\",\"TP_Median\",\"FP_Median\",\"p_value\",\"Cliffs_Delta\",\"Effect_Size\"]]\n",
        "    # Formatting\n",
        "    sub[\"TP_Median\"]  = sub[\"TP_Median\"].map(lambda v: f\"{v:.4g}\" if pd.notna(v) else \"—\")\n",
        "    sub[\"FP_Median\"]  = sub[\"FP_Median\"].map(lambda v: f\"{v:.4g}\" if pd.notna(v) else \"—\")\n",
        "    sub[\"p_value\"]    = sub[\"p_value\"].map(lambda v: \"<0.001\" if (pd.notna(v) and v<1e-3) else (f\"{v:.3g}\" if pd.notna(v) else \"—\"))\n",
        "    sub[\"Cliffs_Delta\"]= sub[\"Cliffs_Delta\"].map(lambda v: f\"{v:+.2f}\" if pd.notna(v) else \"—\")\n",
        "    print(f\"\\n### {task} — Feature Distribution: TP vs FP\")\n",
        "    print(\"| Feature | TP Median | FP Median | p-value | Effect Size (Cliff’s Δ) |\")\n",
        "    print(\"|---|---:|---:|---:|---:|\")\n",
        "    for _, r in sub.iterrows():\n",
        "        print(f\"| **{r['Feature']}** | {r['TP_Median']} | {r['FP_Median']} | {r['p_value']} | {r['Cliffs_Delta']} ({summary.loc[(_==_), 'Effect_Size'] if False else r['Effect_Size']}) |\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YnUJLfvt9DY",
        "outputId": "ab8334ae-221c-4811-964d-816be48bd03a"
      },
      "outputs": [],
      "source": [
        "# ---- Shim: define AF feature extractor if missing (mirrors your Stage-2 code) ----\n",
        "try:\n",
        "    _ = extract_features  # noqa: F821\n",
        "except NameError:\n",
        "    import numpy as np\n",
        "    from scipy.signal import butter, sosfiltfilt, welch, find_peaks\n",
        "\n",
        "    FS = 100\n",
        "    LEADS = {'I':0,'II':1,'III':2,'aVR':3,'aVL':4,'aVF':5,'V1':6,'V2':7,'V3':8,'V4':9,'V5':10,'V6':11}\n",
        "\n",
        "    def bandpass(sig, fs, lo, hi, order=4):\n",
        "        ny = fs/2; hi = min(hi, ny*0.98)\n",
        "        sos = butter(order, [lo/ny, hi/ny], btype='band', output='sos')\n",
        "        return sosfiltfilt(sos, sig)\n",
        "\n",
        "    def safe_load_np(path):\n",
        "        x = np.load(path)  # (T,12)\n",
        "        if x.ndim!=2 or x.shape[1]!=12:\n",
        "            raise ValueError(f\"bad shape {getattr(x,'shape',None)}\")\n",
        "        return x\n",
        "\n",
        "    def pick_lead_for_rhythm(x):\n",
        "        for name in ['II','V1','V2']:\n",
        "            idx = LEADS[name]\n",
        "            if np.std(x[:,idx]) > 1e-4:\n",
        "                return x[:,idx]\n",
        "        return x[:, np.argmax(np.std(x, axis=0))]\n",
        "\n",
        "    def detect_qrs(sig, fs=FS):\n",
        "        y = bandpass(sig, fs, 5, 20)\n",
        "        diff = np.diff(y, prepend=y[0])\n",
        "        sq = diff**2\n",
        "        win = max(int(0.12*fs), 1)\n",
        "        integ = np.convolve(sq, np.ones(win)/win, mode='same')\n",
        "        peaks, _ = find_peaks(integ, distance=int(0.25*fs), prominence=np.percentile(integ, 85))\n",
        "        return peaks\n",
        "\n",
        "    def rr_features(qrs_idx, fs=FS):\n",
        "        if len(qrs_idx) < 3:\n",
        "            return dict(rr_mean=np.nan, rr_std=np.nan, rr_cv=np.nan, rr_rmssd=np.nan, rr_reg=np.nan)\n",
        "        rr = np.diff(qrs_idx)/fs\n",
        "        mean = float(np.mean(rr))\n",
        "        std = float(np.std(rr))\n",
        "        cv = float(std/(mean+1e-8))\n",
        "        rmssd = float(np.sqrt(np.mean(np.diff(rr)**2))) if len(rr)>2 else np.nan\n",
        "        reg = float(1.0/(1.0+cv))\n",
        "        return dict(rr_mean=mean, rr_std=std, rr_cv=cv, rr_rmssd=rmssd, rr_reg=reg)\n",
        "\n",
        "    def pwave_metrics(sig, qrs_idx, fs=FS):\n",
        "        if len(qrs_idx) < 2:\n",
        "            return dict(p_en=np.nan, p_present=np.nan, p_to_qrs=np.nan)\n",
        "        bp = bandpass(sig, fs, 4, 15)\n",
        "        wins = []\n",
        "        for q in qrs_idx:\n",
        "            a = max(0, q - int(0.20*fs)); b = max(0, q - int(0.05*fs))\n",
        "            if b > a:\n",
        "                wins.append(bp[a:b])\n",
        "        if not wins:\n",
        "            return dict(p_en=np.nan, p_present=np.nan, p_to_qrs=np.nan)\n",
        "        en = np.array([np.mean(w**2) for w in wins])\n",
        "        p_en = float(np.median(en))\n",
        "        thr = max(np.percentile(en, 60), 1e-6)\n",
        "        p_present = float(np.mean(en > thr))\n",
        "        pband = bandpass(sig, fs, 3, 9)\n",
        "        p_peaks, _ = find_peaks(pband, distance=int(0.18*fs), prominence=np.percentile(np.abs(pband), 80))\n",
        "        p_to_qrs = float(len(p_peaks)/(len(qrs_idx)+1e-6))\n",
        "        return dict(p_en=p_en, p_present=p_present, p_to_qrs=p_to_qrs)\n",
        "\n",
        "    def flutter_bandpower(sig, fs=FS):\n",
        "        f, Pxx = welch(bandpass(sig, fs, 1, 20), fs=fs, nperseg=min(256, len(sig)))\n",
        "        mask_lo = (f>=3) & (f<=8)\n",
        "        mask_all = (f>=1) & (f<=20)\n",
        "        bp_3_8 = float(np.trapz(Pxx[mask_lo], f[mask_lo])) if mask_lo.any() else 0.0\n",
        "        bp_1_20 = float(np.trapz(Pxx[mask_all], f[mask_all])) if mask_all.any() else 1e-12\n",
        "        return dict(flutter_ratio = bp_3_8 / bp_1_20)\n",
        "\n",
        "    def extract_features(npy_path):\n",
        "        x = safe_load_np(npy_path)\n",
        "        lead = pick_lead_for_rhythm(x)\n",
        "        qrs = detect_qrs(lead, FS)\n",
        "        feats = {}\n",
        "        feats.update(rr_features(qrs, FS))\n",
        "        feats.update(pwave_metrics(lead, qrs, FS))\n",
        "        feats.update(flutter_bandpower(lead, FS))\n",
        "        feats.update({\n",
        "            \"amp_med\": float(np.median(lead)),\n",
        "            \"amp_iqr\": float(np.percentile(lead,75)-np.percentile(lead,25))\n",
        "        })\n",
        "        return feats\n",
        "\n",
        "    print(\"[Shim] Defined extract_features and its dependencies for AF.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 978
        },
        "id": "p0sI-erwtLXc",
        "outputId": "50eae1e3-bf1c-457a-a803-9dbe4d101d85"
      },
      "outputs": [],
      "source": [
        "# @title AF TP vs FP table + FP reduction (Stage-1 -> Stage-2)\n",
        "# ========= TP vs FP table + FP reduction (Stage-1 -> Stage-2) =========\n",
        "import json, joblib, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from scipy.stats import mannwhitneyu\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# ---- config ----\n",
        "OUT = Path(\"/content/drive/MyDrive/HuBERT-ECG/processed_ptbxl_5s_100hz_fast\")\n",
        "SPLIT = \"val\"           # use \"val\" for the supervisor's request; you can switch to \"test\"\n",
        "TASK  = \"AFvsREST\"      # or \"AFLvsREST\"\n",
        "\n",
        "# ---- effect size (Cliff's Δ) helpers ----\n",
        "def cliffs_delta(tp_vals, fp_vals):\n",
        "    tp_vals = np.asarray(tp_vals); fp_vals = np.asarray(fp_vals)\n",
        "    n1, n2 = len(tp_vals), len(fp_vals)\n",
        "    if n1 == 0 or n2 == 0: return np.nan\n",
        "    U, _ = mannwhitneyu(tp_vals, fp_vals, alternative='two-sided')\n",
        "    return (2.0 * U) / (n1 * n2) - 1.0\n",
        "\n",
        "def cliffs_size_label(delta):\n",
        "    if pd.isna(delta): return \"NA\"\n",
        "    a = abs(delta)\n",
        "    if a < 0.147: return \"Negligible\"\n",
        "    if a < 0.33:  return \"Small\"\n",
        "    if a < 0.474: return \"Medium\"\n",
        "    return \"Large\"\n",
        "\n",
        "# ---- load Stage-1 candidate set + labels for the chosen split ----\n",
        "pos_wlab = OUT/f\"stage1_{TASK}/positives_{SPLIT}_with_labels.csv\"\n",
        "assert pos_wlab.exists(), f\"Missing {pos_wlab}. Run Stage-1 first.\"\n",
        "\n",
        "df_cand = pd.read_csv(pos_wlab)\n",
        "df_cand[\"record_id\"] = df_cand[\"record_id\"].astype(str)\n",
        "df_cand.rename(columns={\"is_true_positive\":\"y\"}, inplace=True)\n",
        "\n",
        "# map to npy paths\n",
        "split_csv = pd.read_csv(OUT/f\"split_{SPLIT}.csv\").set_index(\"record_id\")\n",
        "split_csv.index = split_csv.index.astype(str)\n",
        "df_cand[\"out_path\"] = df_cand[\"record_id\"].map(split_csv[\"out_path\"])\n",
        "df_cand = df_cand.dropna(subset=[\"out_path\"]).reset_index(drop=True)\n",
        "\n",
        "# ---- extract features for ALL candidates (TP+FP) with your Stage-2 extractor ----\n",
        "rows = []\n",
        "use_afl = (TASK == \"AFLvsREST\")\n",
        "for rid, p, y in zip(df_cand[\"record_id\"], df_cand[\"out_path\"], df_cand[\"y\"]):\n",
        "    try:\n",
        "        feats = extract_features_AFL(p) if use_afl else extract_features(p)\n",
        "        feats[\"record_id\"] = str(rid); feats[\"y\"] = int(y)\n",
        "        rows.append(feats)\n",
        "    except Exception as e:\n",
        "        print(\"Skip (feature extract)\", rid, \":\", e)\n",
        "feat_df = pd.DataFrame(rows).fillna(0.0)\n",
        "assert not feat_df.empty, \"No features extracted.\"\n",
        "\n",
        "# If your AFL validator drops RR-features, mirror that for summaries if you want perfect alignment:\n",
        "# (Optional; leave commented if you want to DISPLAY RR stats too)\n",
        "# if use_afl:\n",
        "#     rr_drop = [\"rr_mean\",\"rr_std\",\"rr_cv\",\"rr_rmssd\",\"rr_reg\"]\n",
        "#     feat_df = feat_df.drop(columns=[c for c in rr_drop if c in feat_df.columns])\n",
        "\n",
        "# ---- build TP vs FP summary table ----\n",
        "TP = feat_df[feat_df[\"y\"]==1]\n",
        "FP = feat_df[feat_df[\"y\"]==0]\n",
        "\n",
        "feature_cols = [c for c in feat_df.columns if c not in [\"record_id\",\"y\"]]\n",
        "rows_sum = []\n",
        "for f in feature_cols:\n",
        "    tp_vals = TP[f].dropna().values\n",
        "    fp_vals = FP[f].dropna().values\n",
        "    tp_med  = float(np.median(tp_vals)) if len(tp_vals) else np.nan\n",
        "    fp_med  = float(np.median(fp_vals)) if len(fp_vals) else np.nan\n",
        "    if len(tp_vals) and len(fp_vals):\n",
        "        _, pval = mannwhitneyu(tp_vals, fp_vals, alternative='two-sided')\n",
        "        delta   = cliffs_delta(tp_vals, fp_vals)\n",
        "    else:\n",
        "        pval, delta = np.nan, np.nan\n",
        "    rows_sum.append({\n",
        "        \"Feature\": f,\n",
        "        \"nTP\": len(tp_vals),\n",
        "        \"nFP\": len(fp_vals),\n",
        "        \"TP Median\": tp_med,\n",
        "        \"FP Median\": fp_med,\n",
        "        \"p-value\": pval,\n",
        "        \"Cliff's Δ\": delta,\n",
        "        \"Effect Size\": cliffs_size_label(delta),\n",
        "    })\n",
        "summary_tbl = pd.DataFrame(rows_sum)\n",
        "summary_tbl[\"|Δ|\"] = summary_tbl[\"Cliff's Δ\"].abs()\n",
        "summary_tbl = summary_tbl.sort_values([\"|Δ|\",\"p-value\"], ascending=[False, True]).drop(columns=\"|Δ|\").reset_index(drop=True)\n",
        "\n",
        "# Save the table (nice for your slide export)\n",
        "summary_path = OUT/f\"analysis/{TASK.lower()}_{SPLIT}_tp_fp_feature_summary.csv\"\n",
        "Path(summary_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "summary_tbl.to_csv(summary_path, index=False)\n",
        "print(f\"[Saved] {summary_path}\")\n",
        "print(f\"Scope: {TASK} ({SPLIT}); n(TP)={len(TP)}, n(FP)={len(FP)}\")\n",
        "\n",
        "# ---- Stage-1 vs Stage-2 FP counts on this candidate pool ----\n",
        "# Stage-1 'positives' == these candidates; so FP_before = #FP here.\n",
        "FP_before = int((df_cand[\"y\"]==0).sum())\n",
        "TP_before = int((df_cand[\"y\"]==1).sum())\n",
        "\n",
        "# Load Stage-2 validator + its threshold + feature list (in training order)\n",
        "clf = joblib.load(OUT/f\"stage2_{TASK}_validator.joblib\")\n",
        "meta = json.load(open(OUT/f\"stage2_{TASK}_meta.json\"))\n",
        "tau  = float(meta[\"threshold\"])\n",
        "feat_cols_valid = meta[\"features\"]  # the columns the validator expects (after any drops)\n",
        "\n",
        "# Align feature matrix to validator columns\n",
        "X_all = feat_df.drop(columns=[\"record_id\",\"y\"]).reindex(columns=feat_cols_valid, fill_value=0.0).to_numpy()\n",
        "proba = clf.predict_proba(X_all)[:,1]\n",
        "kept_mask = (proba >= tau)\n",
        "\n",
        "# Stage-2 predictions on the candidate pool\n",
        "y_true = feat_df[\"y\"].to_numpy().astype(int)\n",
        "FP_after = int(((kept_mask == 1) & (y_true == 0)).sum())\n",
        "TP_after = int(((kept_mask == 1) & (y_true == 1)).sum())\n",
        "\n",
        "red_abs = FP_before - FP_after\n",
        "red_pct = (red_abs / FP_before * 100.0) if FP_before > 0 else np.nan\n",
        "print(f\"FP reduction (Stage-1 -> Stage-2 @τ): {FP_before} -> {FP_after}  (−{red_abs}, {red_pct:.1f}%)\")\n",
        "\n",
        "# ---- Incremental FP reduction curve (add features in |weight| order) ----\n",
        "# We'll refit LR on the candidate pool with the top-k validator features (k=1..m),\n",
        "# pick τ with the same strategy used in Stage-2 meta, and measure FP_after_k.\n",
        "\n",
        "# Figure out weight order from the trained validator\n",
        "lr = clf.named_steps[\"logisticregression\"]\n",
        "w  = lr.coef_.ravel()\n",
        "order = np.argsort(-np.abs(w))        # descending by |weight|\n",
        "ordered_feats = [feat_cols_valid[i] for i in order]\n",
        "\n",
        "def choose_tau(y, scores, strategy_meta):\n",
        "    # Mirror your Stage-2 tuning policies\n",
        "    from sklearn.metrics import precision_recall_curve, recall_score\n",
        "    if \"recall≥\" in strategy_meta and \"precision≥\" in strategy_meta:\n",
        "        # AFL style (recall-first with min precision)\n",
        "        import re\n",
        "        p = float(re.search(r'precision≥([0-9.]+)', strategy_meta).group(1))\n",
        "        r = float(re.search(r'recall≥([0-9.]+)', strategy_meta).group(1))\n",
        "        P, R, T = precision_recall_curve(y, scores)\n",
        "        both = [(pp, rr, tt) for pp, rr, tt in zip(P[:-1], R[:-1], T) if (pp >= p and rr >= r)]\n",
        "        if both:\n",
        "            both.sort(key=lambda z: (z[1], z[0])); return float(both[-1][2])\n",
        "        byR = [(pp, rr, tt) for pp, rr, tt in zip(P[:-1], R[:-1], T) if rr >= r]\n",
        "        if byR:\n",
        "            byR.sort(key=lambda z: (z[0], z[1])); return float(byR[-1][2])\n",
        "        byP = [(pp, rr, tt) for pp, rr, tt in zip(P[:-1], R[:-1], T) if pp >= p]\n",
        "        if byP:\n",
        "            byP.sort(key=lambda z: (z[1], z[0])); return float(byP[-1][2])\n",
        "        return float(T[-1]) if len(T) else 0.5\n",
        "    elif \"recall≥\" in strategy_meta and \"precision≥\" not in strategy_meta:\n",
        "        # AF style (very high recall)\n",
        "        import re\n",
        "        r = float(re.search(r'recall≥([0-9.]+)', strategy_meta).group(1))\n",
        "        thrs = np.unique(scores)[::-1]; best = 0.5\n",
        "        from sklearn.metrics import recall_score\n",
        "        for t in thrs:\n",
        "            rec = recall_score(y, (scores >= t).astype(int), zero_division=0)\n",
        "            if rec >= r: best = float(t)\n",
        "            else: break\n",
        "        return best\n",
        "    else:\n",
        "        # fallback: maximize F1 in PR space\n",
        "        P, R, T = precision_recall_curve(y, scores)\n",
        "        f1 = (2*P*R)/(P+R+1e-12)\n",
        "        i = int(np.nanargmax(f1[:-1]))\n",
        "        return float(T[i])\n",
        "\n",
        "strategy = meta.get(\"strategy\",\"\")\n",
        "fp_curve = []  # rows: k, used_features, FP_after_k, TP_after_k, tau_k\n",
        "for k in range(1, len(ordered_feats)+1):\n",
        "    used = ordered_feats[:k]\n",
        "    Xk = feat_df[used].to_numpy()\n",
        "    # Refit a light LR on this candidate pool (same settings as validator)\n",
        "    clf_k = make_pipeline(\n",
        "        StandardScaler(with_mean=True),\n",
        "        LogisticRegression(max_iter=1000, penalty=\"l1\", solver=\"liblinear\",\n",
        "                           C=0.7, class_weight={0:1.0, 1:4.0}, random_state=42)\n",
        "    ).fit(Xk, y_true)\n",
        "    prob_k = clf_k.predict_proba(Xk)[:,1]\n",
        "    tau_k  = choose_tau(y_true, prob_k, strategy)\n",
        "    pred_k = (prob_k >= tau_k).astype(int)\n",
        "    FP_k   = int(((pred_k == 1) & (y_true == 0)).sum())\n",
        "    TP_k   = int(((pred_k == 1) & (y_true == 1)).sum())\n",
        "    fp_curve.append({\n",
        "        \"k\": k,\n",
        "        \"features_used\": \", \".join(used),\n",
        "        \"FP_before\": FP_before,\n",
        "        \"FP_after_k\": FP_k,\n",
        "        \"TP_after_k\": TP_k,\n",
        "        \"FP_reduction_abs\": FP_before - FP_k,\n",
        "        \"FP_reduction_pct\": (FP_before - FP_k) / FP_before * 100.0 if FP_before>0 else np.nan,\n",
        "        \"tau_k\": tau_k\n",
        "    })\n",
        "\n",
        "fp_curve_df = pd.DataFrame(fp_curve)\n",
        "curve_path = OUT/f\"analysis/{TASK.lower()}_{SPLIT}_fp_reduction_curve.csv\"\n",
        "fp_curve_df.to_csv(curve_path, index=False)\n",
        "print(f\"[Saved] {curve_path}\")\n",
        "\n",
        "# Quick glance\n",
        "display(summary_tbl.head(10))\n",
        "display(fp_curve_df.head(8))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YrPl6v3UxQGs",
        "outputId": "f6f82104-4924-4ba9-828a-b41f04468108"
      },
      "outputs": [],
      "source": [
        "# @title AFL through features table\n",
        "# ================= AFL ANALYSIS (AFLvsREST only) =================\n",
        "# Builds TP vs FP feature table (nTP/nFP, medians, p, Cliff's Δ + size)\n",
        "# Reports FP reduction Stage-1 -> Stage-2, and incremental FP reduction as features are added.\n",
        "\n",
        "SPLIT = \"val\"   # change to \"test\" if needed\n",
        "\n",
        "import re, json, joblib, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from scipy.stats import mannwhitneyu\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "OUT = Path(\"/content/drive/MyDrive/HuBERT-ECG/processed_ptbxl_5s_100hz_fast\")\n",
        "\n",
        "# ---------- Ensure AFL extractor is defined (shim copied from your Stage-2) ----------\n",
        "try:\n",
        "    _ = extract_features_AFL  # noqa: F821\n",
        "except NameError:\n",
        "    import numpy as np\n",
        "    from scipy.signal import butter, sosfiltfilt, welch, find_peaks\n",
        "\n",
        "    FS = 100\n",
        "    LEADS = {'I':0,'II':1,'III':2,'aVR':3,'aVL':4,'aVF':5,'V1':6,'V2':7,'V3':8,'V4':9,'V5':10,'V6':11}\n",
        "\n",
        "    def bandpass(sig, fs, lo, hi, order=4):\n",
        "        ny = fs/2; hi = min(hi, ny*0.98)\n",
        "        sos = butter(order, [lo/ny, hi/ny], btype='band', output='sos')\n",
        "        return sosfiltfilt(sos, sig)\n",
        "\n",
        "    def safe_load_np(path):\n",
        "        x = np.load(path)\n",
        "        if x.ndim!=2 or x.shape[1]!=12: raise ValueError(f\"bad shape {getattr(x,'shape',None)}\")\n",
        "        return x\n",
        "\n",
        "    def detect_qrs(sig, fs=FS):\n",
        "        y = bandpass(sig, fs, 5, 20)\n",
        "        diff = np.diff(y, prepend=y[0]); sq = diff**2\n",
        "        win = max(int(0.12*fs), 1)\n",
        "        integ = np.convolve(sq, np.ones(win)/win, mode='same')\n",
        "        peaks, _ = find_peaks(integ, distance=int(0.25*fs), prominence=np.percentile(integ, 85))\n",
        "        return peaks\n",
        "\n",
        "    def pick_lead_for_qrs(x):\n",
        "        for name in ['II','V1','V2']:\n",
        "            idx = LEADS[name]\n",
        "            if np.std(x[:, idx]) > 1e-4:\n",
        "                return x[:, idx], name\n",
        "        return x[:, np.argmax(np.std(x, axis=0))], 'auto'\n",
        "\n",
        "    def rr_features(qrs_idx, fs=FS):\n",
        "        if len(qrs_idx) < 3:\n",
        "            return dict(rr_mean=np.nan, rr_std=np.nan, rr_cv=np.nan, rr_rmssd=np.nan, rr_reg=np.nan)\n",
        "        rr = np.diff(qrs_idx)/fs\n",
        "        mean = float(np.mean(rr))\n",
        "        std = float(np.std(rr))\n",
        "        cv = float(std/(mean+1e-8))\n",
        "        rmssd = float(np.sqrt(np.mean(np.diff(rr)**2))) if len(rr)>2 else np.nan\n",
        "        reg = float(1.0/(1.0+cv))\n",
        "        return dict(rr_mean=mean, rr_std=std, rr_cv=cv, rr_rmssd=rmssd, rr_reg=reg)\n",
        "\n",
        "    def bandpower_ratio(sig, fs, band, refband):\n",
        "        f, Pxx = welch(bandpass(sig, fs, 1, 20), fs=fs, nperseg=min(256, len(sig)))\n",
        "        m1 = (f>=band[0]) & (f<=band[1]); m2 = (f>=refband[0]) & (f<=refband[1])\n",
        "        bp1 = float(np.trapz(Pxx[m1], f[m1])) if m1.any() else 0.0\n",
        "        bp2 = float(np.trapz(Pxx[m2], f[m2])) if m2.any() else 1e-12\n",
        "        return bp1 / bp2\n",
        "\n",
        "    def pwave_perbeat_metrics(sig, qrs_idx, fs=FS):\n",
        "        if len(qrs_idx) < 2:\n",
        "            return dict(p_en_med=np.nan, p_present_frac=np.nan, p_per_qrs_med=np.nan)\n",
        "        pband = bandpass(sig, fs, 3, 9)\n",
        "        en, p_perq = [], []\n",
        "        for q in qrs_idx:\n",
        "            a = max(0, q - int(0.20*fs)); b = max(0, q - int(0.06*fs))\n",
        "            if b <= a: continue\n",
        "            w = pband[a:b]\n",
        "            en.append(np.mean(w**2))\n",
        "            peaks, _ = find_peaks(w, distance=int(0.08*fs), prominence=np.percentile(np.abs(w), 80))\n",
        "            p_perq.append(len(peaks))\n",
        "        if not en: return dict(p_en_med=np.nan, p_present_frac=np.nan, p_per_qrs_med=np.nan)\n",
        "        en = np.array(en)\n",
        "        p_en_med = float(np.median(en))\n",
        "        thr = max(np.percentile(en, 60), 1e-6)\n",
        "        p_present_frac = float(np.mean(en > thr))\n",
        "        p_per_qrs_med = float(np.median(p_perq)) if len(p_perq) else 0.0\n",
        "        return dict(p_en_med=p_en_med, p_present_frac=p_present_frac, p_per_qrs_med=p_per_qrs_med)\n",
        "\n",
        "    def extract_features_AFL(npy_path):\n",
        "        x = safe_load_np(npy_path)\n",
        "        qrs_lead, _ = pick_lead_for_qrs(x); qrs = detect_qrs(qrs_lead, FS)\n",
        "        feats = {}\n",
        "        # Keep RR features for DISPLAY (your validator may drop them later)\n",
        "        feats.update(rr_features(qrs, FS))\n",
        "        CANDIDATE_LEADS = ['V1','II','V2','III','aVF']\n",
        "        perlead = []\n",
        "        for name in CANDIDATE_LEADS:\n",
        "            sig = x[:, LEADS[name]].astype(float).copy()\n",
        "            if len(qrs) > 0:\n",
        "                for q in qrs:\n",
        "                    a = max(0, q - int(0.06*FS)); b = min(len(sig), q + int(0.06*FS))\n",
        "                    sig[a:b] = 0.0\n",
        "            fratio = bandpower_ratio(sig, FS, (3, 8), (1, 20))\n",
        "            h812   = bandpower_ratio(sig, FS, (8, 12), (1, 20))\n",
        "            score  = fratio + 0.5*h812\n",
        "            perlead.append((name, fratio, h812, score))\n",
        "        best_name, best_fr, best_h2, best_score = max(perlead, key=lambda z: z[3])\n",
        "        best_sig = x[:, LEADS[best_name]]\n",
        "        feats.update(pwave_perbeat_metrics(best_sig, qrs, FS))\n",
        "        feats.update({\n",
        "            \"flutter_ratio_qrs_supp\": float(best_fr),\n",
        "            \"harmonic_8_12\": float(best_h2),\n",
        "            \"flutter_score\": float(best_score),\n",
        "            \"amp_med\": float(np.median(best_sig)),\n",
        "            \"amp_iqr\": float(np.percentile(best_sig,75)-np.percentile(best_sig,25)),\n",
        "        })\n",
        "        f, Pxx = welch(bandpass(best_sig, FS, 1, 20), fs=FS, nperseg=min(256,len(best_sig)))\n",
        "        mask = (f>=3.5) & (f<=7.0)\n",
        "        if mask.any():\n",
        "            i = np.argmax(Pxx[mask]); f_peak = float(f[mask][i]); pk = float(Pxx[mask][i])\n",
        "            lo = max(0, np.where(mask)[0][i]-5); hi = min(len(Pxx), np.where(mask)[0][i]+6)\n",
        "            baseline = 1e-12 + float(np.median(Pxx[lo:hi])); atrial_peak_h = pk / baseline\n",
        "        else:\n",
        "            f_peak, atrial_peak_h = 0.0, 0.0\n",
        "        feats.update({\"atrial_peak_h\": atrial_peak_h, \"atrial_peak_hz\": f_peak})\n",
        "        return feats\n",
        "\n",
        "    print(\"[Shim] AFL extractor defined.\")\n",
        "\n",
        "# ---------- Stats helpers ----------\n",
        "def cliffs_delta(tp_vals, fp_vals):\n",
        "    tp_vals = np.asarray(tp_vals); fp_vals = np.asarray(fp_vals)\n",
        "    n1, n2 = len(tp_vals), len(fp_vals)\n",
        "    if n1 == 0 or n2 == 0: return np.nan\n",
        "    U, _ = mannwhitneyu(tp_vals, fp_vals, alternative='two-sided')\n",
        "    return (2.0 * U) / (n1 * n2) - 1.0\n",
        "\n",
        "def cliffs_size_label(delta):\n",
        "    if pd.isna(delta): return \"NA\"\n",
        "    a = abs(delta)\n",
        "    if a < 0.147: return \"Negligible\"\n",
        "    if a < 0.33:  return \"Small\"\n",
        "    if a < 0.474: return \"Medium\"\n",
        "    return \"Large\"\n",
        "\n",
        "# ---------- Load Stage-1 candidates (AFL) with labels ----------\n",
        "pos_wlab = OUT/\"stage1_AFLvsREST/positives_{}_with_labels.csv\".format(SPLIT)\n",
        "assert pos_wlab.exists(), f\"Missing {pos_wlab}. Run Stage-1 first.\"\n",
        "df_cand = pd.read_csv(pos_wlab)\n",
        "df_cand[\"record_id\"] = df_cand[\"record_id\"].astype(str)\n",
        "df_cand.rename(columns={\"is_true_positive\":\"y\"}, inplace=True)\n",
        "\n",
        "split_df = pd.read_csv(OUT/f\"split_{SPLIT}.csv\").set_index(\"record_id\")\n",
        "split_df.index = split_df.index.astype(str)\n",
        "df_cand[\"out_path\"] = df_cand[\"record_id\"].map(split_df[\"out_path\"])\n",
        "df_cand = df_cand.dropna(subset=[\"out_path\"]).reset_index(drop=True)\n",
        "\n",
        "# ---------- Extract AFL features for ALL candidates ----------\n",
        "rows = []\n",
        "for rid, p, y in zip(df_cand[\"record_id\"], df_cand[\"out_path\"], df_cand[\"y\"]):\n",
        "    try:\n",
        "        feats = extract_features_AFL(p)\n",
        "        feats[\"record_id\"] = str(rid); feats[\"y\"] = int(y)\n",
        "        rows.append(feats)\n",
        "    except Exception as e:\n",
        "        print(\"Skip (AFL feature extract)\", rid, \":\", e)\n",
        "feat_df = pd.DataFrame(rows).fillna(0.0)\n",
        "assert not feat_df.empty, \"No features extracted.\"\n",
        "\n",
        "# ---------- TP vs FP feature table ----------\n",
        "from scipy.stats import mannwhitneyu\n",
        "TP = feat_df[feat_df[\"y\"]==1]; FP = feat_df[feat_df[\"y\"]==0]\n",
        "feature_cols = [c for c in feat_df.columns if c not in [\"record_id\",\"y\"]]\n",
        "\n",
        "rows_sum = []\n",
        "for f in feature_cols:\n",
        "    tp_vals = TP[f].dropna().values\n",
        "    fp_vals = FP[f].dropna().values\n",
        "    tp_med  = float(np.median(tp_vals)) if len(tp_vals) else np.nan\n",
        "    fp_med  = float(np.median(fp_vals)) if len(fp_vals) else np.nan\n",
        "    if len(tp_vals) and len(fp_vals):\n",
        "        _, pval = mannwhitneyu(tp_vals, fp_vals, alternative='two-sided')\n",
        "        delta   = cliffs_delta(tp_vals, fp_vals)\n",
        "    else:\n",
        "        pval, delta = np.nan, np.nan\n",
        "    rows_sum.append({\n",
        "        \"Feature\": f, \"nTP\": len(tp_vals), \"nFP\": len(fp_vals),\n",
        "        \"TP Median\": tp_med, \"FP Median\": fp_med,\n",
        "        \"p-value\": pval, \"Cliff's Δ\": delta, \"Effect Size\": cliffs_size_label(delta),\n",
        "    })\n",
        "summary_tbl = pd.DataFrame(rows_sum)\n",
        "summary_tbl[\"|Δ|\"] = summary_tbl[\"Cliff's Δ\"].abs()\n",
        "summary_tbl = summary_tbl.sort_values([\"|Δ|\",\"p-value\"], ascending=[False, True]).drop(columns=\"|Δ|\").reset_index(drop=True)\n",
        "\n",
        "Path(OUT/\"analysis\").mkdir(parents=True, exist_ok=True)\n",
        "summary_tbl.to_csv(OUT/f\"analysis/afl_{SPLIT}_tp_fp_feature_summary.csv\", index=False)\n",
        "print(f\"[Saved] {OUT/'analysis'}/afl_{SPLIT}_tp_fp_feature_summary.csv\")\n",
        "print(f\"Scope: AFLvsREST ({SPLIT}); n(TP)={len(TP)}, n(FP)={len(FP)}\")\n",
        "\n",
        "# ---------- FP reduction Stage-1 -> Stage-2 (using YOUR AFL validator) ----------\n",
        "FP_before = int((df_cand[\"y\"]==0).sum())\n",
        "TP_before = int((df_cand[\"y\"]==1).sum())\n",
        "\n",
        "clf = joblib.load(OUT/\"stage2_AFLvsREST_validator.joblib\")\n",
        "meta = json.load(open(OUT/\"stage2_AFLvsREST_meta.json\"))\n",
        "tau  = float(meta[\"threshold\"])\n",
        "feat_cols_valid = meta[\"features\"]   # validator's AFL feature set/order\n",
        "\n",
        "X_all = feat_df.drop(columns=[\"record_id\",\"y\"]).reindex(columns=feat_cols_valid, fill_value=0.0).to_numpy()\n",
        "proba = clf.predict_proba(X_all)[:,1]\n",
        "kept_mask = (proba >= tau)\n",
        "y_true = feat_df[\"y\"].to_numpy().astype(int)\n",
        "\n",
        "FP_after = int(((kept_mask == 1) & (y_true == 0)).sum())\n",
        "TP_after = int(((kept_mask == 1) & (y_true == 1)).sum())\n",
        "red_abs = FP_before - FP_after\n",
        "red_pct = (red_abs / FP_before * 100.0) if FP_before > 0 else np.nan\n",
        "print(f\"[AFL] FP reduction (Stage-1 -> Stage-2 @τ): {FP_before} -> {FP_after}  (−{red_abs}, {red_pct:.1f}%)\")\n",
        "\n",
        "# ---------- Incremental FP reduction curve (add AFL features by |weight|) ----------\n",
        "def choose_tau(y, scores, strategy_meta):\n",
        "    from sklearn.metrics import precision_recall_curve\n",
        "    mP = re.search(r'precision≥([0-9.]+)', strategy_meta)\n",
        "    mR = re.search(r'recall≥([0-9.]+)', strategy_meta)\n",
        "    if mP and mR:\n",
        "        p_min = float(mP.group(1)); r_min = float(mR.group(1))\n",
        "        P, R, T = precision_recall_curve(y, scores)\n",
        "        both = [(pp, rr, tt) for pp, rr, tt in zip(P[:-1], R[:-1], T) if (pp >= p_min and rr >= r_min)]\n",
        "        if both:\n",
        "            both.sort(key=lambda z: (z[1], z[0])); return float(both[-1][2])\n",
        "        byR = [(pp, rr, tt) for pp, rr, tt in zip(P[:-1], R[:-1], T) if rr >= r_min]\n",
        "        if byR:\n",
        "            byR.sort(key=lambda z: (z[0], z[1])); return float(byR[-1][2])\n",
        "        byP = [(pp, rr, tt) for pp, rr, tt in zip(P[:-1], R[:-1], T) if pp >= p_min]\n",
        "        if byP:\n",
        "            byP.sort(key=lambda z: (z[1], z[0])); return float(byP[-1][2])\n",
        "        return float(T[-1]) if len(T) else 0.5\n",
        "    # fallback: PR-F1 max\n",
        "    P, R, T = precision_recall_curve(y, scores)\n",
        "    f1 = (2*P*R)/(P+R+1e-12)\n",
        "    i = int(np.nanargmax(f1[:-1])); return float(T[i])\n",
        "\n",
        "strategy = meta.get(\"strategy\",\"precision≥0.30, recall≥0.85\")\n",
        "lr = clf.named_steps[\"logisticregression\"]; w = lr.coef_.ravel()\n",
        "order = np.argsort(-np.abs(w))\n",
        "ordered_feats = [feat_cols_valid[i] for i in order]\n",
        "\n",
        "fp_curve = []\n",
        "for k in range(1, len(ordered_feats)+1):\n",
        "    used = ordered_feats[:k]\n",
        "    Xk = feat_df[used].to_numpy()\n",
        "    clf_k = make_pipeline(\n",
        "        StandardScaler(with_mean=True),\n",
        "        LogisticRegression(max_iter=1000, penalty=\"l1\", solver=\"liblinear\",\n",
        "                           C=0.7, class_weight={0:1.0, 1:4.0}, random_state=42)\n",
        "    ).fit(Xk, y_true)\n",
        "    prob_k = clf_k.predict_proba(Xk)[:,1]\n",
        "    tau_k  = choose_tau(y_true, prob_k, strategy)\n",
        "    pred_k = (prob_k >= tau_k).astype(int)\n",
        "    FP_k   = int(((pred_k == 1) & (y_true == 0)).sum())\n",
        "    TP_k   = int(((pred_k == 1) & (y_true == 1)).sum())\n",
        "    fp_curve.append({\n",
        "        \"k\": k,\n",
        "        \"features_used\": \", \".join(used),\n",
        "        \"FP_before\": FP_before,\n",
        "        \"FP_after_k\": FP_k,\n",
        "        \"TP_after_k\": TP_k,\n",
        "        \"FP_reduction_abs\": FP_before - FP_k,\n",
        "        \"FP_reduction_pct\": (FP_before - FP_k) / FP_before * 100.0 if FP_before>0 else np.nan,\n",
        "        \"tau_k\": tau_k\n",
        "    })\n",
        "\n",
        "fp_curve_df = pd.DataFrame(fp_curve)\n",
        "fp_curve_df.to_csv(OUT/f\"analysis/afl_{SPLIT}_fp_reduction_curve.csv\", index=False)\n",
        "print(f\"[Saved] {OUT/'analysis'}/afl_{SPLIT}_fp_reduction_curve.csv\")\n",
        "\n",
        "try:\n",
        "    from IPython.display import display\n",
        "    display(summary_tbl.head(10))\n",
        "    display(fp_curve_df.head(8))\n",
        "except Exception:\n",
        "    pass\n",
        "# ================= END AFL ANALYSIS =================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uwspwXolzXE3",
        "outputId": "1eb11e1f-ca5f-46cd-c37a-ac1fe0006a64"
      },
      "outputs": [],
      "source": [
        "# ===================== QUALITATIVE CASE PLOTS (AF & AFL) =====================\n",
        "# - Plots RR intervals, QRS-masked atrial strip, PSD with shaded bands\n",
        "# - Prints EXACT features used by your Stage-2 (AF or AFL)\n",
        "# ---------------------------------------------------------------------------\n",
        "# How to use:\n",
        "# 1) Ensure your Stage-1 and Stage-2 ran (positives_<split>_with_labels.csv exists).\n",
        "# 2) Set TASK and SPLIT below. Either set record_ids_tp/fp manually or keep None to auto-sample.\n",
        "# 3) Run; figures appear for each record.\n",
        "\n",
        "TASK  = \"AFLvsREST\"   # \"AFvsREST\" or \"AFLvsREST\"\n",
        "SPLIT = \"val\"         # \"val\" or \"test\"\n",
        "\n",
        "record_ids_tp = None  # e.g. [\"06_060_JS05283\", ...] or None to auto-sample\n",
        "record_ids_fp = None  # e.g. [\"06_999_ABC123\", ...] or None to auto-sample\n",
        "\n",
        "# ---------------------------- Imports & config ------------------------------\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from scipy.signal import butter, sosfiltfilt, welch, find_peaks\n",
        "\n",
        "OUT = Path(\"/content/drive/MyDrive/HuBERT-ECG/processed_ptbxl_5s_100hz_fast\")\n",
        "FS = 100\n",
        "LEADS = {'I':0,'II':1,'III':2,'aVR':3,'aVL':4,'aVF':5,'V1':6,'V2':7,'V3':8,'V4':9,'V5':10,'V6':11}\n",
        "\n",
        "# ---------------------------- Signal helpers --------------------------------\n",
        "def bandpass(sig, fs, lo, hi, order=4):\n",
        "    ny = fs/2; hi = min(hi, ny*0.98)\n",
        "    sos = butter(order, [lo/ny, hi/ny], btype='band', output='sos')\n",
        "    return sosfiltfilt(sos, sig)\n",
        "\n",
        "def safe_load_np(path):\n",
        "    x = np.load(path)\n",
        "    if x.ndim!=2 or x.shape[1]!=12:\n",
        "        raise ValueError(f\"bad shape {getattr(x,'shape',None)}\")\n",
        "    return x\n",
        "\n",
        "def pick_lead_for_rhythm(x):\n",
        "    for name in ['II','V1','V2']:\n",
        "        idx = LEADS[name]\n",
        "        if np.std(x[:,idx]) > 1e-4:\n",
        "            return x[:,idx], name\n",
        "    idx = np.argmax(np.std(x, axis=0))\n",
        "    inv = {v:k for k,v in LEADS.items()}\n",
        "    return x[:, idx], inv[idx]\n",
        "\n",
        "def pick_lead_for_qrs(x):\n",
        "    for name in ['II','V1','V2']:\n",
        "        idx = LEADS[name]\n",
        "        if np.std(x[:, idx]) > 1e-4:\n",
        "            return x[:, idx], name\n",
        "    idx = np.argmax(np.std(x, axis=0))\n",
        "    inv = {v:k for k,v in LEADS.items()}\n",
        "    return x[:, idx], inv[idx]\n",
        "\n",
        "def detect_qrs(sig, fs=FS):\n",
        "    y = bandpass(sig, fs, 5, 20)\n",
        "    diff = np.diff(y, prepend=y[0]); sq = diff**2\n",
        "    win = max(int(0.12*fs), 1)\n",
        "    integ = np.convolve(sq, np.ones(win)/win, mode='same')\n",
        "    peaks, _ = find_peaks(integ, distance=int(0.25*fs), prominence=np.percentile(integ, 85))\n",
        "    return peaks\n",
        "\n",
        "# ----------------------------- Feature blocks --------------------------------\n",
        "# AF features (your Stage-2 AF extractor)\n",
        "def rr_features(qrs_idx, fs=FS):\n",
        "    if len(qrs_idx) < 3:\n",
        "        return dict(rr_mean=np.nan, rr_std=np.nan, rr_cv=np.nan, rr_rmssd=np.nan, rr_reg=np.nan)\n",
        "    rr = np.diff(qrs_idx)/fs\n",
        "    mean = float(np.mean(rr)); std = float(np.std(rr))\n",
        "    cv = float(std/(mean+1e-8))\n",
        "    rmssd = float(np.sqrt(np.mean(np.diff(rr)**2))) if len(rr)>2 else np.nan\n",
        "    reg = float(1.0/(1.0+cv))\n",
        "    return dict(rr_mean=mean, rr_std=std, rr_cv=cv, rr_rmssd=rmssd, rr_reg=reg)\n",
        "\n",
        "def pwave_metrics(sig, qrs_idx, fs=FS):\n",
        "    if len(qrs_idx) < 2:\n",
        "        return dict(p_en=np.nan, p_present=np.nan, p_to_qrs=np.nan)\n",
        "    bp = bandpass(sig, fs, 4, 15)\n",
        "    wins = []\n",
        "    for q in qrs_idx:\n",
        "        a = max(0, q - int(0.20*fs)); b = max(0, q - int(0.05*fs))\n",
        "        if b > a:\n",
        "            wins.append(bp[a:b])\n",
        "    if not wins:\n",
        "        return dict(p_en=np.nan, p_present=np.nan, p_to_qrs=np.nan)\n",
        "    en = np.array([np.mean(w**2) for w in wins])\n",
        "    p_en = float(np.median(en))\n",
        "    thr = max(np.percentile(en, 60), 1e-6)\n",
        "    p_present = float(np.mean(en > thr))\n",
        "    pband = bandpass(sig, fs, 3, 9)\n",
        "    p_peaks, _ = find_peaks(pband, distance=int(0.18*fs), prominence=np.percentile(np.abs(pband), 80))\n",
        "    p_to_qrs = float(len(p_peaks)/(len(qrs_idx)+1e-6))\n",
        "    return dict(p_en=p_en, p_present=p_present, p_to_qrs=p_to_qrs)\n",
        "\n",
        "def flutter_bandpower(sig, fs=FS):\n",
        "    f, Pxx = welch(bandpass(sig, fs, 1, 20), fs=fs, nperseg=min(256, len(sig)))\n",
        "    mask_lo = (f>=3) & (f<=8)\n",
        "    mask_all = (f>=1) & (f<=20)\n",
        "    bp_3_8 = float(np.trapz(Pxx[mask_lo], f[mask_lo])) if mask_lo.any() else 0.0\n",
        "    bp_1_20 = float(np.trapz(Pxx[mask_all], f[mask_all])) if mask_all.any() else 1e-12\n",
        "    return dict(flutter_ratio = bp_3_8 / bp_1_20), (f, Pxx)\n",
        "\n",
        "# AFL extras (your Stage-2 AFL extractor)\n",
        "def bandpower_ratio(sig, fs, band, refband):\n",
        "    f, Pxx = welch(bandpass(sig, fs, 1, 20), fs=fs, nperseg=min(256, len(sig)))\n",
        "    m1 = (f>=band[0]) & (f<=band[1]); m2 = (f>=refband[0]) & (f<=refband[1])\n",
        "    bp1 = float(np.trapz(Pxx[m1], f[m1])) if m1.any() else 0.0\n",
        "    bp2 = float(np.trapz(Pxx[m2], f[m2])) if m2.any() else 1e-12\n",
        "    return (bp1 / bp2), (f, Pxx)\n",
        "\n",
        "def pwave_perbeat_metrics(sig, qrs_idx, fs=FS):\n",
        "    if len(qrs_idx) < 2:\n",
        "        return dict(p_en_med=np.nan, p_present_frac=np.nan, p_per_qrs_med=np.nan)\n",
        "    pband = bandpass(sig, fs, 3, 9)\n",
        "    en, p_perq = [], []\n",
        "    for q in qrs_idx:\n",
        "        a = max(0, q - int(0.20*fs)); b = max(0, q - int(0.06*fs))\n",
        "        if b <= a: continue\n",
        "        w = pband[a:b]\n",
        "        en.append(np.mean(w**2))\n",
        "        peaks, _ = find_peaks(w, distance=int(0.08*fs), prominence=np.percentile(np.abs(w), 80))\n",
        "        p_perq.append(len(peaks))\n",
        "    if not en: return dict(p_en_med=np.nan, p_present_frac=np.nan, p_per_qrs_med=np.nan)\n",
        "    en = np.array(en)\n",
        "    p_en_med = float(np.median(en))\n",
        "    thr = max(np.percentile(en, 60), 1e-6)\n",
        "    p_present_frac = float(np.mean(en > thr))\n",
        "    p_per_qrs_med = float(np.median(p_perq)) if len(p_perq) else 0.0\n",
        "    return dict(p_en_med=p_en_med, p_present_frac=p_present_frac, p_per_qrs_med=p_per_qrs_med)\n",
        "\n",
        "# --------------------------- Plotting primitives -----------------------------\n",
        "def plot_rr_panel(ax, qrs_idx):\n",
        "    ax.set_title(\"A. RR Tachogram\")\n",
        "    if len(qrs_idx) < 2:\n",
        "        ax.text(0.05,0.5,\"<too few beats>\", transform=ax.transAxes)\n",
        "        return {}\n",
        "    rr = np.diff(qrs_idx)/FS\n",
        "    t = np.cumsum(np.r_[0, rr[:-1]])\n",
        "    ax.plot(t, rr, marker='o', lw=1)\n",
        "    stats = rr_features(qrs_idx)\n",
        "    txt = (f\"meanRR={stats['rr_mean']:.3f}s  HR≈{60.0/max(stats['rr_mean'],1e-6):.0f} bpm\\n\"\n",
        "           f\"SDNN={stats['rr_std']:.3f}s  CV={stats['rr_cv']:.3f}  RMSSD={stats['rr_rmssd']:.3f}s\\n\"\n",
        "           f\"Regularity={stats['rr_reg']:.3f}\")\n",
        "    ax.set_xlabel(\"Time (s)\"); ax.set_ylabel(\"RR (s)\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.text(0.01, 0.98, txt, ha=\"left\", va=\"top\", transform=ax.transAxes,\n",
        "            fontsize=9, bbox=dict(boxstyle=\"round\", fc=\"white\", ec=\"0.8\"))\n",
        "    return stats\n",
        "\n",
        "def qrs_mask_signal(sig, qrs_idx, width_ms=60):\n",
        "    w = int(width_ms*FS/1000)\n",
        "    sig2 = sig.copy()\n",
        "    for q in qrs_idx:\n",
        "        a = max(0, q - w); b = min(len(sig2), q + w)\n",
        "        sig2[a:b] = 0.0\n",
        "    return sig2\n",
        "\n",
        "def plot_strip_panel_af(ax, sig, qrs_idx, lead_name=\"auto\"):\n",
        "    ax.set_title(f\"B. Lead {lead_name} • QRS-masked atrial\")\n",
        "    t = np.arange(len(sig))/FS\n",
        "    ax.plot(t, sig, lw=0.8, alpha=0.6, label=\"original\")\n",
        "    for q in qrs_idx:\n",
        "        ax.axvline(q/FS, color='k', lw=0.7, alpha=0.25)\n",
        "    masked = qrs_mask_signal(sig, qrs_idx, 60)\n",
        "    ax.plot(t, masked, lw=1.0, label=\"QRS-masked\")\n",
        "    ax.set_xlabel(\"Time (s)\"); ax.set_ylabel(\"mV (a.u.)\"); ax.grid(True, alpha=0.3)\n",
        "    ax.legend(loc=\"upper right\", fontsize=9)\n",
        "    return masked\n",
        "\n",
        "def plot_psd_panel_af(ax, sig, title=\"C. PSD (1–20 Hz)\"):\n",
        "    ax.set_title(title)\n",
        "    f, Pxx = welch(bandpass(sig, FS, 1, 20), fs=FS, nperseg=min(256, len(sig)))\n",
        "    ax.semilogy(f, Pxx+1e-15, lw=1.0)\n",
        "    # Shade 3–8 Hz\n",
        "    ax.axvspan(3, 8, alpha=0.2, label=\"3–8 Hz\")\n",
        "    ax.axvspan(8, 12, alpha=0.1, label=\"8–12 Hz\")\n",
        "    ax.set_xlim(1, 20); ax.set_xlabel(\"Frequency (Hz)\"); ax.set_ylabel(\"Power\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    return f, Pxx\n",
        "\n",
        "# ----------------------- AF: full panel (features) ---------------------------\n",
        "def plot_case_AF(record_id, out_path, label_text=\"\"):\n",
        "    x = safe_load_np(out_path)\n",
        "    sig, lead_name = pick_lead_for_rhythm(x)\n",
        "    qrs = detect_qrs(sig, FS)\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 7))\n",
        "    gs = fig.add_gridspec(3, 1, height_ratios=[1.1, 1.2, 1.2], hspace=0.35)\n",
        "    ax1 = fig.add_subplot(gs[0]); ax2 = fig.add_subplot(gs[1]); ax3 = fig.add_subplot(gs[2])\n",
        "\n",
        "    rr_stats = plot_rr_panel(ax1, qrs)\n",
        "    masked = plot_strip_panel_af(ax2, sig, qrs, lead_name)\n",
        "\n",
        "    # AF features & PSD\n",
        "    flutter_dict, _ = flutter_bandpower(sig, FS)\n",
        "    p_dict = pwave_metrics(sig, qrs, FS)\n",
        "    f, Pxx = plot_psd_panel_af(ax3, sig)\n",
        "    # overlay feature text\n",
        "    feat_txt = (f\"AF features:\\n\"\n",
        "                f\"rr_mean={rr_stats.get('rr_mean',np.nan):.3f}, rr_std={rr_stats.get('rr_std',np.nan):.3f}, \"\n",
        "                f\"cv={rr_stats.get('rr_cv',np.nan):.3f}, rmssd={rr_stats.get('rr_rmssd',np.nan):.3f}, reg={rr_stats.get('rr_reg',np.nan):.3f}\\n\"\n",
        "                f\"p_en={p_dict['p_en']:.3e}, p_present={p_dict['p_present']:.2f}, p_to_qrs={p_dict['p_to_qrs']:.2f}\\n\"\n",
        "                f\"flutter_ratio(3–8 / 1–20)={flutter_dict['flutter_ratio']:.3f}\")\n",
        "    ax3.text(0.01, 0.98, feat_txt, ha=\"left\", va=\"top\", transform=ax3.transAxes,\n",
        "             fontsize=9, bbox=dict(boxstyle=\"round\", fc=\"white\", ec=\"0.8\"))\n",
        "\n",
        "    fig.suptitle(f\"AF case • record_id={record_id} • {label_text}\", y=1.02, fontsize=12)\n",
        "    plt.show()\n",
        "\n",
        "# ----------------------- AFL: full panel (features) --------------------------\n",
        "def pick_best_afl_lead_and_scores(x, qrs):\n",
        "    # Evaluate candidate leads with QRS suppression for flutter ratios / harmonics\n",
        "    CANDIDATE_LEADS = ['V1','II','V2','III','aVF']\n",
        "    best = None\n",
        "    for name in CANDIDATE_LEADS:\n",
        "        sig = x[:, LEADS[name]].astype(float).copy()\n",
        "        if len(qrs) > 0:\n",
        "            for q in qrs:\n",
        "                a = max(0, q - int(0.06*FS)); b = min(len(sig), q + int(0.06*FS))\n",
        "                sig[a:b] = 0.0\n",
        "        fr, (f, Pxx) = bandpower_ratio(sig, FS, (3,8), (1,20))\n",
        "        h2, _       = bandpower_ratio(sig, FS, (8,12), (1,20))\n",
        "        score = fr + 0.5*h2\n",
        "        item = (name, sig, fr, h2, score, f, Pxx)\n",
        "        if (best is None) or (score > best[4]):\n",
        "            best = item\n",
        "    return best  # (name, sig, fr, h2, score, f, Pxx)\n",
        "\n",
        "def plot_case_AFL(record_id, out_path, label_text=\"\"):\n",
        "    x = safe_load_np(out_path)\n",
        "    qrs_lead, _ = pick_lead_for_qrs(x)\n",
        "    qrs = detect_qrs(qrs_lead, FS)\n",
        "\n",
        "    # pick best lead for atrial activity (with QRS suppression)\n",
        "    best_name, best_sig_qs, fr, h2, score, f_best, Pxx_best = pick_best_afl_lead_and_scores(x, qrs)\n",
        "    # Get raw (unmasked) best lead for plotting the strip\n",
        "    best_sig_raw = x[:, LEADS[best_name]]\n",
        "\n",
        "    # Compute P-wave per-beat metrics on the best lead (raw)\n",
        "    pbeat = pwave_perbeat_metrics(best_sig_raw, qrs, FS)\n",
        "\n",
        "    # Atrial peak height around 3.5–7 Hz\n",
        "    f_psd, Pxx_psd = welch(bandpass(best_sig_raw, FS, 1, 20), fs=FS, nperseg=min(256,len(best_sig_raw)))\n",
        "    mask = (f_psd>=3.5) & (f_psd<=7.0)\n",
        "    if mask.any():\n",
        "        i = np.argmax(Pxx_psd[mask]); f_peak = float(f_psd[mask][i]); pk = float(Pxx_psd[mask][i])\n",
        "        lo = max(0, np.where(mask)[0][i]-5); hi = min(len(Pxx_psd), np.where(mask)[0][i]+6)\n",
        "        baseline = 1e-12 + float(np.median(Pxx_psd[lo:hi])); atrial_peak_h = pk / baseline\n",
        "    else:\n",
        "        f_peak, atrial_peak_h = 0.0, 0.0\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 7))\n",
        "    gs = fig.add_gridspec(3, 1, height_ratios=[1.1, 1.2, 1.2], hspace=0.35)\n",
        "    ax1 = fig.add_subplot(gs[0]); ax2 = fig.add_subplot(gs[1]); ax3 = fig.add_subplot(gs[2])\n",
        "\n",
        "    rr_stats = plot_rr_panel(ax1, qrs)\n",
        "    # Strip: show raw + QRS-masked on same best lead\n",
        "    masked = plot_strip_panel_af(ax2, best_sig_raw, qrs, lead_name=best_name)\n",
        "\n",
        "    # PSD on best lead (raw) for visibility; shade 3–8 and 8–12 bands\n",
        "    ax3.set_title(\"C. PSD (1–20 Hz) • AFL bands\")\n",
        "    ax3.semilogy(f_psd, Pxx_psd+1e-15, lw=1.0)\n",
        "    ax3.axvspan(3, 8, alpha=0.2, label=\"3–8 Hz\")\n",
        "    ax3.axvspan(8, 12, alpha=0.1, label=\"8–12 Hz\")\n",
        "    ax3.set_xlim(1, 20); ax3.set_xlabel(\"Frequency (Hz)\"); ax3.set_ylabel(\"Power\")\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    # AFL feature box (exact Stage-2 AFL features)\n",
        "    feat_txt = (f\"AFL features (best lead {best_name}):\\n\"\n",
        "                f\"flutter_ratio_qrs_supp={fr:.3f}  harmonic_8_12={h2:.3f}  flutter_score={score:.3f}\\n\"\n",
        "                f\"p_en_med={pbeat['p_en_med']:.3e}  p_present_frac={pbeat['p_present_frac']:.2f}  \"\n",
        "                f\"p_per_qrs_med={pbeat['p_per_qrs_med']:.2f}\\n\"\n",
        "                f\"atrial_peak_h={atrial_peak_h:.2f}  atrial_peak_hz={f_peak:.2f}\\n\"\n",
        "                f\"amp_med={np.median(best_sig_raw):.3f}  amp_iqr={(np.percentile(best_sig_raw,75)-np.percentile(best_sig_raw,25)):.3f}\")\n",
        "    ax3.text(0.01, 0.98, feat_txt, ha=\"left\", va=\"top\", transform=ax3.transAxes,\n",
        "             fontsize=9, bbox=dict(boxstyle=\"round\", fc=\"white\", ec=\"0.8\"))\n",
        "\n",
        "    fig.suptitle(f\"AFL case • record_id={record_id} • {label_text}\", y=1.02, fontsize=12)\n",
        "    plt.show()\n",
        "\n",
        "# ------------------------- Driver: pick examples & plot -----------------------\n",
        "# Load Stage-1 candidates with labels and paths\n",
        "pos_wlab = OUT/f\"stage1_{TASK}/positives_{SPLIT}_with_labels.csv\"\n",
        "assert Path(pos_wlab).exists(), f\"Missing {pos_wlab}. Run Stage-1 first.\"\n",
        "cand = pd.read_csv(pos_wlab)\n",
        "cand[\"record_id\"] = cand[\"record_id\"].astype(str)\n",
        "cand.rename(columns={\"is_true_positive\":\"y\"}, inplace=True)\n",
        "\n",
        "split_df = pd.read_csv(OUT/f\"split_{SPLIT}.csv\").set_index(\"record_id\")\n",
        "split_df.index = split_df.index.astype(str)\n",
        "cand[\"out_path\"] = cand[\"record_id\"].map(split_df[\"out_path\"])\n",
        "cand = cand.dropna(subset=[\"out_path\"]).reset_index(drop=True)\n",
        "\n",
        "# Auto-sample records if none provided\n",
        "if record_ids_tp is None:\n",
        "    record_ids_tp = cand.loc[cand[\"y\"]==1, \"record_id\"].head(3).tolist()\n",
        "if record_ids_fp is None:\n",
        "    record_ids_fp = cand.loc[cand[\"y\"]==0, \"record_id\"].head(3).tolist()\n",
        "\n",
        "print(f\"Plotting {len(record_ids_tp)} TP and {len(record_ids_fp)} FP examples for {TASK} ({SPLIT})\")\n",
        "\n",
        "def plot_one(record_id, y_val):\n",
        "    row = cand[cand[\"record_id\"]==record_id]\n",
        "    if row.empty:\n",
        "        print(f\"  ! skip {record_id}: not in candidate pool\")\n",
        "        return\n",
        "    path = row.iloc[0][\"out_path\"]\n",
        "    lab = \"TP (label=1)\" if y_val==1 else \"FP (label=0)\"\n",
        "    if TASK == \"AFvsREST\":\n",
        "        plot_case_AF(record_id, path, label_text=lab)\n",
        "    else:\n",
        "        plot_case_AFL(record_id, path, label_text=lab)\n",
        "\n",
        "for rid in record_ids_tp:\n",
        "    plot_one(rid, 1)\n",
        "for rid in record_ids_fp:\n",
        "    plot_one(rid, 0)\n",
        "# =================== END QUALITATIVE CASE PLOTS ===================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIwrUGKDLDVg"
      },
      "outputs": [],
      "source": [
        "# from the project root\n",
        "!zip -r project.zip .\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0ebbc734d2b04ad4864809e2831ca4d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "151b6650398a4f6a8cb4646569e8333e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "243981cc97e14c0caacf2f84a681d7d5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2641a1f1eadc4d149777900ed619385e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "282b545db5504ed2b83af4f3be132274": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ebbc734d2b04ad4864809e2831ca4d4",
            "placeholder": "​",
            "style": "IPY_MODEL_151b6650398a4f6a8cb4646569e8333e",
            "value": " 1.50k/? [00:00&lt;00:00, 124kB/s]"
          }
        },
        "2a856a402963424292ffd655cb85d0cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "32f663c4149d46d0915db39df0511e99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2ff8e5c08d44ea8865691a6db47088c",
              "IPY_MODEL_ee0704dc70eb4cbf854f75e90b048ad9",
              "IPY_MODEL_7d4d677896e9468881d72bfe8574b864"
            ],
            "layout": "IPY_MODEL_c188ccf3b4e54278ad190fa39ee381e1"
          }
        },
        "332fd3d7ccb54ea7889fce1bdc6ff24a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33541973fce7430fbb8bc8e822534dec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ca7ef085e264155b922226c368aa38b",
              "IPY_MODEL_611a073da6ae4bde9e5065f913a762a2",
              "IPY_MODEL_bbc7adaa932644dab4851db28a782869"
            ],
            "layout": "IPY_MODEL_bafcfec6b23a47619d7884fb428e0df2"
          }
        },
        "3924349dafd74d008d634488aa800371": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ea6b2edc9a14b22afa613c41d145093": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43452960f1fa43ba8713767a711c8664": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_243981cc97e14c0caacf2f84a681d7d5",
            "placeholder": "​",
            "style": "IPY_MODEL_332fd3d7ccb54ea7889fce1bdc6ff24a",
            "value": "config.json: "
          }
        },
        "583f013494554c34a81be875b0fa4c30": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "584cc7a8235c4de183c92d731e5cc868": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "5bed995149a0440a95d1ef0ad716a906": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_43452960f1fa43ba8713767a711c8664",
              "IPY_MODEL_afbc31723a5e48d686a8accb8f050802",
              "IPY_MODEL_282b545db5504ed2b83af4f3be132274"
            ],
            "layout": "IPY_MODEL_b13c364fced4488b92eeafbfa59adb55"
          }
        },
        "5ca7ef085e264155b922226c368aa38b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82f737ef8d68401d8352f211f8ccd584",
            "placeholder": "​",
            "style": "IPY_MODEL_583f013494554c34a81be875b0fa4c30",
            "value": "model.safetensors: 100%"
          }
        },
        "611a073da6ae4bde9e5065f913a762a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77a5950bfbc2446286295790ad7e64d7",
            "max": 372518672,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2a856a402963424292ffd655cb85d0cc",
            "value": 372518672
          }
        },
        "6fee8fbd1da9408a93aa319260469321": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "77a5950bfbc2446286295790ad7e64d7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a47595834134ee4ab62e5556d4d3599": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7a497d2b2a3a46e58159e63a1fc91ffc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d4d677896e9468881d72bfe8574b864": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96d202d918844c3388f9710d5a6a08b0",
            "placeholder": "​",
            "style": "IPY_MODEL_7a497d2b2a3a46e58159e63a1fc91ffc",
            "value": " 2.08k/? [00:00&lt;00:00, 132kB/s]"
          }
        },
        "82f737ef8d68401d8352f211f8ccd584": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96d202d918844c3388f9710d5a6a08b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afbc31723a5e48d686a8accb8f050802": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fee8fbd1da9408a93aa319260469321",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a47595834134ee4ab62e5556d4d3599",
            "value": 1
          }
        },
        "b13c364fced4488b92eeafbfa59adb55": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bafcfec6b23a47619d7884fb428e0df2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbc7adaa932644dab4851db28a782869": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2641a1f1eadc4d149777900ed619385e",
            "placeholder": "​",
            "style": "IPY_MODEL_3ea6b2edc9a14b22afa613c41d145093",
            "value": " 373M/373M [00:04&lt;00:00, 167MB/s]"
          }
        },
        "c188ccf3b4e54278ad190fa39ee381e1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e04a77f0380346bd9a10b999d0376f29": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee0704dc70eb4cbf854f75e90b048ad9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_584cc7a8235c4de183c92d731e5cc868",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f723dc87836d4e75b40019e0e482a095",
            "value": 1
          }
        },
        "f2ff8e5c08d44ea8865691a6db47088c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e04a77f0380346bd9a10b999d0376f29",
            "placeholder": "​",
            "style": "IPY_MODEL_3924349dafd74d008d634488aa800371",
            "value": "hubert_ecg.py: "
          }
        },
        "f723dc87836d4e75b40019e0e482a095": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
